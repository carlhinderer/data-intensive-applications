-----------------------------------------------------------------------------
| CHAPTER 3 - STORAGE AND RETRIEVAL                                         |
-----------------------------------------------------------------------------

- World's Simplest Database

    - Here is the database:

        #!/bin/bash

        db_set () {
            echo "$1,$2" >> database
        }

        db_get () {
            grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
        }


    - To use it:

        $ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'
        $ db_get 42


    - Appending to our database (append-only log file) is very fast.  Lookups, however, have
        O(n) performance.


    - In order to implement lookups efficiently, we need an index, an additional data structure
        derived from primary data.  Basically, we keep some metadata on the side which helps locate
        the data we want.

      Note that maintaining additional structures does increase overhead, especially on writes.



- Hash indexes

    - Key/value stores are similar to dictionaries in PLs, and are usually implemented as hash maps.


    - For instance, we can just keep an in-memory hash map of where every key is mapped to a byte
        offset in the data file.  Every time we append a new key/value pair to the file, we update
        the hash map with the new key and offset.

      This is essentially what BitCask (the default storage engine in Riak) does.


    - Note that all of the available keys must fit in memory.  This approach is well suited to 
        applications where the value of each key is updated frequently.


    - We also have to solve the problem of running out of disk space if we only ever append to the
        log.  To solve this, we can break the log into segments and close a file when it reaches
        a certain size.

      Then, we can use compaction (throw away duplicate keys in the log, only keeping the latest
        update) on these segments.  This can be done in a background thread while we are able to
        read and write requests as normal.


    - If you want to delete a key/value pair, you have to append a special deletion record (a tombstone).
        When the log segments are merged, the tombstone tells the merging process to discard any
        previous values for the key.


    - In a crash, the hash map is lost, and all of the log files must be read to restore it.  We can
        mitigate this by saving periodic snapshots of the hash map to disk.


    - Since writes are appended in a strictly sequential order, it is common to have just a single
        writer thread.  Since data files are append-only and immutable, they can be read concurrently
        by multiple threads.


    - Range queries are not efficient.  In order to scan over all keys 0000-9999, you have to look
        up each key separately.



- SSTables and LSM-Trees

    - The first change we make is to keep our segment files sorted by key.  We call this format
        Sorted String Table, or SSTable.  We also require that each key only appears once in a 
        segment file (the compaction process already ensures this for multiple files).


    - Advantages of this:

        1. Merging segments is simple and efficient with a mergesort-like approach.

        2. No longer have to keep all keys in memory, since we can search through sorted files once we
             get close to the key.  We still need an in-memory index for some of the keys.

        3. We can compress blocks of key/value pairs to save disk space and reduce I/O if desired.



- Constructing and Maintaining SSTables

    - Now, we think about how we keep our data sorted by key in the first place.  The approach involves
        using an in-memory sorted tree structure like a Red-Black Tree or AVL tree.


    - So, our storage engine works as follows:

        1. When a write comes in, add it to our in-memory Red-Black tree.  This in-memory tree is
             sometimes called a 'memtable'.

        2. When a memtable gets bigger than some threshold (ie a few MB), we write it out to disk as
             an SSTable file.  This can be done efficiently, since we already have the key-value pairs
             sorted by key in our tree.  The new SSTable file becomes the most recent segment of the
             database.  While the SSTable is being written out to disk, writes can continue to a 
             new memtable instance.

        3. In order to serve a read request, first look in the memtable, then the most recent on-disk
             segment, then the next oldest, etc.

        4. From time to time, run a merging and compacting process on the background to combine
             segment files.


    - One problem here is that if the database crashes, we lose the writes in the current memtable.
        To solve this problem, we can just keep a separate log on disk we append every write to 
        immediately.

      It is not sorted, but that doesn't matter, since it's only used for recovery.  We can discard
        the log every time we write the memtable to disk.



- Making an LSM-tree out of SSTables

    - This simple algorithm is used by LevelDB and RocksDB, simple key-value storage engine libraries
        meant to be embedded in other applications.  LevelDB can also be used as a storage engine
        for Riak.

    - Similar storage engines are used in Cassandra and HBase, both of which were inspired by Google's
        BigTable paper (which introduced the terms 'SSTable' and 'memtable').

    - Originally this indexing structure was called a 'LSM Tree' (Log-Structured Merge Tree).  We can
        think of an LSM Tree as the cascade of SSTables that are merged in the background.


    - Lucene, an indexing engine for full-text search and used by ElasticSearch and Solr, uses a
        similar method for storing its 'term dictionary'.  In this case, the key is a search term,
        and the value is a postings list (list of the ids of the documents that contain the term).

      In Lucene, the mapping from term to postings list is kept in SSTable-like sorted files.



- Performance Optimizations

    - LSM-tree algorithms can be slow when looking up keys that do not exist in the database, as you
        have to look through every segment.

      In order to optimize this, storage engines often use additional Bloom filters.  A 'Bloom filter'
        is a memory-efficient data structure for approximating the contents of a set.  It can tell you
        if a key doesn't appear in the database.


    - There are different strategies to determine the order and timing of how SSTables are compacted
        and merged.  The most common are 'size-tiered' and 'leveled'.

      In size-tiered compaction, newer and smaller SSTables are successively merged into older and
        larger SSTables.  In leveled compaction, the key range is split up into smaller SSTables
        and older data is moved into separate levels, which allows compaction to proceed more
        incrementally.

      HBase uses tier-sized, LevelDB and RocksDB used leveled, and Cassandra supports both.



- B-Trees

    - B-trees (1970) are by far the most widely used indexing structure.  They are the standard 
        index implementation in almost all relational databases, and many nonrelational ones.

      Like SSTables, BTrees keep key-value pairs sorted by key, allowing for efficient value
        lookups and range queries.  Otherwise, they are very different.  They use a 'write in place'
        philosophy as opposed to the append-only philosophy.



    - The log-structured indexes we saw earlier break the database down into variable-size segments, 
        typically several MB+ in size, and always write a segment sequentially.

      By contrast, B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB 
        in size (sometimes bigger), and read or write one page at a time. This design corresponds more 
        closely to the underlying hardware, as disks are also arranged in fixed-size blocks.


    - Each page can be identified using an address, which allows one page to refer to another.  This is
        similar to pointers, but they are stored on disk.

                    "Look up user_id = 251"

                ref | 100 | ref | 200 | ref | 300 | ref
                 |           |           |           |
                 v           v           v           v
                                 200 <= key < 300
                                         |
                                         v
                              ref | 250 | ref | 260 | ref | 270 | ref
                                           |
                                           v
                                          ...


    - One page is designated the root of the B-tree.  Whenever you want to look up a key in the index,
        you start there.  You keep traversing the tree until you find the key.

      Eventually, we get down to a page containing individual keys (a leaf page), which either contains
        the value or references to the pages where the values can be found.

      The number of references to child pages on a page is called the 'branching factor'.  (In this 
        example, it is 4)  It is typically several hundred.


    - If you want to update the value for an existing key, search the leaf for the leaf page containing
        that key, change the value in that page, and write the page back to disk.


    - If you want to add a new key, find the page whose range encompasses the new key and add it to
        the page.  If it's too large, split it into 2 pages.


    - This algorithm ensures that the tree always remains balanced.  A B-tree with n keys always has
        O(lg n) depth.  Most databases can fit into a B-tree that is 3 or 4 levels deep, so you don't
        have to follow too many page references.



- Making B-Trees Reliable

    - The basic underlying write is to overwrite a page on disk with new data, unlike LSM-trees which
        only append to files.  Since this is a hardware operation, things outside of your control
        can go wrong.


    - In order to make the database resilient to crashes, it is common for B-tree implementations to
        store an additional data structure on disk called a 'Write Ahead Log' ('WAL' or 'redo log').  

      This is an append-only file to which every B-tree modification must be written before it can be
        applied to the pages of the tree itself.  If the database crashes, this file is used to restore
        it to a consistent state.



- Comparing B-Trees and LSM-Trees

    - Generally, LSM-trees are faster for writes, while B-trees are faster for reads.  However, this
        can vary based on the details for the workload.

    - A B-tree must write every piece of data at least twice (WAL and page), and it has to write an
        entire page at a time.  We may want to avoid extra writes if we're using SSDs to avoid wearing
        them out.

    - With LSMs, the compaction process sometimes interferes with reads and writes.  You need 
        monitoring to ensure your compaction process is keeping up with writes.

    - With B-trees, every key exists in exactly one place in the index.  This aspect makes B-trees
        attractive in databases that want to provide strong transactional semantics.



- Secondary Indexes

    - So far, we have only discussed key-value indexes, which are like a primary key index in the
        relational model.  It's also very common to have 'secondary indexes'.  

      In relational databases, we can define indexes on non-primary-key columns, such as columns used
        in lookups and foreign key columns.  This is often essential for performant lookups and joins.


    - A secondary index can easily be constructed from a key-value index, with the caveat that keys 
        will not be unique.  This can be solved in 2 ways:

        1. Make each value in the index a list of matching row identifiers (like a full-text index)

        2. Make each key unique by appending a row identifier to it

      Either way, both B-trees and log-structured indexes can be used as secondary indexes.



- Storing Values Within an Index

    - The key is always the thing that queries search for, but the value can be one of two things: 

        1. The row/document/vertex in question
        2. A reference to the row stored elsewhere.


    - In the case where the row is stored elsewhere, rows are stored in a 'heap file', which store data
        in no particular order.  This approach is common, because if multiple secondary indexes are
        present, each can just point to a single coopy of the actual data.


    - When updating a value with a new value that is not larger than the old value, we can perform the
        update in place in the heap file.

      If the new value is larger, we likely need to move it somewhere else in the heap with more space.
        In this case, we need to update all of the indexes to point at the new location (or a 
        forwarding pointer is left in the old heap location).


    - In some cases, the extra hop from the index to the heap is too much of a performance penalty for
        reads, so we place a row directly within an index.  This is known as a 'clustered index'.

      A compromise between a clustered index (store all row data in the index) and a nonclustered index
        (just store a reference to the row) is called a 'covering index' (put some of the tables columns
        in the index, but not all).



- Multi Column Indexes

    - The indexes we have used so far only map a single key to a value.  There are also many cases
        in which we need to query multiple columns of a table simultaneously.


    - The most common type of multi-index column is called a 'concatenated index', which simply
        combines several fields by appending the column values together.

      Note that these indexes depend on order.  This is similar to lookups in a phone book.  You can
        look up (lastname, firstname) in a phone book, but there is no way to get all the people
        with a particular firstname using the index.


    - Multi-dimensional indexes are a more general way of querying several columns at once, which is
        particularly important for geospatial data.

        SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079
                                    AND longitude > -0.1162 AND longitude < -0.1004;

      A standard B-tree or LSM-tree index can give you all the restaurants in a range of latitudes,
        or a range of longitudes, but not both.


    - One option is to translate a 2D location into a single number using a space-filling curve, then
        just use a regular B-tree index.

      More commonly, specialized spatial indexes such as R-trees are used.  For example, PostGIS
        implements geospatial indexes using R-trees along with Postgres's 'Generalized Search Tree'
        indexing facility.


    - There are other interesting applications for multi-dimensional indexes such as searching by
        color with (r, g, b) or using 2D weather data with (date, temperature).



- Full Text Search and Fuzzy Indexes

    - All the indexes used so far assume you have the exact key you're querying for.  It doesn't
        allow you to search for similar keys such as misspelled words.  This 'fuzzy' querying
        requires different techniques.


    - For instance, Lucene can search for words within a certain 'edit distance' (the number of
        characters different).

      It does this by keeping an in-memory index, which is a finite state automaton over the 
        characters in the keys, similar to a 'trie'.  This automaton can be transferred into a
        Levenshtein automaton, which supports efficient search for words within a given edit
        distance.



- Keeping Everything in Memory

    - We store things on disk because disks are durable and cheaper.  However, memory keeps getting
        cheaper, so the cost difference is always eroding.  Sometimes, it makes sense to use an
        'in-memory database'.

    - For instance, Memcached is intended to be used for caching only, so it's acceptable for data to
        be lost if a machine restarts.


    - Some special systems make in-memory data durable by:

        - Using special hardware like battery-powered RAM
        - Writing a log of changes to disk
        - Writing periodic snapshots to disk
        - Replicating the in-memory state to other machines


    - Note that conterintuitively, the performance advantage we get from using in-memory databases
        is not due to the fact that you don't have to read from disk.  Even in a disk-based storage
        engine, we may never need to read from disk if we have enough memory, since the O/S caches
        recently used disk blocks in memory anyways.

      Actually, they are faster because they avoid the overheads of encoding in-memory data structures
        in a way that can be written to disk.



- Transaction Processing vs Analytics

    - In the early days of databases, a write to a database typically corresponded to a commercial
        transaction, and the term transaction has stuck ever since.


    - Even as relational databases started being used for many different kinds of data, the basic
        access pattern remained similar:

        1. An application looks up a small number of records by key, using an index.
        2. Records are inserted or updated based on the user's input.

      Because these applications are interactive, the access pattern became known as OLTP
        (Online Transaction Processing).


    - However, databases started being used increasingly for data analysis, which has very different
        access patterns.

      Usually, an analytic query needs to scan over a huge number of records, only reading a few
        columns per record, and calculate aggregate statistics (ie count, sum, average).

        - What was the total revenue of each of our stores in January?
        - How many more bananas than usual did we sell during our latest promotion?
        - Which brand of baby food is most often purchased together with brand X diapers?


    - Analytics queries are often written by business analysts, and feed into reports that help
        the management of the company make better decisions (business intelligence).

      These systems are often called OLAP (Online Analytic Processing).


    - At first, the same database was used for transaction processing and analytic queries.  SQL
        is flexible enough to be used for both.  In the late 1980s and early 1990s, it became 
        popular to run analytics in a separate database instead.  This database was called a
        'data warehouse'.



- OLTP vs OLAP

    Property                   OLTP                                OLAP
    ------------------------------------------------------------------------------------------------------
    Main read pattern          Small number of records by key      Aggregate over large number of records
    Main write pattern         Random-access from user input       Bulk ETL or event stream
    Primary user               End user/customer                   Internal analyst for decision support
    What data represents       Latest state of data (current)      History of events over time
    Dataset size               GB to TB                            TB to PB



- Data Warehousing

    - An enterprise may have dozens of different transaction processing systems:

        - Systems powering the customer-facing website
        - POS Systems
        - Inventory tracking systems
        - Planning routes for vehicles
        - Managing suppliers
        - HR managenent
        - etc...

      Each of these systems is complex and needs a team of people to manage it, so they operate
        mostly autonomously.


    - The OLTP systems need to be highly available and be able to process transactions with low 
        latency.  DBAs guard them, and don't want analysts running ad hoc queries on them.


    - The data warehouse contains a copy of all the data in all the various OLTP systems in the
        company.  Data is extracted from OLTP databases (either a periodic dump or a continuous stream
        of updates), transformed into an analysis-friendly schema, cleaned up, and loaded into the
        data warehouse.  This process is known as 'ETL'.



- The Divergence Between OLTP databases and Data Warehouses

    - On the surface, OLTP and OLAP databases look similar, since they both provide a SQL-like 
        interface.  However, the internals are quite different.  Most vendors now focus on supporting
        either transaction processing or analytics workloads, but not both.

    - Some databases, like Sql Server and SAP HANA, have support for both.  However, underneath the 
        hood they are using separate engines.


    - Data warehouse vendors sell their systems under expensive commercial licenses.  These include:

        - Teradata
        - Vertica
        - SAP HANA
        - ParAccel
        - AWS Redshift (hosted version of ParAccel)


    - More recently, a plethora of open source SQL-on-Hadoop projects have emerged.  They aim to compete
        with commercial products:

        - Apache Hive
        - Spark SQL
        - Cloudera Impala
        - Facebook Presto
        - Apache Tajo
        - Apache Drill



- Stars and Snowflakes - Schemas for Analytics

    - Most data warehouses use a fairly standard style known as a 'star schema' (aka 'dimensional
        modeling').

    - At the center of the star schema is the 'fact table'.  Each row in the fact table represents
        an event that occurred at a particular time.  It could be a product purchase or a page
        view or click by a user.


    - Some of the columns in the fact table are attributes, such as the price at which the item was
        sold.  Other columns are foreign key references to other tables, called 'dimension tables'.
        These represent the who, what, where, why, when, and how of the event.

      For example, each row in the 'fact_sales' table uses a foreign key to indicate which product
        was bought.


    - Even things like date and time are often represented using dimension tables, since you can 
        encode additional metadata, like whether it was a holiday.


    - A variation of this template is known as the 'snowflake schema', where dimensions are further
        broken down into subdimensions.  For instance, the 'dim_product' table could store the
        brand and category as foreign keys instead of storing them as strings.

      Snowflake schemas are more normalized, but star schemas are often preferred because they are
        simpler for analysts to work with.


    - In a typical data warehouse, tables are very wide.  A fact table may have 100 to several 
        hundred columns, and the dimension tables may have a lot of detail also.



- Column-Oriented Storage

    - If you have trillions of rows and PB of data in your fact tables, storing and querying them
        efficiently becomes a challenging problem (note that dimension tables are usually much smaller,
        ie millions of rows).


    - Although fact tables are often over 100 columns wide, you are often only accessing 4 or 5 of
        them at a time.

      In OLTP databases, a row is stored as a contiguous series of bytes.  In document databases, the
        entire document is contiguous.  The idea behind column-oriented storage is that we store
        an entire column together instead.  This way we only grab the columns we need, which can save
        a lot of work.


    - The column-oriented storage layout relies on each column file containing the rows in the same
        order.  If you need to reassemble an entire row, you take the 23rd entry of each column, for
        instance.


    - Columnar storage is easiest to understand in a relational data model, but it applies equally to
        nonrelational data.  For instance, Parquet is a columnar storage format that supports a 
        document data format, based on Google's Dremel.



- Column Compression

    - In addition to only loading the required columns, we can also reduce demands on disk by 
        compressing data.  Column-oriented storage lends itself very well to compression, since
        the data in a column tends to be very similar.


    - One technique that works well is 'bitmap encoding', which works well if the number of distinct
        values is low.  This can make the encoding of a column remarkably compact.

        value = 29
        Bit value: 00 0000 0001 0000 0000
        Bitmap-indexed: 9,1 (nine 0s, one 1, rest 0s)


    - For instance, if we run this query:

        WHERE product_sk IN (30, 68, 69)

      We can load the 3 bitmaps for 30, 68, and 69.  Then we can calculate the bitwise OR of the 3
        bitmaps.  



- Column-Oriented Storage and Column Families

    - Cassandra and HBase have a concept of 'column families' which is inherited from BigTable.
        However, it is misleading to call them column-oriented.

    - Within each column family, they store all columns from a row together, along with a row key,
        and still do not use column compression.  So, the BigTable model is still mostly row-oriented.



- Memory Bandwidth and Vectorized Processing

    - For data warehouse queries that need to scan over millions of rows, getting data from disk to
        memory is a big bottleneck.  Developers of analytical databases also worry about the 
        bottleneck between mamory and CPU, avoiding branch mispredictions, and using SIMD instructions
        in modern CPUs.


    - Since compressed columns allow more data to fit in the CPU's L1 cache, they are good for making
        efficient use of CPU cycles.  

      Operators such as the bitwise AND and OR can be designed to operate on the compressed chunks
        directly.  This is known as 'vectorized processing'.



- Sort Order in Column Storage

    - In a column store, it doesn't necessarily matter what order the rows are stored in.  If we
        just store them in the order they were inserted, we can just append to each of the column
        files.

    - However, we can choose to impose an order, like we did with SSTables previously, and use that
        as an indexing mechanism.  Note that we have to sort all the columns at the same time.

    - We can use a first sort key to sort the rows, then a second sort key for any rows that had the
        same value for the first key.

    - C-Store and Vertica store the same data sorted in several different ways to speed up different
        types of queries.  Since data has to be replicated on multiple machines for redundancy 
        anyways, just store it sorted different ways on different machines.



- Writing to Column-Oriented Storage

    - All of these optimizations make sense since most of the data consists of large read-only queries
        run by analysts.  Column-oriented storage, compression, and sorting all make reads faster.
        However, they make writes more difficult.


    - If you want to insert a row in the middle of a sorted table, you have to rewrite all the column
        files.

      The solution to this is to use an LSM-tree approach where we store all data in memory, sorted, 
        before writing it to disk.  When enough writes have accumulated, they are merged with column 
        files on disk and written to new files in bulk.



- Aggregation: Data Cubes and Materialized Views

    - Data warehouses queries often involve an aggregate function (ie COUNT, SUM, AVG, MIN, MAX).
        If the same aggregates are used by many different queries, it makes sense to cache some of
        the counts or sums that are used most often.

    - One way to create such a cache is with a 'materialized view', which is a copy of query results
        written to disk for use later.  Note that the view must be updated in some way when data
        changes.  This often makes more sense in read-heavy data warehouses.


    - A 'data cube' (aka 'OLAP cube') is a common special case of materialized view.  It is a grid of
        aggregate counts grouped by different dimensions.

      For instance, if you have a 'fact_sales' table with dimension tables 'date_key' and 'product_sk',
        you can draw a 2D table with dates on one axis and products on another.  The same concept
        can be extended into a hypercube with multiple dimensions.

      This makes certain queries fast since they have been partially precomputed.  The disadvantage is
        that you lose the flexibility of querying the raw data.
-----------------------------------------------------------------------------
| CHAPTER 3 - STORAGE AND RETRIEVAL                                         |
-----------------------------------------------------------------------------

- World's Simplest Database

    - Here is the database:

        #!/bin/bash

        db_set () {
            echo "$1,$2" >> database
        }

        db_get () {
            grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
        }


    - To use it:

        $ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'
        $ db_get 42


    - Appending to our database (append-only log file) is very fast.  Lookups, however, have
        O(n) performance.


    - In order to implement lookups efficiently, we need an index, an additional data structure
        derived from primary data.  Basically, we keep some metadata on the side which helps locate
        the data we want.

      Note that maintaining additional structures does increase overhead, especially on writes.



- Hash indexes

    - Key/value stores are similar to dictionaries in PLs, and are usually implemented as hash maps.


    - For instance, we can just keep an in-memory hash map of where every key is mapped to a byte
        offset in the data file.  Every time we append a new key/value pair to the file, we update
        the hash map with the new key and offset.

      This is essentially what BitCask (the default storage engine in Riak) does.


    - Note that all of the available keys must fit in memory.  This approach is well suited to 
        applications where the value of each key is updated frequently.


    - We also have to solve the problem of running out of disk space if we only ever append to the
        log.  To solve this, we can break the log into segments and close a file when it reaches
        a certain size.

      Then, we can use compaction (throw away duplicate keys in the log, only keeping the latest
        update) on these segments.  This can be done in a background thread while we are able to
        read and write requests as normal.


    - If you want to delete a key/value pair, you have to append a special deletion record (a tombstone).
        When the log segments are merged, the tombstone tells the merging process to discard any
        previous values for the key.


    - In a crash, the hash map is lost, and all of the log files must be read to restore it.  We can
        mitigate this by saving periodic snapshots of the hash map to disk.


    - Since writes are appended in a strictly sequential order, it is common to have just a single
        writer thread.  Since data files are append-only and immutable, they can be read concurrently
        by multiple threads.


    - Range queries are not efficient.  In order to scan over all keys 0000-9999, you have to look
        up each key separately.



- SSTables and LSM-Trees

    - The first change we make is to keep our segment files sorted by key.  We call this format
        Sorted String Table, or SSTable.  We also require that each key only appears once in a 
        segment file (the compaction process already ensures this for multiple files).


    - Advantages of this:

        1. Merging segments is simple and efficient with a mergesort-like approach.

        2. No longer have to keep all keys in memory, since we can search through sorted files once we
             get close to the key.  We still need an in-memory index for some of the keys.

        3. We can compress blocks of key/value pairs to save disk space and reduce I/O if desired.



- Constructing and Maintaining SSTables

    - Now, we think about how we keep our data sorted by key in the first place.  The approach involves
        using an in-memory sorted tree structure like a Red-Black Tree or AVL tree.


    - So, our storage engine works as follows:

        1. When a write comes in, add it to our in-memory Red-Black tree.  This in-memory tree is
             sometimes called a 'memtable'.

        2. When a memtable gets bigger than some threshold (ie a few MB), we write it out to disk as
             an SSTable file.  This can be done efficiently, since we already have the key-value pairs
             sorted by key in our tree.  The new SSTable file becomes the most recent segment of the
             database.  While the SSTable is being written out to disk, writes can continue to a 
             new memtable instance.

        3. In order to serve a read request, first look in the memtable, then the most recent on-disk
             segment, then the next oldest, etc.

        4. From time to time, run a merging and compacting process on the background to combine
             segment files.


    - One problem here is that if the database crashes, we lose the writes in the current memtable.
        To solve this problem, we can just keep a separate log on disk we append every write to 
        immediately.

      It is not sorted, but that doesn't matter, since it's only used for recovery.  We can discard
        the log every time we write the memtable to disk.



- Making an LSM-tree out of SSTables

    - This simple algorithm is used by LevelDB and RocksDB, simple key-value storage engine libraries
        meant to be embedded in other applications.  LevelDB can also be used as a storage engine
        for Riak.

    - Similar storage engines are used in Cassandra and HBase, both of which were inspired by Google's
        BigTable paper (which introduced the terms 'SSTable' and 'memtable').

    - Originally this indexing structure was called a 'LSM Tree' (Log-Structured Merge Tree).  We can
        think of an LSM Tree as the cascade of SSTables that are merged in the background.


    - Lucene, an indexing engine for full-text search and used by ElasticSearch and Solr, uses a
        similar method for storing its 'term dictionary'.  In this case, the key is a search term,
        and the value is a postings list (list of the ids of the documents that contain the term).

      In Lucene, the mapping from term to postings list is kept in SSTable-like sorted files.



- Performance Optimizations

    - LSM-tree algorithms can be slow when looking up keys that do not exist in the database, as you
        have to look through every segment.

      In order to optimize this, storage engines often use additional Bloom filters.  A 'Bloom filter'
        is a memory-efficient data structure for approximating the content of a set.  It can tell you
        if a key doesn't appear in the database.


    - There are different strategies to determine the order and timing of how SSTables are compacted
        and merged.  The most common are 'size-tiered' and 'leveled'.

      In size-tiered compaction, newer and smaller SSTables are successively merged into older and
        larger SSTables.  In leveled compaction, the key range is split up into smaller SSTables
        and older data is moved into separate levels, which allows compaction to proceed more
        incrementally.

      HBase uses tier-sized, LevelDB and RocksDB used leveled, and Cassandra supports both.



- B-Trees

    - B-trees (1970) are by far the most widely used indexing structure.  They are the standard 
        index implementation in almost all relational databases, and many nonrelational ones.

      Like SSTables, BTrees keep key-value pairs sorted by key, allowing for efficient value
        lookups and range queries.  Otherwise, they are very different.


    - The log-structured indexes we saw earlier break the database down into variable-size segments, 
        typically several MB+ in size, and always write a segment sequentially.

      By contrast, B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB 
        in size (sometimes bigger), and read or write one page at a time. This design corresponds more 
        closely to the underlying hardware, as disks are also arranged in fixed-size blocks.


    - Each page can be identified using an address, which allows one page to refer to another.  This is
        similar to pointers, but they are stored on disk.

                    "Look up user_id = 251"

                ref | 100 | ref | 200 | ref | 300 | ref
                 |           |           |           |
                 v           v           v           v
                                 200 <= key < 300
                                         |
                                         v
                              ref | 250 | ref | 260 | ref | 270 | ref
                                           |
                                           v
                                          ...


    - One page is designated the root of the B-tree.  Whenever you want to look up a key in the index,
        you start there.  You keep traversing the tree until you find the key.

      Eventually, we get down to a page containing individual keys (a leaf page), which either contains
        the value or references to the pages where the values can be found.

      The number of references to child pages on a page is called the 'branching factor'.  (In this 
        example, it is 4)  It is typically several hundred.


    - If you want to update the value for an existing key, search the leaf for the leaf page containing
        that key, change the value in that page, and write the page back to disk.


    - If you want to add a new key, find the page whose range encompasses the new key and add it to
        the page.  If it's too large, split it into 2 pages.


    - This algorithm ensures that the tree always remains balanced.  A B-tree with n keys always has
        O(lg n) depth.  Most databases can fit into a B-tree that is 3 or 4 levels deep, so you don't
        have to follow too many page references.



- Making B-Trees Reliable

    - The basic underlying write is to overwrite a page on disk with new data, unlike LSM-trees which
        only append to files.  Since this is a hardware operation, things outside of your control
        can go wrong.


    - In order to make the database resilient to crashes, it is common for B-tree implementations to
        store an additional data structure on disk called a 'Write Ahead Log' ('WAL' or 'redo log').  

      This is an append-only file to which every B-tree modification must be written before it can be
        applied to the pages of the tree itself.  If the database crashes, this file is used to restore
        it to a consistent state.



- Comparing B-Trees and LSM-Trees

    - Generally, LSM-trees are faster for writes, while B-trees are faster for reads.  However, this
        can vary based on the details for the workload.

    - A B-tree must write every piece of data at least twice (WAL and page), and it has to write an
        entire page at a time.  We may want to avoid extra writes if we're using SSDs to avoid wearing
        them out.

    - With LSMs, the compaction process sometimes interferes with reads and writes.  You need 
        monitoring to ensure your compaction process is keeping up with writes.

    - With B-trees, every key exists in exactly one place in the index.  This aspect makes B-trees
        attractive in databases that want to provide strong transactional semantics.



- Secondary Indexes

    - So far, we have only discussed key-value indexes, which are like a primary key index in the
        relational model.  It's also very common to have 'secondary indexes'.  

      In relational databases, we can define indexes on non-primary-key columns, such as columns used
        in lookups and foreign key columns.  This is often essential for performant lookups and joins.


    - A secondary index can easily be constructed from a key-value index, with the caveat that keys 
        will not be unique.  This can be solved in 2 ways:

        1. Make each value in the index a list of matching row identifiers (like a full-text index)

        2. Make each key unique by appending a row identifier to it

      Either way, both B-trees and log-structured indexes can be used as secondary indexes.



- Storing Values Within an Index

    - The key is always the thing that queries search for, but the value can be one of two things: 

        1. The row/document/vertex in question
        2. A reference to the row stored elsewhere.


    - In the case where the row is stored elsewhere, rows are stored in a 'heap file', which store data
        in no particular order.  This approach is common, because if multiple secondary indexes are
        present, each can just point to a single coopy of the actual data.


    - When updating a value with a new value that is not larger than the old value, we can perform the
        update in place in the heap file.

      If the new value is larger, we likely need to move it somewhere else in the heap with more space.
        In this case, we need to update all of the indexes to point at the new location (or a 
        forwarding pointer is left in the old heap location).


    - In some cases, the extra hop from the index to the heap is too much of a performance penalty for
        reads, so we place a row directly within an index.  This is known as a 'clustered index'.

      A compromise between a clustered index (store all row data in the index) and a nonclustered index
        (just store a reference to the row) is called a 'covering index' (put some of the tables columns
        in the index, but not all).



- Multi Column Indexes

    - The indexes we have used so far only map a single key to a value.  There are also many cases
        in which we need to query multiple columns of a table simultaneously.


    - The most common type of multi-index column is called a 'concatenated index', which simply
        combines several fields by appending the column values together.

      Note that these indexes depend on order.  This is similar to lookups in a phone book.  You can
        look up (lastname, firstname) in a phone book, but there is no way to get all the people
        with a particular firstname using the index.


    - Multi-dimensional indexes are a more general way of querying several columns at once, which is
        particularly important for geospatial data.

        SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079
                                    AND longitude > -0.1162 AND longitude < -0.1004;

      A standard B-tree or LSM-tree index can give you all the restaurants in a range of latitudes,
        or a range of longitudes, but not both.


    - One option is to translate a 2D location into a single number using a space-filling curve, then
        just use a regular B-tree index.

      More commonly, specialized spatial indexes such as R-trees are used.  For example, PostGIS
        implements geospatial indexes using R-trees along with Postgres's 'Generalized Search Tree'
        indexing facility.


    - There are other interesting applications for multi-dimensional indexes such as searching by
        color with (r, g, b) or using 2D weather data with (date, temperature).



- Full Text Search and Fuzzy Indexes

    - All the indexes used so far assume you have the exact key you're querying for.  It doesn't
        allow you to search for similar keys such as misspelled words.  This 'fuzzy' querying
        requires different techniques.


    - For instance, Lucene can search for words within a certain 'edit distance' (the number of
        characters different).

      It does this by keeping an in-memory index, which is a finite state automaton over the 
        characters in the keys, similar to a 'trie'.  This automaton can be transferred into a
        Levenshtein automaton, which supports efficient search for words within a given edit
        distance.



- Keeping Everything in Memory

    - We store things on disk because disks are durable and cheaper.  However, memory keeps getting
        cheaper, so the cost difference is always eroding.  Sometimes, it makes sense to use an
        'in-memory database'.

    - For instance, Memcached is intended to be used for caching only, so it's acceptable for data to
        be lost if a machine restarts.


    - Some special systems make in-memory data durable by:

        - Using special hardware like battery-powered RAM
        - Writing a log of changes to disk
        - Writing periodic snapshots to disk
        - Replicating the in-memory state to other machines


    - Note that conterintuitively, the performance advantage we get from using in-memory databases
        is not due to the fact that you don't have to read from disk.  Even in a disk-based storage
        engine, we may never need to read from disk if we have enough memory, since the O/S caches
        recently used disk blocks in memory anyways.

      Actually, they are faster because they avoid the overheads of encoding in-memory data structures
        in a way that can be written to disk.

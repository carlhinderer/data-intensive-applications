-----------------------------------------------------------------------------
| CHAPTER 5 - REPLICATION                                                   |
-----------------------------------------------------------------------------

- Reasons to Distribute a Database Across Multiple Machines

    1. Scalability
    2. Fault tolerance & High availability
    3. Latency (especially for global audience)



- Scaling to Higher Load

    - In a 'shared-memory architecture', we just add a lot of CPUs and memory to a single machine.  This
        approach will eventually hit practical limits.


    - In a 'shared-disk architecture', several machines share the same disk cluster, connected via a
        fast network.  This approach is used for some data warehousing workloads, but contention and
        the overhead of locking limit the scalability of this approach.


    - With a 'shared-nothing architecture', each machine is called a 'node'.  Any coordination between
        nodes is done at the software level, using a conventional network.



- Replication vs Partitioning

    - There are 2 common ways data is distributed across multiple nodes: replication and partitioning.


    - 'Replication' is keeping a copy of the same data on several different nodes, potentially in
        different locations.  Replication provides redundancy and can also help improve performance.


    - 'Partitioning' is splitting a big database into smaller subsets called 'partitions' so that
        different partitions can be assigned to different nodes (aka 'sharding').



- Resons to Replicate Data

    1. Keep data geographically close to users (to reduce latency)

    2. To allow system to continue working through node failures (to increase availability)

    3. Scale out number of machines that can serve read throughput (to increase read throughput)



- Leaders and Followers

    - In this chapter, we'll assume we have a dataset that can fit entirely on one machine.  Each
        node that stores a copy of the database is called a 'replica'.


    - Every write to the DB needs to be processed by every replica.  The most common solution for this
        is called 'leader-based replication' (aka 'active/passive' or 'master-slave replication').

        1. One of the replicas is designated the 'leader' (aka the 'master' or 'primary').  When clients
             want to write to the DB, they must send their requests to the leader, which first writes
             the data to it's local storage.

        2. The other replicas are known as 'followers' (aka 'read replicas', 'slaves', 'secondaries',
             'hot standbys').  Whenever the leader writes new data to its local storage, it also sends
             the data changes to each of its followers as part of a 'replication log' or 'change stream'.

           Each follower takes the log from the leader and updates its local copy accordingly, by 
             applying the writes in the same order.

        3. When a client wants to read from the database, it can query any of the followers.


    - This mode of replication is a built-in feature of many relational databases, such as Postgres,
        MySQL, Oracle, and SqlServer.  It is also used by nonrelational databases (like MongoDB and
        Espresso) and distributed message brokers (Kafka and RabbitMQ High Available Queues).



- Synchronous Versus Asynchronous Replication

    - An important detail is whether replication happens synchronously or asynchronously.  (In relational
        DBs, this is often configurable).


    - With 'synchronous' replication, the leader waits until a follower has confirmed it received the
        write before reporting success to the user and making the write visible to other clients.


    - With 'asynchronous' replication, the leader sends the message to the follower, but doesn't wait
        for a response.


    - The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date
        copy of the data that is consistent with the leader.  If the leader suddenly fails, we still
        have the data on any follower.

      The disadvantage is that if a synchronous follower doesn't respond, the leader must block all
        writes to wait for it to be available again.


    - For the latter reason, it is impractical for all followers to be synchronous.  Any one node 
        outage grinds the system to a halt.  In practice, if you enable synchronous replication, it
        means that one of the followers is synchronous and the others are asynchronous.  If the
        synchronous follower stops responding, one of the other asynchronous followers becomes the
        synchronous one.  This configuration is sometimes called 'semi-synchronous'.


    - Often, leader-based replication is configured to be completely asynchronous.  In this case, if the
        leader fails, any writes that have not been replicated are lost.  This means that a write
        is not guaranteed to be durable, even if it is confirmed to the client.

      However, the system can keep procesing writes even if all of it's followers have fallen behind.
        This might seem like a bad trade off, but it is widely used.



- Setting Up New Followers

    - From time to time you need to set up new followers, perhaps to increase the number of replicas
        or replace failed nodes.  To get the new follower a copy of the leader's data, copying files
        is usually not sufficient, because we won't be getting the changes that are accumulating.


    - We could lock the database to make a copy, but we don't want the downtime.  Instead, we use this
        process:

        1. Take a consistent snapshot of the leader's database at some point in time.  We should be
             able to do this without locking the database, since this feature is also required for
             taking backups.

        2. Copy the snapshot to the new follower node.

        3. The follow connects to the leader and requests all changes that have occurred since the
             snapshot was taken.  This requires knowing the exact position where the snapshot was
             taken in the leader's replication log (this is known as the 'log sequence number' in
             Postgres for instance).

        4. Once the follower has processed all changes, it has caught up.



- Handling Node Outages

    - If a follower crashes or is restarted, or their connection to the leader is temporary lost,
        the follower can recover quite easily.  They know from their log the last transaction they
        processed, and can just request the changes since that point when they are reconnected to the
        leader.


    - Handling a leader failure is trickier.  One of the followers needs to be promoted to the leader,
        and clients and followers need to know who the new leader is.  The 'failover' process looks
        like this:

        1. Determining that the leader has failed.  Maybe this happens if it stops responding for 30
             seconds.

        2. Choosing a new leader.  This could be done via an election process or by some controller
             node.

        3. Reconfigure the system to use the new leader.  Clients need to send write requests to the 
             new leader.  Followers need to know who the new leader is.  If the dead leader comes back
             online, it needs to know it's not the leader any more.


    - Failover is frought with things that can go wrong:

        1. If asynchronous replication is used, the new leader might not have gotten all the writes
             from the original leader.  If the old leader rejoins the cluster, what should happen to
             those writes?  We could choose to simply discard them, but this might violate durability
             guarantees.

        2. What if discarding writes leads to some other downstream problem.  For instance, if they
             cause a SQL DB's auto-incrementing primary keys to be off?

        3. If 2 nodes both believe they are the leader ('split brain'), they might both be accepting
             writes with no way to resolve conflicts.  Some systems will actually shut down if 2
             leaders are detected.

        4. What is the right timeout before a leader is declared dead?  If the system is just slow
             due to network problems, an unnecessary failover is going to make things worse.  For this
             reason, many teams prefer to perform failovers manually even if an automated way is
             possible.



- Implementation of Replication Logs

    - Statement-Based Replication

        - In the simplest case, the leader logs every write request (statement) that it executes, and
            sends that statement log to its followers.  In a relational DB, this means that every
            INSERT, UPDATE, and DELETE statement is forwarded to followers.


        - Problems:

            1. Nondeterministic functions like 'NOW()' or 'RAND()' will generate a different value on
                 each replica.

            2. Auto-incrementing columns will probably generate different values.

            3. Statements with side effects (ie triggers or stored procedures) may result in different
                 side effects on each replica.


        - Used to be used by old versions of MySQL.  VoltDB uses it safely by requiring transactions
            to be deterministic.



    - Write-Ahead Log (WAL) Shipping

        - In Chapter 3, we discussed how every write is usually appended to a log:

             1. With SSTables, the log itself
             2. With B-Trees, every modification is first written to a Write-Ahead Log


        - In either case, the log is an append-only sequence of bytes containing all writes to the
            database.  We can just send this entire log to followers, and they can build a copy
            of the exact same data structures as the leader.


        - This method is used by Postgres and Oracle, among others.


        - The main disadvantage is that the log describes the data on a very low level: which bytes
            changed an on which disk blocks.  This means replication is very closely coupled to the
            storage engine.  If a new version of the database is released, upgrading it might
            require downtime.



    - Logical (Row-Based) Replication

        - Another options is to use different log formats for replication and storage, which decouples
            the storage internals.  This kind of replication log is called a 'logical log' to 
            distinguish it from the physical data representation.


        - A logical log for a relational DB is usually a sequence of records describing database writes
            to tables at the granularity of a row.

            1. For an inserted row, all columns are contained.

            2. For a deleted row, just the information to identify the row are included.

            3. For an updated row, need the identifying information and the values of changed columns.


        - This method can more easily be kept backwards compatible, and this format is easier for
            external applications to parse.  This is useful if you want to send the contents of the
            database to an external system like a data warehouse (a technique called 'change data
            capture').



    - Trigger-Based Replication

        - All of the methods so far are implemented by the database system, without involving any
            application code.  You might need application code, for instance, if you only want to
            replicate a subset of the data, need to do some kind of translation, or need some kind of
            conflict resolution.


        - Some tools like Oracle GoldenGate make changes available to an application by reading the
            database log.


        - Another option is to use triggers and stored procedures that can log changes to another table,
            to be read by external processes.  'Databus' for Oracle and 'Bucardo' for Postgres work
            like this.  This approach has high overhead but allows for maximum flexibility.
            
-----------------------------------------------------------------------------
| CHAPTER 5 - REPLICATION                                                   |
-----------------------------------------------------------------------------

- Reasons to Distribute a Database Across Multiple Machines

    1. Scalability
    2. Fault tolerance & High availability
    3. Latency (especially for global audience)



- Scaling to Higher Load

    - In a 'shared-memory architecture', we just add a lot of CPUs and memory to a single machine.  This
        approach will eventually hit practical limits.


    - In a 'shared-disk architecture', several machines share the same disk cluster, connected via a
        fast network.  This approach is used for some data warehousing workloads, but contention and
        the overhead of locking limit the scalability of this approach.


    - With a 'shared-nothing architecture', each machine is called a 'node'.  Any coordination between
        nodes is done at the software level, using a conventional network.



- Replication vs Partitioning

    - There are 2 common ways data is distributed across multiple nodes: replication and partitioning.


    - 'Replication' is keeping a copy of the same data on several different nodes, potentially in
        different locations.  Replication provides redundancy and can also help improve performance.


    - 'Partitioning' is splitting a big database into smaller subsets called 'partitions' so that
        different partitions can be assigned to different nodes (aka 'sharding').



- Resons to Replicate Data

    1. Keep data geographically close to users (to reduce latency)

    2. To allow system to continue working through node failures (to increase availability)

    3. Scale out number of machines that can serve read throughput (to increase read throughput)



- Leaders and Followers

    - In this chapter, we'll assume we have a dataset that can fit entirely on one machine.  Each
        node that stores a copy of the database is called a 'replica'.


    - Every write to the DB needs to be processed by every replica.  The most common solution for this
        is called 'leader-based replication' (aka 'active/passive' or 'master-slave replication').

        1. One of the replicas is designated the 'leader' (aka the 'master' or 'primary').  When clients
             want to write to the DB, they must send their requests to the leader, which first writes
             the data to it's local storage.

        2. The other replicas are known as 'followers' (aka 'read replicas', 'slaves', 'secondaries',
             'hot standbys').  Whenever the leader writes new data to its local storage, it also sends
             the data changes to each of its followers as part of a 'replication log' or 'change stream'.

           Each follower takes the log from the leader and updates its local copy accordingly, by 
             applying the writes in the same order.

        3. When a client wants to read from the database, it can query any of the followers.


    - This mode of replication is a built-in feature of many relational databases, such as Postgres,
        MySQL, Oracle, and SqlServer.  It is also used by nonrelational databases (like MongoDB and
        Espresso) and distributed message brokers (Kafka and RabbitMQ High Available Queues).



- Synchronous Versus Asynchronous Replication

    - An important detail is whether replication happens synchronously or asynchronously.  (In relational
        DBs, this is often configurable).


    - With 'synchronous' replication, the leader waits until a follower has confirmed it received the
        write before reporting success to the user and making the write visible to other clients.


    - With 'asynchronous' replication, the leader sends the message to the follower, but doesn't wait
        for a response.


    - The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date
        copy of the data that is consistent with the leader.  If the leader suddenly fails, we still
        have the data on any follower.

      The disadvantage is that if a synchronous follower doesn't respond, the leader must block all
        writes to wait for it to be available again.


    - For the latter reason, it is impractical for all followers to be synchronous.  Any one node 
        outage grinds the system to a halt.  In practice, if you enable synchronous replication, it
        means that one of the followers is synchronous and the others are asynchronous.  If the
        synchronous follower stops responding, one of the other asynchronous followers becomes the
        synchronous one.  This configuration is sometimes called 'semi-synchronous'.


    - Often, leader-based replication is configured to be completely asynchronous.  In this case, if the
        leader fails, any writes that have not been replicated are lost.  This means that a write
        is not guaranteed to be durable, even if it is confirmed to the client.

      However, the system can keep procesing writes even if all of it's followers have fallen behind.
        This might seem like a bad trade off, but it is widely used.



- Setting Up New Followers

    - From time to time you need to set up new followers, perhaps to increase the number of replicas
        or replace failed nodes.  To get the new follower a copy of the leader's data, copying files
        is usually not sufficient, because we won't be getting the changes that are accumulating.


    - We could lock the database to make a copy, but we don't want the downtime.  Instead, we use this
        process:

        1. Take a consistent snapshot of the leader's database at some point in time.  We should be
             able to do this without locking the database, since this feature is also required for
             taking backups.

        2. Copy the snapshot to the new follower node.

        3. The follow connects to the leader and requests all changes that have occurred since the
             snapshot was taken.  This requires knowing the exact position where the snapshot was
             taken in the leader's replication log (this is known as the 'log sequence number' in
             Postgres for instance).

        4. Once the follower has processed all changes, it has caught up.



- Handling Node Outages

    - If a follower crashes or is restarted, or their connection to the leader is temporary lost,
        the follower can recover quite easily.  They know from their log the last transaction they
        processed, and can just request the changes since that point when they are reconnected to the
        leader.


    - Handling a leader failure is trickier.  One of the followers needs to be promoted to the leader,
        and clients and followers need to know who the new leader is.  The 'failover' process looks
        like this:

        1. Determining that the leader has failed.  Maybe this happens if it stops responding for 30
             seconds.

        2. Choosing a new leader.  This could be done via an election process or by some controller
             node.

        3. Reconfigure the system to use the new leader.  Clients need to send write requests to the 
             new leader.  Followers need to know who the new leader is.  If the dead leader comes back
             online, it needs to know it's not the leader any more.


    - Failover is frought with things that can go wrong:

        1. If asynchronous replication is used, the new leader might not have gotten all the writes
             from the original leader.  If the old leader rejoins the cluster, what should happen to
             those writes?  We could choose to simply discard them, but this might violate durability
             guarantees.

        2. What if discarding writes leads to some other downstream problem.  For instance, if they
             cause a SQL DB's auto-incrementing primary keys to be off?

        3. If 2 nodes both believe they are the leader ('split brain'), they might both be accepting
             writes with no way to resolve conflicts.  Some systems will actually shut down if 2
             leaders are detected.

        4. What is the right timeout before a leader is declared dead?  If the system is just slow
             due to network problems, an unnecessary failover is going to make things worse.  For this
             reason, many teams prefer to perform failovers manually even if an automated way is
             possible.



- Implementation of Replication Logs

    - Statement-Based Replication

        - In the simplest case, the leader logs every write request (statement) that it executes, and
            sends that statement log to its followers.  In a relational DB, this means that every
            INSERT, UPDATE, and DELETE statement is forwarded to followers.


        - Problems:

            1. Nondeterministic functions like 'NOW()' or 'RAND()' will generate a different value on
                 each replica.

            2. Auto-incrementing columns will probably generate different values.

            3. Statements with side effects (ie triggers or stored procedures) may result in different
                 side effects on each replica.


        - Used to be used by old versions of MySQL.  VoltDB uses it safely by requiring transactions
            to be deterministic.



    - Write-Ahead Log (WAL) Shipping

        - In Chapter 3, we discussed how every write is usually appended to a log:

             1. With SSTables, the log itself
             2. With B-Trees, every modification is first written to a Write-Ahead Log


        - In either case, the log is an append-only sequence of bytes containing all writes to the
            database.  We can just send this entire log to followers, and they can build a copy
            of the exact same data structures as the leader.


        - This method is used by Postgres and Oracle, among others.


        - The main disadvantage is that the log describes the data on a very low level: which bytes
            changed an on which disk blocks.  This means replication is very closely coupled to the
            storage engine.  If a new version of the database is released, upgrading it might
            require downtime.



    - Logical (Row-Based) Replication

        - Another options is to use different log formats for replication and storage, which decouples
            the storage internals.  This kind of replication log is called a 'logical log' to 
            distinguish it from the physical data representation.


        - A logical log for a relational DB is usually a sequence of records describing database writes
            to tables at the granularity of a row.

            1. For an inserted row, all columns are contained.

            2. For a deleted row, just the information to identify the row are included.

            3. For an updated row, need the identifying information and the values of changed columns.


        - This method can more easily be kept backwards compatible, and this format is easier for
            external applications to parse.  This is useful if you want to send the contents of the
            database to an external system like a data warehouse (a technique called 'change data
            capture').



    - Trigger-Based Replication

        - All of the methods so far are implemented by the database system, without involving any
            application code.  You might need application code, for instance, if you only want to
            replicate a subset of the data, need to do some kind of translation, or need some kind of
            conflict resolution.


        - Some tools like Oracle GoldenGate make changes available to an application by reading the
            database log.


        - Another option is to use triggers and stored procedures that can log changes to another table,
            to be read by external processes.  'Databus' for Oracle and 'Bucardo' for Postgres work
            like this.  This approach has high overhead but allows for maximum flexibility.



- Problems with Replication Lag

    - Being able to tolerate node failures is one reason for wanting replication.  Scalability and
        latency are the others.

      Leader-based replication requires all writes to go through a single node, but read queries can
        go to any node.  This works well for workloads that have only a small percentage of writes.
        We create lots of followers to handle all the reads, and the load is taken off of the leader
        (known as 'read-scaling architecture').


    - This read-scaling approach only practically works with asynchronous replication.  This means 
        that a client might see outdated information, since a given follower might not have gotten all
        the new writes.

      Since this inconsistency is just a temporary state, it is known as 'eventual consistency'.  Note 
        that the term 'eventual' is deliberately vague - the 'replication lag' might be a fraction of
        a second, or it might be several seconds or even minutes if the system is near capacity.



- Reading Your Own Writes

    - Many applications let a user submit some data, then show them the data that was just submitted.
        With asynchronous replication, the new data might not be there when they try to view it,
        leading to a confusing user experience.


    - In this situation, we need 'read-after-write consistency' (aka 'read-your-writes consistency').
        This guarantees that users will see their own writes.  There are a few ways we could do this:

        1. When reading something the user might be modifying, read it from the leader.  Otherwise,
             read it from a replica.  This implies you know if it's something the reader might have
             modified (like their own account).

        2. Maybe use other criteria for reading it from the leader.  For instance, we could read all
             writes made in the last minute from the leader.

        3. Client can send the timestamp of its last write, and the system can ensure replicas have
             updates at least until that timestamp.



- Monotonic Reads

    - With asynchronous followers, it is possible for a user to see things 'moving backwards in time'.
        For instance, they send a query to one follower, then they send another query to a follower 
        with greater lag.

      'Monotonic reads' is a guarantee that this kind of anomaly does not happen.  It's a guarantee
         between strong consistency and eventual consistency.  You may see an old value, but you
         won't see an old value overwrite a new one.


    - One way to achieve monotonic reads is to make sure each user always makes their reads from the
        same replica.  However, it that replica fails, the user's queries will need to be rerouted to
        another replica.



- Consistent Prefix Reads

    - Our third example of replication lag anomalies concerns violations of causality.  Imagine a 
        third party hears this conversation out of order:

        Mrs. Cake
          About ten seconds usually, Mr. Poons.

        Mr. Poons
          How far into the future can you see, Mrs. Cake?


    - We have Event B, which is dependent on Event A, but we receive Event B first, so it makes no
        sense.  Preventing this problem requires 'consistent prefix reads', which guarantees that if
        a series of writes happens in a certain order, anyone reading them will see them in the
        same order.


    - One solution is to make sure that any writes that are causually related to each other are
        written to the same partition.  In some applications, this cannot be done efficiently.



- Multi-Leader Replication

    - The downside of leader-based replication is that all writes must go through the single leader,
        and if that reader becomes unresponsive, all writes have to stop.  The 'multi-leader'
        configuration allows us to have multiple leaders.


    - Use cases for multi-leader replication:

        1. Multi-datacenter operation = Put a leader in each datacenter, the leaders communicate all
                                          writes to each other, same as single-leader within a 
                                          datacenter.  If data is changed at 2 datacenters
                                          concurrently, need conflict resolution.

        2. Clients with offline operation = App (ie calendar app) has local DB that acts as leader
                                              (it accepts write requests), and there is an 
                                              asynchronous replication between all devices with the
                                              calendar app.  CouchDB is designed for this kind of
                                              operation.

        3. Collaborative editing = Several people can edit a document simultaneously (ie Google Docs).
                                     When a user edits a document, their changes are applied to their
                                     local replica, then asynchronously replicated to the server and
                                     any other users.  Need to lock while an individual user performs
                                     writes, but can do this on every keystroke.



- Handling Write Conflicts

    - If 2 leaders both make accept writes to the same data, then try to asynchronously replicate to
        each other, a conflict will occur.  


    - In a single-leader database, the second writer will either block and wait for the first write to
        complete, or we can abort and force the user to retry the write.

      In a multi-leader database, the conflict might only be detected at a later time, so it may be
        too late to ask the user to resolve it.


    - The first strategy for dealing with this problem is avoiding conflicts in the first place.  For
        instance, we could guarantee all writes for a particular record go through the same leader.
        We could implement this by giving each user a 'home datacenter'.

      Note that if you want to change a leader on the fly (for instance, if one goes down), this 
        strategy will break down.



- Converging Toward a Consistent State

    - 






- Multi-Leader Replication Topologies


- Leaderless Replication


- Writing to the Database When a Node is Down


- Limitations of Quorum Consistency


- Sloppy Quorums and Hinted Handoff


- Detecting Concurrent Writes
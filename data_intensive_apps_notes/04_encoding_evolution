-----------------------------------------------------------------------------
| CHAPTER 4 - ENCODING AND EVOLUTION                                        |
-----------------------------------------------------------------------------

- Encoding and Evolution

    - Changing an application's features often leads to a change in the data that is stored.
        Relational databases assume all the data conforms to one schema, and the schema can
        be changed with migrations (ie ALTER statements).  There is always exactly one schema.

    - Schema-on-read databases don't enforce a schema, so the database can contain a mixture of 
        older and newer data formats written at different times.


    - When a schema changes, a corresponding change to application code also needs to happen.
        However, in a large application, this might not happen instantaneously.

        1. With server-side applications, you may want to perform a rolling upgrade.
        2. With client-side applications, the user will upgrade whenever they feel like it.


    - Since old and new versions of code, and potentially old and new data formats, may all exist
        in the system at the same time, we need to maintain compatibility in both directions.

        Backward compatibility = newer code can read data written by older code
        Forward compatibility = older code can read data written by newer code

      Backward compatibility is relatively straightforward, since you know the old data format.
        Forward compatibility can be trickier, since it requires older code to ignore changes
        made by newer versions.



- Formats for Encoding Data

    - Programs usually work with data in at least 2 different representations:

        1. In memory, data is kept in structs, lists, arrays, hash tables, etc.  These data structures
             are optimized for efficient access and manipulation by the CPU (typically using pointers).

        2. When we want to write data to a file or send it over the network, you have to encode it as
             some kind of self-contained sequence of bytes (ie a JSON document).


    - So, we need some kind of translation between the 2 representations.  The translation from an 
        in-memory representation to a byte sequence is called 'encoding' (aka serialization or
        marshalling).  

      The reverse is called 'decoding' (aka parsing, deserialization, unmarshalling).



- Language-Specific Formats

    - Many programming languages come with built-in support for encoding in-memory objects into byte
        sequences.

        - Java has 'java.io.Serializable'
        - Ruby has 'Marshal'
        - Python has 'pickle'

      There are many third party libraries also.


    - There encoding libraries are very convenient, because they allow in-memory objects to be saved
        and restored with minimal additional code.  They have deep problems however:

        1. The encoding is tied to one PL, and can't be read by another PL.

        2. In order to restore data, the decoding process needs to be able to execute arbitrary
             classes, which can be a security risk.

        3. Versioning data is often an afterthought, making forwards and backwards compatibility
             difficult.

        4. Efficiency is an afterthought, and built-in serialization is known for bad performance.



- JSON, XML, and CSV

    - JSON, XML, and CSV are text formats, so they're somewhat human-readable.  They are flexible
        and good enough for most purposes.  However, they do have some problems:

        1. There is a lot of ambiguity around the encoding of numbers.  JSON doesn't distinguish
             between integers and floating points, and encoding very large numbers requires hacks.

        2. JSON and XML have good support for Unicode strings, but don't support binary strings
             (sequences of bytes without a character encoding).  Binary strings are useful, so
             people encode them with Base64, which is hacky and increases the data size.

        3. There is optional schema support for JSON and XML.  They are powerful, and somewhat
             complicated to learn and use.  XML schemas are widely used, JSON not as much.
             If a schema is not used, the decoding logic must be hard-coded into the application.

        4. CSV doesn't have any schema, and data problems can arise from the separator.



- Binary Encoding

    - For data that is only used within your organization, there is less pressure to use a common
        standard.  You could choose a format that is more compact or faster to parse.  For TB-sized
        datasets, the encoding can make a big difference.

    - JSON is less verbose than XML, but both use a lot of space compared to binary formats.  This
        has led to a profusion of binary encodings for JSON (MessagePack, BSON, BISON, Smile, etc.)
        and for XML (WBXML, Fast Infoset, etc.).  None of these are as widely adopted as the textual
        formats.


    - Here is an example record that we'll encode in several binary formats:

        {
            "userName": "Martin",
            "favoriteNumber": 1337,
            "interests": ["daydreaming", "hacking"]
        }


    - Here is part of the MessagePack encoding for the record:

        object   string         userName            string       Martin
          83       a8    75 73 65 72 4e 61 6d 65      a8    4d 61 72 74 69 6e

      The MessagePack binary encoding is 66 bytes long, only a little less than the 81 bytes taken
        by the textual JSON (with the whitespace removed).



- Thrift and Protocol Buffers

    - Apache Thrift (originally at Facebook) and Protocol Buffers (originally at Google), are
        binary encoding libraries open sourced in 2007-08.


    - Both Thrift and Protocol Buffers require a schema for any data that is encoded.  

        # Thrift IDL (Interface Definition Language)
        struct Person {
            1: required string userName,
            2: optional i64 favoriteNumber,
            3: optional list<string> interests
        }

        # Protobuf schema definition
        message Person {
            required string user_name = 1;
            optional int64 favorite_number = 2;
            repeated string interests = 3;
        }


    - Both come with a code generation tool that takes a schema definition, and produces classes
        that implement the schema in various programming languages.  Your application can call this
        generated code to encode or decode records of the schema.


    - Each of the fields in the schemas are marked as 'required' or 'optional'.  This has no effect
        on the encoding of the data.  The difference is simply the 'requires' enables a runtime check
        that fails if the field is not set.


    - Thrift has 2 different binary encoding formats:

        1. BinaryProtocol (59 bytes)

            - Each field has a type and length if needed
            - Strings are still ASCII (UTF-8)
            - No field names, tag numbers used instead

            string     field_tag     length       M  a  r  t  i  n
              0b         00 01     00 00 00 06   4d 61 72 74 69 6e

        2. CompactProtocol (34 bytes)

            - Field type and tag name combined into single byte
            - Uses variable length integers

            field_tag_and_type   length     M  a  r  t  i  n
                   18              06      4d 61 72 74 69 6e


    - Protocol Buffers fits the same record in 33 bytes by encoding the array and structs more
        efficiently.

            field_tag_and_type   length     M  a  r  t  i  n
                   18              06      4d 61 72 74 69 6e



- Field Tags and Schema Evolution

    - An encoded record is just the concatenation of it's encoded fields, which are indentified by
        a tag number and annotated with a datatype.

    - You can change the name of a field in the schema, since the encoded data never refers to field
        names.  However, you can't change a field's tag, since that would make all existing encoded
        data invalid.

    - You can add new fields to the schema, as long as you give each field a new tag number.  If old
        code reads the data and sees a tag number it doesn't recognize, it just ignores it.

    - For backwards compatibility, as long as each field has a unique tag number, new code can always
        read old data since the tag numbers still have the same meaning.  However, when you add a 
        new field after the initial deployment of the schema, you cannot make it required.

    - You can only remove a field that is optional, and you can never use the same tag again.



- Datatypes and Schema Evolution

    - You may be able to change the datatype of a field (ie a number), but you might risk losing
        precision or getting truncated.

    - Protocol Buffers does not have an array/list datatype, but instead uses a 'repeated' marker
        for fields (which is a third option besides 'required' and 'optional').  This means you
        can change an 'optional' single value to a 'repeated' multi-value.  New code sees the list,
        and old code just sees the last element of it.

-----------------------------------------------------------------------------
| CHAPTER 4 - ENCODING AND EVOLUTION                                        |
-----------------------------------------------------------------------------

- Encoding and Evolution

    - Changing an application's features often leads to a change in the data that is stored.
        Relational databases assume all the data conforms to one schema, and the schema can
        be changed with migrations (ie ALTER statements).  There is always exactly one schema.

    - Schema-on-read databases don't enforce a schema, so the database can contain a mixture of 
        older and newer data formats written at different times.


    - When a schema changes, a corresponding change to application code also needs to happen.
        However, in a large application, this might not happen instantaneously.

        1. With server-side applications, you may want to perform a rolling upgrade.
        2. With client-side applications, the user will upgrade whenever they feel like it.


    - Since old and new versions of code, and potentially old and new data formats, may all exist
        in the system at the same time, we need to maintain compatibility in both directions.

        Backward compatibility = newer code can read data written by older code
        Forward compatibility = older code can read data written by newer code

      Backward compatibility is relatively straightforward, since you know the old data format.
        Forward compatibility can be trickier, since it requires older code to ignore changes
        made by newer versions.



- Formats for Encoding Data

    - Programs usually work with data in at least 2 different representations:

        1. In memory, data is kept in structs, lists, arrays, hash tables, etc.  These data structures
             are optimized for efficient access and manipulation by the CPU (typically using pointers).

        2. When we want to write data to a file or send it over the network, you have to encode it as
             some kind of self-contained sequence of bytes (ie a JSON document).


    - So, we need some kind of translation between the 2 representations.  The translation from an 
        in-memory representation to a byte sequence is called 'encoding' (aka serialization or
        marshalling).  

      The reverse is called 'decoding' (aka parsing, deserialization, unmarshalling).



- Language-Specific Formats

    - Many programming languages come with built-in support for encoding in-memory objects into byte
        sequences.

        - Java has 'java.io.Serializable'
        - Ruby has 'Marshal'
        - Python has 'pickle'

      There are many third party libraries also.


    - There encoding libraries are very convenient, because they allow in-memory objects to be saved
        and restored with minimal additional code.  They have deep problems however:

        1. The encoding is tied to one PL, and can't be read by another PL.

        2. In order to restore data, the decoding process needs to be able to execute arbitrary
             classes, which can be a security risk.

        3. Versioning data is often an afterthought, making forwards and backwards compatibility
             difficult.

        4. Efficiency is an afterthought, and built-in serialization is known for bad performance.



- JSON, XML, and CSV

    - JSON, XML, and CSV are text formats, so they're somewhat human-readable.  They are flexible
        and good enough for most purposes.  However, they do have some problems:

        1. There is a lot of ambiguity around the encoding of numbers.  JSON doesn't distinguish
             between integers and floating points, and encoding very large numbers requires hacks.

        2. JSON and XML have good support for Unicode strings, but don't support binary strings
             (sequences of bytes without a character encoding).  Binary strings are useful, so
             people encode them with Base64, which is hacky and increases the data size.

        3. There is optional schema support for JSON and XML.  They are powerful, and somewhat
             complicated to learn and use.  XML schemas are widely used, JSON not as much.
             If a schema is not used, the decoding logic must be hard-coded into the application.

        4. CSV doesn't have any schema, and data problems can arise from the separator.



- Binary Encoding

    - For data that is only used within your organization, there is less pressure to use a common
        standard.  You could choose a format that is more compact or faster to parse.  For TB-sized
        datasets, the encoding can make a big difference.

    - JSON is less verbose than XML, but both use a lot of space compared to binary formats.  This
        has led to a profusion of binary encodings for JSON (MessagePack, BSON, BISON, Smile, etc.)
        and for XML (WBXML, Fast Infoset, etc.).  None of these are as widely adopted as the textual
        formats.


    - Here is an example record that we'll encode in several binary formats:

        {
            "userName": "Martin",
            "favoriteNumber": 1337,
            "interests": ["daydreaming", "hacking"]
        }


    - Here is part of the MessagePack encoding for the record:

        object   string         userName            string       Martin
          83       a8    75 73 65 72 4e 61 6d 65      a8    4d 61 72 74 69 6e

      The MessagePack binary encoding is 66 bytes long, only a little less than the 81 bytes taken
        by the textual JSON (with the whitespace removed).



- Thrift and Protocol Buffers

    - Apache Thrift (originally at Facebook) and Protocol Buffers (originally at Google), are
        binary encoding libraries open sourced in 2007-08.


    - Both Thrift and Protocol Buffers require a schema for any data that is encoded.  

        # Thrift IDL (Interface Definition Language)
        struct Person {
            1: required string userName,
            2: optional i64 favoriteNumber,
            3: optional list<string> interests
        }

        # Protobuf schema definition
        message Person {
            required string user_name = 1;
            optional int64 favorite_number = 2;
            repeated string interests = 3;
        }


    - Both come with a code generation tool that takes a schema definition, and produces classes
        that implement the schema in various programming languages.  Your application can call this
        generated code to encode or decode records of the schema.


    - Each of the fields in the schemas are marked as 'required' or 'optional'.  This has no effect
        on the encoding of the data.  The difference is simply the 'requires' enables a runtime check
        that fails if the field is not set.


    - Thrift has 2 different binary encoding formats:

        1. BinaryProtocol (59 bytes)

            - Each field has a type and length if needed
            - Strings are still ASCII (UTF-8)
            - No field names, tag numbers used instead

            string     field_tag     length       M  a  r  t  i  n
              0b         00 01     00 00 00 06   4d 61 72 74 69 6e

        2. CompactProtocol (34 bytes)

            - Field type and tag name combined into single byte
            - Uses variable length integers

            field_tag_and_type   length     M  a  r  t  i  n
                   18              06      4d 61 72 74 69 6e


    - Protocol Buffers fits the same record in 33 bytes by encoding the array and structs more
        efficiently.

            field_tag_and_type   length     M  a  r  t  i  n
                   18              06      4d 61 72 74 69 6e



- Field Tags and Schema Evolution

    - An encoded record is just the concatenation of it's encoded fields, which are indentified by
        a tag number and annotated with a datatype.

    - You can change the name of a field in the schema, since the encoded data never refers to field
        names.  However, you can't change a field's tag, since that would make all existing encoded
        data invalid.

    - You can add new fields to the schema, as long as you give each field a new tag number.  If old
        code reads the data and sees a tag number it doesn't recognize, it just ignores it.

    - For backwards compatibility, as long as each field has a unique tag number, new code can always
        read old data since the tag numbers still have the same meaning.  However, when you add a 
        new field after the initial deployment of the schema, you cannot make it required.

    - You can only remove a field that is optional, and you can never use the same tag again.



- Datatypes and Schema Evolution

    - You may be able to change the datatype of a field (ie a number), but you might risk losing
        precision or getting truncated.

    - Protocol Buffers does not have an array/list datatype, but instead uses a 'repeated' marker
        for fields (which is a third option besides 'required' and 'optional').  This means you
        can change an 'optional' single value to a 'repeated' multi-value.  New code sees the list,
        and old code just sees the last element of it.



- Avro

    - Apache Avro is a binary encoding format that was started in 2009 as a subproject of Hadoop,
        since Thrift was not a good fit for their use cases.


    - Avro also uses a schema to specify the structure of the data being encoded.  It has 2 schema
        languages: Avro IDL (for human editing) and a JSON one (more easily machine-readable).

        # Avro IDL Schema
        record Person {
            string userName;
            union { null, long } favoriteNumber = null;
            array<string> interests;
        }

        # Equivalent JSON version
        {
            "type": "record",
            "name": "Person",
            "fields": [
                {"name": "userName", "type": "string"},
                {"name": "favoriteNumber", "type": ["null", "long"], "default": null},
                {"name": "interests", "type": {"type": "array", "items": "string"}}
            ]
        }


    - Avro doesn't use tag numbers in the schema.  Our example record can be encoded in Avro with
        32 bytes.  Also, we don't include the data types in the data.  They are inferred from the
        schema instead.

        length_and_sign    M  a  r  t  i  n
             0c           4d 61 72 74 69 6e


    - This means that the binary data can only be decoded correctly if the code reading the data is
        using the exact same schema as the code that wrote the data.  Any mismatch would result in
        incorrectly decoded data.



- Avro Writer's Schema and Reader's Schema

    - When an application wants to encode some data, it encodes the data using whatever version of
        the schema it knows about (for example, the one compiled into the application).  This is
        called the 'writer's schema'.

    - When an application wants to decode some data, it is expecting the data to be in some schema,
        which is called the 'reader's schema'.


    - The writer's schema and reader's schema don't have to the be the same.  They just have to be
        compatible.  When data is read, the Avro library resolves the differences by looking at
        the writer's and reader's schemas side-by-side and translating the data from the writer's
        schema to the reader's.

        1. If the writer's and reader's schemas have the same fields but in a different order, Avro
             will figure that out and match them up.

        2. If a field is in the writer's schema but not the reader's schema, it is ignored.

        3. If the code reading the data expects some field, but the writer's schema doesn't contain
             the field, it is filled in with a default value declared in the reader's schema.



- Avro Schema Evolution Rules

    - With Avro, forward compatibility means the writer schema is newer than the reader.  Backward
        compatibility means the reader is newer than the writer.

    - To maintain compatibility, you may only add or remove a field that has a default value.


    - In Avro, if you want a variable to be allowed to be null, it has to be declared in a union
        type.

        union { null, long, string }

      Avro doesn't have the 'optional' and 'required' markers like Thrift and Protobuf do.



- But What is the Writer's Schema?

    - But how does the reader know the writer's schema?  We can't just include the schema with every
        record, or we'll lose our space savings.


    - The answer depends on the context in which Avro is being used.

        1. Large files with lots of records 
             = Writer can just include the writer's schema once at the beginning of the file

        2. Database with individually written records
             = Include a version number at the beginning of every encoded record, and keep a list of 
                 schema versions in your database.  A reader can fetch a record, then fetch the writer's
                 schema for that version number from the database.

        3. Sending records over a network connection
             = 2 Processes can negotiate the schema version on connection setup, then use that schema
                 for the lifetime of the connection.  The Avro RPC protocol works like this.



- Dynamically Generated Schemas

    - Since Avro doesn't have tag numbers for fields, it is friendlier to dynamically generated 
        schemas.


    - For instance, if we wanted to dump a relational DB into a file, we can fairly easily generate
        an Avro schema from the relational schema and encode the contents using that schema, dumping
        it all into an Avro container file.

      If the database schema changes, you can just generate a new Avro schema.  This can be automated.



- Code Generation and Dynamically Typed Languages

    - Thrift and Protobuf rely on code generation.  After a schema is defined, you can generate code
        that implements this schema in your PL.  This is useful in statically typed languages, because
        it allows efficient in-memory structures to be used for decoded data, and allows type checking
        and autocompletion in IDEs.

    - In dynamically typed languages, there isn't much of a point in generating code, since there is
        no compile-time checker to satisfy.  In these languages, code generation is an unwelcome
        compliation step.

    - Avro provides code generation for statically typed languages, but it can be used without any
        code generation also.

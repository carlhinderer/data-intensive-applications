-----------------------------------------------------------------------------
| CHAPTER 4 - ENCODING AND EVOLUTION                                        |
-----------------------------------------------------------------------------

- Encoding and Evolution

    - Changing an application's features often leads to a change in the data that is stored.
        Relational databases assume all the data conforms to one schema, and the schema can
        be changed with migrations (ie ALTER statements).  There is always exactly one schema.

    - Schema-on-read databases don't enforce a schema, so the database can contain a mixture of 
        older and newer data formats written at different times.


    - When a schema changes, a corresponding change to application code also needs to happen.
        However, in a large application, this might not happen instantaneously.

        1. With server-side applications, you may want to perform a rolling upgrade.
        2. With client-side applications, the user will upgrade whenever they feel like it.


    - Since old and new versions of code, and potentially old and new data formats, may all exist
        in the system at the same time, we need to maintain compatibility in both directions.

        Backward compatibility = newer code can read data written by older code
        Forward compatibility = older code can read data written by newer code

      Backward compatibility is relatively straightforward, since you know the old data format.
        Forward compatibility can be trickier, since it requires older code to ignore changes
        made by newer versions.



- Formats for Encoding Data

    - Programs usually work with data in at least 2 different representations:

        1. In memory, data is kept in structs, lists, arrays, hash tables, etc.  These data structures
             are optimized for efficient access and manipulation by the CPU (typically using pointers).

        2. When we want to write data to a file or send it over the network, you have to encode it as
             some kind of self-contained sequence of bytes (ie a JSON document).


    - So, we need some kind of translation between the 2 representations.  The translation from an 
        in-memory representation to a byte sequence is called 'encoding' (aka serialization or
        marshalling).  

      The reverse is called 'decoding' (aka parsing, deserialization, unmarshalling).



- Language-Specific Formats

    - Many programming languages come with built-in support for encoding in-memory objects into byte
        sequences.

        - Java has 'java.io.Serializable'
        - Ruby has 'Marshal'
        - Python has 'pickle'

      There are many third party libraries also.


    - There encoding libraries are very convenient, because they allow in-memory objects to be saved
        and restored with minimal additional code.  They have deep problems however:

        1. The encoding is tied to one PL, and can't be read by another PL.

        2. In order to restore data, the decoding process needs to be able to execute arbitrary
             classes, which can be a security risk.

        3. Versioning data is often an afterthought, making forwards and backwards compatibility
             difficult.

        4. Efficiency is an afterthought, and built-in serialization is known for bad performance.



- JSON, XML, and CSV

    - JSON, XML, and CSV are text formats, so they're somewhat human-readable.  They are flexible
        and good enough for most purposes.  However, they do have some problems:

        1. There is a lot of ambiguity around the encoding of numbers.  JSON doesn't distinguish
             between integers and floating points, and encoding very large numbers requires hacks.

        2. JSON and XML have good support for Unicode strings, but don't support binary strings
             (sequences of bytes without a character encoding).  Binary strings are useful, so
             people encode them with Base64, which is hacky and increases the data size.

        3. There is optional schema support for JSON and XML.  They are powerful, and somewhat
             complicated to learn and use.  XML schemas are widely used, JSON not as much.
             If a schema is not used, the decoding logic must be hard-coded into the application.

        4. CSV doesn't have any schema, and data problems can arise from the separator.



- Binary Encoding

    - For data that is only used within your organization, there is less pressure to use a common
        standard.  You could choose a format that is more compact or faster to parse.  For TB-sized
        datasets, the encoding can make a big difference.

    - JSON is less verbose than XML, but both use a lot of space compared to binary formats.  This
        has led to a profusion of binary encodings for JSON (MessagePack, BSON, BISON, Smile, etc.)
        and for XML (WBXML, Fast Infoset, etc.).  None of these are as widely adopted as the textual
        formats.


    - Here is an example record that we'll encode in several binary formats:

        {
            "userName": "Martin",
            "favoriteNumber": 1337,
            "interests": ["daydreaming", "hacking"]
        }


    - Here is part of the MessagePack encoding for the record:

        object   string         userName            string       Martin
          83       a8    75 73 65 72 4e 61 6d 65      a8    4d 61 72 74 69 6e

      The MessagePack binary encoding is 66 bytes long, only a little less than the 81 bytes taken
        by the textual JSON (with the whitespace removed).



- Thrift and Protocol Buffers

    - Apache Thrift (originally at Facebook) and Protocol Buffers (originally at Google), are
        binary encoding libraries open sourced in 2007-08.


    - Both Thrift and Protocol Buffers require a schema for any data that is encoded.  

        # Thrift IDL (Interface Definition Language)
        struct Person {
            1: required string userName,
            2: optional i64 favoriteNumber,
            3: optional list<string> interests
        }

        # Protobuf schema definition
        message Person {
            required string user_name = 1;
            optional int64 favorite_number = 2;
            repeated string interests = 3;
        }


    - Both come with a code generation tool that takes a schema definition, and produces classes
        that implement the schema in various programming languages.  Your application can call this
        generated code to encode or decode records of the schema.


    - Each of the fields in the schemas are marked as 'required' or 'optional'.  This has no effect
        on the encoding of the data.  The difference is simply the 'requires' enables a runtime check
        that fails if the field is not set.


    - Thrift has 2 different binary encoding formats:

        1. BinaryProtocol (59 bytes)

            - Each field has a type and length if needed
            - Strings are still ASCII (UTF-8)
            - No field names, tag numbers used instead

            string     field_tag     length       M  a  r  t  i  n
              0b         00 01     00 00 00 06   4d 61 72 74 69 6e

        2. CompactProtocol (34 bytes)

            - Field type and tag name combined into single byte
            - Uses variable length integers

            field_tag_and_type   length     M  a  r  t  i  n
                   18              06      4d 61 72 74 69 6e


    - Protocol Buffers fits the same record in 33 bytes by encoding the array and structs more
        efficiently.

            field_tag_and_type   length     M  a  r  t  i  n
                   18              06      4d 61 72 74 69 6e



- Field Tags and Schema Evolution

    - An encoded record is just the concatenation of it's encoded fields, which are indentified by
        a tag number and annotated with a datatype.

    - You can change the name of a field in the schema, since the encoded data never refers to field
        names.  However, you can't change a field's tag, since that would make all existing encoded
        data invalid.

    - You can add new fields to the schema, as long as you give each field a new tag number.  If old
        code reads the data and sees a tag number it doesn't recognize, it just ignores it.

    - For backwards compatibility, as long as each field has a unique tag number, new code can always
        read old data since the tag numbers still have the same meaning.  However, when you add a 
        new field after the initial deployment of the schema, you cannot make it required.

    - You can only remove a field that is optional, and you can never use the same tag again.



- Datatypes and Schema Evolution

    - You may be able to change the datatype of a field (ie a number), but you might risk losing
        precision or getting truncated.

    - Protocol Buffers does not have an array/list datatype, but instead uses a 'repeated' marker
        for fields (which is a third option besides 'required' and 'optional').  This means you
        can change an 'optional' single value to a 'repeated' multi-value.  New code sees the list,
        and old code just sees the last element of it.



- Avro

    - Apache Avro is a binary encoding format that was started in 2009 as a subproject of Hadoop,
        since Thrift was not a good fit for their use cases.


    - Avro also uses a schema to specify the structure of the data being encoded.  It has 2 schema
        languages: Avro IDL (for human editing) and a JSON one (more easily machine-readable).

        # Avro IDL Schema
        record Person {
            string userName;
            union { null, long } favoriteNumber = null;
            array<string> interests;
        }

        # Equivalent JSON version
        {
            "type": "record",
            "name": "Person",
            "fields": [
                {"name": "userName", "type": "string"},
                {"name": "favoriteNumber", "type": ["null", "long"], "default": null},
                {"name": "interests", "type": {"type": "array", "items": "string"}}
            ]
        }


    - Avro doesn't use tag numbers in the schema.  Our example record can be encoded in Avro with
        32 bytes.  Also, we don't include the data types in the data.  They are inferred from the
        schema instead.

        length_and_sign    M  a  r  t  i  n
             0c           4d 61 72 74 69 6e


    - This means that the binary data can only be decoded correctly if the code reading the data is
        using the exact same schema as the code that wrote the data.  Any mismatch would result in
        incorrectly decoded data.



- Avro Writer's Schema and Reader's Schema

    - When an application wants to encode some data, it encodes the data using whatever version of
        the schema it knows about (for example, the one compiled into the application).  This is
        called the 'writer's schema'.

    - When an application wants to decode some data, it is expecting the data to be in some schema,
        which is called the 'reader's schema'.


    - The writer's schema and reader's schema don't have to the be the same.  They just have to be
        compatible.  When data is read, the Avro library resolves the differences by looking at
        the writer's and reader's schemas side-by-side and translating the data from the writer's
        schema to the reader's.

        1. If the writer's and reader's schemas have the same fields but in a different order, Avro
             will figure that out and match them up.

        2. If a field is in the writer's schema but not the reader's schema, it is ignored.

        3. If the code reading the data expects some field, but the writer's schema doesn't contain
             the field, it is filled in with a default value declared in the reader's schema.



- Avro Schema Evolution Rules

    - With Avro, forward compatibility means the writer schema is newer than the reader.  Backward
        compatibility means the reader is newer than the writer.

    - To maintain compatibility, you may only add or remove a field that has a default value.


    - In Avro, if you want a variable to be allowed to be null, it has to be declared in a union
        type.

        union { null, long, string }

      Avro doesn't have the 'optional' and 'required' markers like Thrift and Protobuf do.



- But What is the Writer's Schema?

    - But how does the reader know the writer's schema?  We can't just include the schema with every
        record, or we'll lose our space savings.


    - The answer depends on the context in which Avro is being used.

        1. Large files with lots of records 
             = Writer can just include the writer's schema once at the beginning of the file

        2. Database with individually written records
             = Include a version number at the beginning of every encoded record, and keep a list of 
                 schema versions in your database.  A reader can fetch a record, then fetch the writer's
                 schema for that version number from the database.

        3. Sending records over a network connection
             = 2 Processes can negotiate the schema version on connection setup, then use that schema
                 for the lifetime of the connection.  The Avro RPC protocol works like this.



- Dynamically Generated Schemas

    - Since Avro doesn't have tag numbers for fields, it is friendlier to dynamically generated 
        schemas.


    - For instance, if we wanted to dump a relational DB into a file, we can fairly easily generate
        an Avro schema from the relational schema and encode the contents using that schema, dumping
        it all into an Avro container file.

      If the database schema changes, you can just generate a new Avro schema.  This can be automated.



- Code Generation and Dynamically Typed Languages

    - Thrift and Protobuf rely on code generation.  After a schema is defined, you can generate code
        that implements this schema in your PL.  This is useful in statically typed languages, because
        it allows efficient in-memory structures to be used for decoded data, and allows type checking
        and autocompletion in IDEs.

    - In dynamically typed languages, there isn't much of a point in generating code, since there is
        no compile-time checker to satisfy.  In these languages, code generation is an unwelcome
        compliation step.

    - Avro provides code generation for statically typed languages, but it can be used without any
        code generation also.



- The Merits of Schemas

    - Many data systems also implement some kind of proprietary binary encoding for their data.  For
        example, most relational DBs have a network protocol over which you send queries and get
        responses.

      These protocols are specific to  a particular database, and the database vendor provides a 
        driver (ie using the ODBC or JDBC APIs).  The driver is used to decode the responses from
        the database into in-memory data structures.


    - Advantages of schemas:

        1. Can be much more compact by omitting field names
        2. Schema is form of documentation
        3. Keeping a database of schemas allows you to check forward and backward comatibility
        4. Code generation for statically typed languages



- Modes of Dataflow

    - The Most Common Methods of Dataflow

        1. Dataflow through databases
        2. Dataflow through services (REST and RPC)
        3. Message Passing Dataflow



- Dataflow Through Databases

    - In a database, the process that writes to the database encodes the data, and the process that
        reads the database decodes it.  

      It could just be the same process reading and writing the data, in which case you are sending 
        a message to your future self.


    - Forward compatibility is often required since we want to do rolling updates.  An older, still
        running version of the code will have to read data created by the newer version of the code.

      If older code encounters a new field it doesn't know about, it should just ignore it and leave
        it alone instead of overwriting it.


    - Rewriting (migrating) data into a new schema is expensive on a large dataset, so most databases
        avoid it if possible.

      In a relational database, you can add a column with a 'null' default value.  When an old row
        is read, the database fills in nulls for any columns that are missing from the encoded data
        on disk.  



- Dataflow Through Services - REST and RPC

    - Web browsers, native apps running on mobile devices, and desktop applications are the most
        common service clients.

    - A client-side JavaScript application running inside a web browser can use XmlHttpRequest to
        become an HTTP client (this is known as AJAX).  In this case, the server's response is
        typically not HTML, but rather encoded data such as JSON.

    - When a service is implemented using HTTP, it is called a 'web service'.  REST and SOAP are
        the most popular approaches to web services.  

    - Before the rise of web services, there were many standards for RPC, which tried to make a
        request to a remote network service look the same as calling a local function in your PL.
        This approach had serious problems, like difficulty in dealing with failures and timeouts,
        duplicated data, and an awkward programming model.



- Current Directions for RPC

    - There is a new generation of RPC frameworks being built on top of the encodings mentioned in
        this chapter.  

        - Thrift and Avro come with RPC support included
        - gRPC is an RPC implementation using Protobuf
        - Finagle uses Thrift
        - Rest.li uses 


    - This new generation of RPC frameworks is more explicit about the fact that a remote request is
        different from a local function call.

      For example, Finagle uses 'futures' (aka 'promises') to encapsulate asynchronous actions that
        may fail.  Futures also simplify situations where you need to make requests to multiple
        services in parallel and combine their results.

      gRPC uses 'streams', where a call consists of not just one request and one response, but a 
        series of requests and responses over time.



- Message-Passing Dataflow

    - 'Asynchronous message-passing systems' are somewhere between RPC and databases.  Like with RPC,
        a client's request ('message') is delivered to another process with low latency.

      As with databases, the message is not sent via direct network connection, but to an intermediary
        ('message broker' or 'message queue'), which stores it temporarily.


    - Using a message broker has several advantages over direct RPC:

        1. It can act as a buffer if the recipient is unavailable or overloaded.

        2. It can automatically try to redeliver failed messages.

        3. It avoids the sender needing to know the IP address and port number of the recipient.

        4. It allows one message to be sent to several recipients.

        5. It logically decouples the sender from the recipient.



- Message Brokers

    - In the past, the landscape of message brokers was dominated by commercial enterprise software
        from companies such as TIBCO, IBM WebSphere, and webMethods.

      More recently, open source implementations such as RabbitMQ, ActiveMQ, HornetQ, NATS, and
        Apache Kafka have become popular.


    - Delivery semantics vary by implementation and configuration, but in general, message brokers are
        used as follows:

        1. One process sends a message to a named 'queue' or 'topic'.

        2. The broker ensures the message is delivered to one or more 'consumers' or 'subscribers' to 
             that queue or topic.

        3. There can be many producers and many consumers on the same topic.


    - A topic provides a one-way dataflow.  However, a consumer may itself publish messages to other
        topics, or to a reply queue to send a message back to the original sender.



- Distributed Actor Frameworks

    - The 'actor model' is a programming model for concurrency in a single process.  Rather than
        dealing with threads (and the associated problems of race conditions, locking, and deadlock),
        logic is encapsulated in 'actors'.

      Each actor represents one client or entity, may have some local state, and communicates with
        other actors by sending asynchronous messages.  Message delivery is not guaranteed, in certain
        error conditions, messages will be lost.

      Since each actor processes only message at a time, it doesn't need to worry about threads,
        and each actor can be scheduled independently by the framework.


    - In 'distributed actor frameworks', this model is used to scale an application across multiple
        nodes.  The same message-passing mechanism is used.

      Essentially, a distributed actor framework essentially integrates a message broker and the
        actor programming model into a single framework.


    - Popular distributed actor frameworks include Akka, Orleans, and Erlang OTP.
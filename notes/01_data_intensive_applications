---------------------------------------------------------------------------
CHAPTER 1 - RELIABLE, SCALABLE, MAINTAINABLE APPLICATIONS
---------------------------------------------------------------------------

- Data-intensive applications

    Many applications today are data-intensive, rather than CPU-intensive.  Raw CPU power
      is rarely a limiting factor - the bigger problems are the size, complexity and 
      speed of change of the data.


    A data-intensive application may need to:

        1. Store data for later use (databases)
        2. Remember results of expensive operations (caches)
        3. Allow users to search and filter data (search indexes)
        4. Send a message to another process to be handled asynchronously (stream processing)
        5. Periodically crunch a large amount of accumulated data (batch processing)


- Data Systems

    1. Reliability = The system should continue to work correctly even in the face of 
                       hardware or software faults or human error.

    2. Scalability = As the system grows, there should be reasonable ways of dealing with
                       that growth.

    3. Maintainability = Over time, many people should be able to work on the system
                           productively.


- Reliability

    - Software Reliability
        1. The application performs the function the user expected
        2. It can tolerate the user using the software in unexpected ways
        3. Its performance is good enough under the expected load and data volume
        4. The system prevents unauthorized access and abuse


    - A 'fault' is when one component of the system deviates from its spec.  A 'failure'
        is when the system as a whole stops providing the required service to the user.
        It's best to build fault-tolerant designs that prevent faults from causing
        failures.


    - Tools like Netflix's 'Chaos Monkey', which randomly kills process and VMs, can help
        test the fault tolerance of a system.



- Hardware Failure

    - Our first response to the threat of hardware failure is usually to add redundancy
        to the individual components of each machine.  

        - Disks in RAID configurations
        - Servers with dual power supplies and hot-swappable CPUs
        - Datacenters with backup batteries and diesel generators
        - Backups able to quickly restore machines


    - However, as data volumes and computing demands have increased, more applications
        have begun using larger numbers of machines, which proportionally increases
        the rate of hardware faults.  On platforms like AWS, it is fairly common for
        VM instances to become unavailable without warning.


    - Thus, we are moving towards systems that can tolerate the loss of entire machines.
        This approach has additional benefits, like being able to patch or reboot servers
        without bringing the entire system down.



- Software Failure

    - Examples of software failure include:
        1. A software bug that causes every instance to crash when given a particular input
        2. A runaway process that uses up some shared resource (CPU, memory, disk, bandwidth)
        3. A service that the system depends on slows down or becomes unresponsive
        4. Cascading failures, where failure of one component triggers failures of others


    - To mitigate these failures, we can:
        - Carefully think about assumptions and interactions in the system
        - Test thoroughly
        - Isolate processes
        - Allow processes to crash and restart
        - Measure, monitor, and analyze system behavior in production



- Human Error

    - Humans design and build software systems, and humans keep them running.  In one 
        study, operator error was the most common reason for failures.  Hardware 
        faults played a role in only 10-25% of them.


    - To make our systems reliable against unreliable humans:

        - Design systems with well-defined abstractions and APIs

        - Decouple the places where most mistakes are made from the places where failures 
            can be caused.

        - Test thoroughly from the unit-level to the whole-system integration level.

        - Allow quick and easy recovery from errors.  Make it fast to roll back config
            changes.  Roll out new code gradually.  Provide tools to recompute data.

        - Set up details and clear monitoring.

        - Implement good management practices and training.



- Scalability

    - Scalability is the ability of a system to cope with increased load.  Load is
        measured differently in each system, but examples include:

        - Requests/second to a web server
        - ratio of reads to writes in a database
        - number of simultaneous users in a chat room
        - hit rate on a cache



- Twitter Example

    As of November 2012, Twitter had the following numbers:
      - A user can post a tweet to their followers (4600 requests/second average, 12K peak)
      - A user can view tweets posted by people they follow (300K requests/second)


    - There are 2 broad ways of implementing these operations.

        1. Posting a tweet inserts the new tweet into a global collection of tweets.
             When a user requests their home timeline, look up all the people they follow,
             find all the tweets for each of those users, and merge them sorted by time.
             In a relational database:

           SELECT tweets.*, users.* 
           FROM tweets
           JOIN users ON tweets.sender_id = users.id
           JOIN follows ON follows.followee_id = users.id
           WHERE follows.follower_id = current_user;

        2. Maintain a cache for each user's home timeline.  When a user posts a tweet, look
             up all the people who follow that user, and insert the new tweet into each of 
             their home timeline caches.  Then, the request to read is cheap, because the
             result has been computed ahead of time.


    - In the early days, Twitter used approach #1, but the system eventually struggled
        to keep up with the load of home timeline queries, so they switched to approach
        #2.  

    - This means that if a user has 30 million followers, and that user publishes a tweet,
        then 30 million writes must occur within 5 seconds.  

    - Although Twitter broadly uses approach #2, they actually use a hybrid approach when
        it comes to users with a very high number of followers.  Tweets from these users
        are fetched separately and merged with the user's home timeline when it is ready,
        similarly to approach #1.  



- Describing Performance

    - We want to describe what happens when we increase the load on our system.  We can
        look at this in 2 ways.

        1. When you increase a load parameter and keep the system resources unchanged,
             how it performance affected?

        2. When you increase a load parameter, how much do you need to increase resources
             to keep performance unchanged?


    - In a batch-processing system like Hadoop, we usually care about 'throughput', the
        number of records we can process per second.  In online systems, we usually care
        more about 'response time'.


    - Note that 'latency' means how long it takes to process the request.
      'Response time' means how long it takes to get a response (which includes 
         network delays and queueing delays).


    - There are many factors that can affect response time:
        - Context switch to a background process
        - Loss of a network packet and TCP retransmission
        - Garbage collection pause
        - Page fault forcing a read from disk
        - Mechanical vibrations in the server rack
        ...

      For this reason, we usually measure response time either by looking at the
        average, or by taking percentile values.  Percentile values are often used in
        SLOs (Service Level Objectives) and SLAs (Service Level Agreements).

      For example, an SLA may state that a service is considered to be up if it has a
        median response time of <200ms and a 99th percentile of <1s, and the service
        is required to be up 99.9% of the time.


    - Note that in order to test response time performance in a realistic situation
        that accounts for time spent queueing for requests, testing must be done 
        from the client side.  And the load tester should send requests at random
        intervals, not just wait for responses and send another request.



- Coping With Load

    - An architecture that is appropriate for one level of load is unlikely to cope with
        10x that load.  Therefore, you will usually need to rethink your architecture
        every time you increase your load by an order of magnitude.


    - 'Scaling Up' = vertical scaling, move to a more powerful machine
    - 'Scaling Out' = horizontal scaling, distribute the load across more machines
                      (aka 'Shared Nothing')


    - 'Elastic' systems can automatically add computing resources when they detect a load
        increase, whereas other systems are scaled manually.  


    - Distributing stateless services is easy, but distributing stateful services is hard.
        For this reason, common wisdom is to use a single DB node until you are forced
        to make the DB distributed.


    - There is no one-size-fits-all approach to scaling, since requirements are different
        different for different services.  The problem may be:

        1. Volume of reads
        2. Volume of writes
        3. Volume of data to store
        4. Complexity of the data
        5. Response time requirements
        6. Access patterns
        ...


    - Premature scaling is usually a mistake, because it's impossible to know what your load
        parameters will be in the future.  In early stages, it's usually more important to 
        be able to iterate quickly then to scale to some hypothetical future load.



- Maintainability

    The majority of the cost of software is not in its initial development, but in its 
      ongoing maintenance.  Yet, many software professionals dislike maintenance of legacy
      systems.  We can design software in such a way that it will hopefully minimize pain during
      maintenance using 3 design principles.

      1. Operability = Make it easy for operations teams to keep systems running smoothly.

      2. Simplicity = Make it easy for new engineers to understand the system, by removing
                        as much complexity as possible

      3. Evolvability = Make it easy for engineers to change the system in the future
                          (this is aka 'extensibility', 'modifiability', or 'plasticity')



- Operability

    Data systems can do various things to make routine tasks easy, including:

      - Provide visibility into runtime behavior of the system with good monitoring
      - Provide good support for automation and integration with standard tools
      - Avoid dependency on individual machines
      - Provide good documentation
      - Provide good default behavoir but provide freedom for overriding defaults
      - Self-healing where appropriate
      - Exhibit predictable behavoir and avoid surprises



- Simplicity

    - Small software projects can have delightfully simple and expressive code, but as the
        projects get larger, they often become very complex and difficult to understand.
        Common causes of complexity include:

        - Explosion of the state space
        - Tight coupling of modules
        - Tangled dependencies
        - Inconsistent naming and terminologies
        - Hacks aimed at solving performance problems
        - Special casing to work around issues elsewhere


    - Abstraction is one of the best tools we have for removing accidental complexity (complexity
        not needed to solve the domain problem).  For example, relational databases and high-level
        programming languages provide good abstractions.



- Evolvability

    Common sources of flux in software systems include:

      - Learn new facts
      - Previously unanticipated use cases emerge
      - Business priorities change
      - Users request new features
      - New platforms replace old platforms
      - Legal or regulatory requirements change
      - Growth of system forces architectural changes
      ...


    - Solutions to make evolving systems more easily include:
        - TDD
        - Agile practices
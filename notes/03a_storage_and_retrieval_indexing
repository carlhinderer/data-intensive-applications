---------------------------------------------------------------------------
CHAPTER 3A - STORAGE AND RETRIEVAL - INDEXING
---------------------------------------------------------------------------

- The World's Simplest Database

    #!/bin/bash

    db_set () {
        echo "$1,$2" >> database
    }

    db_get () {
        grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
    }


    These two functions implement a key-value store. You can call 'db_set key value', which will 
      store key and value in the database. The key and value can be (almost) anything you like—
      for example, the value could be a JSON document. You can then call 'db_get key', which looks 
      up the most recent value associated with that particular key and returns it.

    $ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'

    $ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'

    $ db_get 42
    {"name":"San Francisco","attractions":["Golden Gate Bridge"]}



- The Simplest Database is a Log

    Our 'db_set' function actually has pretty good performance for something that is so simple, 
      because appending to a file is generally very efficient. Similarly to what 'db_set' does, 
      many databases internally use a log, which is an append-only data file. 

    Real databases have more issues to deal with (such as concurrency control, reclaiming disk 
      space so that the log doesn’t grow forever, and handling errors and partially written 
      records), but the basic principle is the same. Logs are incredibly useful, and we will 
      encounter them several times in the rest of this book.

    Note that the word log is often used to refer to application logs, where an application outputs 
      text that describes what’s happening. In this book, log is used in the more general sense: 
      an append-only sequence of records. It doesn’t have to be human-readable; it might be binary and 
      intended only for other programs to read.



- Retrieving Values From the Simplest Database

    On the other hand, our 'db_get' function has terrible performance if you have a large number of 
      records in your database. Every time you want to look up a key, db_get has to scan the entire 
      database file from beginning to end, looking for occurrences of the key. In algorithmic terms, 
      the cost of a lookup is O(n): if you double the number of records n in your database, a lookup 
      takes twice as long. That’s not good.

    In order to efficiently find the value for a particular key in the database, we need a different 
      data structure: an 'index'. In this chapter we will look at a range of indexing structures and 
      see how they compare; the general idea behind them is to keep some additional metadata on the 
      side, which acts as a signpost and helps you to locate the data you want. If you want to search 
      the same data in several different ways, you may need several different indexes on different 
      parts of the data.

    An index is an additional structure that is derived from the primary data. Many databases allow 
      you to add and remove indexes, and this doesn’t affect the contents of the database; it only 
      affects the performance of queries. Maintaining additional structures incurs overhead, especially 
      on writes. For writes, it’s hard to beat the performance of simply appending to a file, because 
      that’s the simplest possible write operation. Any kind of index usually slows down writes, because 
      the index also needs to be updated every time data is written.

    This is an important trade-off in storage systems: well-chosen indexes speed up read queries, but 
      every index slows down writes. For this reason, databases don’t usually index everything by 
      default, but require you—the application developer or database administrator—to choose indexes 
      manually, using your knowledge of the application’s typical query patterns. You can then choose 
      the indexes that give your application the greatest benefit, without introducing more overhead 
      than necessary.



- Hash Indexes

    Let’s start with indexes for key-value data. This is not the only kind of data you can index, but 
      it’s very common, and it’s a useful building block for more complex indexes.

    Key-value stores are quite similar to the dictionary type that you can find in most programming 
      languages, and which is usually implemented as a hash map (hash table). Since we already have hash 
      maps for our in-memory data structures, why not use them to index our data on disk?

    Let’s say our data storage consists only of appending to a file, as in the preceding example. Then 
      the simplest possible indexing strategy is this: keep an in-memory hash map where every key is 
      mapped to a byte offset in the data file—the location at which the value can be found. Whenever 
      you append a new key-value pair to the file, you also update the hash map to reflect the offset 
      of the data you just wrote (this works both for inserting new keys and for updating existing keys). 
      When you want to look up a value, use the hash map to find the offset in the data file, seek to 
      that location, and read the value.



- Hash Indexes in Bitcask

  This hash indexing strategy may sound simplistic, but it is a viable approach. In fact, this is 
    essentially what Bitcask (the default storage engine in Riak) does. Bitcask offers high-performance 
    reads and writes, subject to the requirement that all the keys fit in the available RAM, since the 
    hash map is kept completely in memory. The values can use more space than there is available memory, 
    since they can be loaded from disk with just one disk seek. If that part of the data file is already 
    in the filesystem cache, a read doesn’t require any disk I/O at all.

  A storage engine like Bitcask is well suited to situations where the value for each key is updated 
    frequently. For example, the key might be the URL of a cat video, and the value might be the number 
    of times it has been played (incremented every time someone hits the play button). In this kind of 
    workload, there are a lot of writes, but there are not too many distinct keys—you have a large number 
    of writes per key, but it’s feasible to keep all keys in memory.



- Compacting

    As described so far, we only ever append to a file — so how do we avoid eventually running out of 
      disk space? A good solution is to break the log into segments of a certain size by closing a 
      segment file when it reaches a certain size, and making subsequent writes to a new segment file. 
      We can then perform 'compaction' on these segments. Compaction means throwing away duplicate keys in 
      the log, and keeping only the most recent update for each key.


    Data File Segment 1
    -----------------
    mew: 1078   
    purr: 2103  
    purr: 2014  
    mew: 1079   
    mew: 1080   
    mew: 1081
    purr: 2105  
    purr: 2106  
    purr: 2107  
    yawn: 511   
    purr: 2108  
    mew: 1082

    Data File Segment 2
    -----------------
    purr: 2109  
    purr: 2110  
    mew: 1083   
    scratch: 252
    mew: 1084
    mew: 1085
    purr: 2111
    mew: 1086
    purr: 2112
    purr: 2113
    mew: 1087
    pur: 2114

    Merged Segments 1 and 2
    ----------------
    yawn: 511
    scratch: 252
    mew: 1087
    purr: 2114


    Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to 
      find the value for a key, we first check the most recent segment’s hash map; if the key is 
      not present we check the second-most-recent segment, and so on. The merging process keeps 
      the number of segments small, so lookups don’t need to check many hash maps.



- Implementation Details of Hash Indexes

    - File Format 

        CSV is not a good format for a log, it's faster and simpler to use a binary format that 
          first encodes the length of a string in bytes, followed by the raw string

    - Deleting Records

        If you want to delete a key and its associated value, you have to append a special deletion 
          record (a 'tombstone').  When log segments are merged, the tombstone tells the merging 
          process to discard any previous values for the deleted key

    - Crash Recovery

        If the database is restarted, the in-memory hash maps are lost.  In principle, you can 
          restore each segment's hash map by reading the entire segment file from beginning to end.
          However, that might take a long time if the segment files are large, making server 
          restarts painful.  Bitcask speeds up recovery by storing a snapshot of each segment's
          hash map on disk, which can be loaded into memory more quickly.

    - Partially Written Records

        The database may crash at any time, incluing halfway through appending a record to the 
          log.  Bitcask files include checksums, allowing such corrupted parts of the logs to
          be detected and ignored.

    - Concurrency Control

        As writes are appended to the log in a strictly sequential order, a common implementation
          choice is to have only one writer thread.  Data file segments are append-only and 
          otherwise immutable, so they can be read concurrently by multiple threads.



- Advantages of Hash Indexes

    1. Appending and segment merging are sequential write operations, which are generally much
         faster than random writes, especially on magnetic hard drives.

    2. Concurrency and crash recovery are much simpler if segment files are append-only or
         immutable.  For example, you don't have to worry about the case where a crash 
         happened while a value was being overwritten, leaving you with a file containing 
         part of the old and part of the new values spliced together.

    3. Merging old segments avoids the problem of data files getting fragmented over time.



- Limitations of Hash Indexes

    1. The hash table must fit in memory, so if you have a very large number of keys, this 
         approach will not work.  It is difficult to make an on-disk hash map work well, because
         it requires a lot of random I/O, is expensive to grow, and collisions require
         fiddly logic.

    2. Range queries are not efficient.  For instance, you cannot easily scan over all keys
         between 'kitty0000' and 'kitty9999'.  You'd have to look up each key individually.



- SSTables

    With hash indexing, we include key-value pairs in the order in which they were written.  Other
      than that, the order of key-value pairs does not matter.  Now, we can make a simple change
      to the format of our segment files: we require that the sequence of key-value pairs is 
      sorted by key.  We call this format 'Sorted String Table' or 'SSTable'.  With this format,
      we cannot append new key-value pairs to the segment immediately, since writes can occur
      in any order.


    SSTables have several big advantages over log segments with hash indexes:

      1. Merging segments is simple and efficient, even if the files are bigger than the 
           available memory.  The approach is similar to a mergesort algorithm.


        Segment 1
        -------------
        handbag: 8786
        handful: 40308
        handicap: 65995
        handkerchief: 16324
        handlebars: 3869
        handprinted: 11150

        Segment 2
        -------------
        handcuffs: 2729
        handful: 42307
        handicap: 67884
        handiwork: 16912
        handkerchief: 20952
        handprinted: 15725

        Segment 3
        -------------
        handful: 446623
        handicap: 70836
        handiwork: 45521
        handlebars: 3869
        handoff: 5741
        handprinted: 33632

        Merged 1,2,3
        -------------
        handbag: 8786
        handcuffs: 2729
        handful: 44662
        handicap: 70836
        handiwork: 45521
        handkerchief: 20952
        handlebars: 2869
        handoff: 5741
        handprinted: 33632


    2. In order to find a particular key in a file, you no longer need to keep an index of all
         the keys in memory.  You can keep an in-memory index for some of the keys, then 
         look in between the keys you have for other keys.  For instance, if you have offsets 
         for 'handbag' and 'handsome' stored, you can look for the key 'handiwork' between 
         those two.

    3. Since read requests need to scan over several key-value pairs in the requested range
         anyway, it is possible to group those records in a block and compress it before
         writing to disk.  Each entry of the sparse in-memory index then points at the start
         of a compressed block.  Besides saving disk space, compression also reduces I/O
         bandwidth.



- Constructing and Maintaining SSTables

    Next, we need to figure out how we get our data sorted by key in the first place, when 
      incoming writes can occur in any order.  Maintaining a sorted structure on disk is
      possible, but maintaining it in memory is much easier.  There are plenty of well-known
      tree data structures that can be used, such as red-black trees or AVL trees.  With these
      data structures, you can insert keys in any order and read them back in sorted order.


    We can now make our storage engine work as follows:

      - When a write comes in, add it to an in-memory balanced data structure 
          (ie a red-black tree).  This in-memory tree is sometimes called a 'memtable'.

      - When the memtable gets bigger than some threshold (typically a few MBs), write it out
          to disk as an SSTable file.  This can be done efficiently, because the tree already
          maintains the key-value pairs sorted by key.  The new SSTable file becomes the most
          recent segment of the database.  While the SSTable file is being written out to disk,
          writes can continue to a new memtable instance.

      - In order to serve a read request, first try and find the key in the memtable, then in 
          the most recent on-disk segment, then in the next-older segment, etc.

      - From time to time, run a merging and compaction process in the background to combine
          segment files and to discard overwritten or deleted values.


    This scheme works very well.  It only suffers from one problem: if the database crashes, the
      most recent writes (which are in memtable and haven't been written to disk) are lost.  In
      order to avoid that problem, we can keep a separate log on disk to which every write is
      immediately appended.  That log is not in sorted order, because its only purpose is to
      restore the memtable after a crash.  Every time the memtable is written out to an SSTable,
      the corresponding log can be discarded.



- Making an LSM-Tree Out of SSTables

    The algorithm described here is essentially what is used in 'LevelDB' and 'RocksDB',
      key-value storage engine libraries that are designed to be embedded into other
      applications.  Among other things, LevelDB can be used in Riak as an alternative to
      Bitcask.  Similar storage engines are used in Cassandra and HBase, both of which were
      inspired by Google's 'BigTable' paper (which introduced the terms 'SSTable' and 'memtable').
      
    Originally this indexing structure was described by Patrick O'Neil under the name
      'Log-Structured Merge-Tree' ('LSM-Tree').  This built on earlier work on log-structured
      filesystems.  Storage engines based on this principle of merging and compacting sorted files
      are often called 'LSM Storage Engines'.

    Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a similar
      method for storing its 'term dictionary'.  A full-text index is much more complex than a
      key-value index, but is based on a similar idea: given a word in a search query, find all
      the documents (web pages, product descriptions, etc.) that mention the word.  This is 
      implemented with a key-value structure where the key is a word (a 'term') and the value
      is the list of IDs of all the documents that contain the word (the 'postings list').  In
      Lucene, this mapping from term to postings list is kept in SSTable-like sorted files, 
      which are merged in the background as needed.



- Performance Optimizations

    The LSM-Tree algorithm can be slow when looking up keys that do not exist in the database.
      You have to check the memtable, then the segments all the way back to the oldest, possibly
      having to read from disk in each one, before you can be sure the key does not exist.  In order
      to optimize this kind of access, storage engines often use additional 'Bloom filters'.  A
      Bloom filter is a memory-efficient data structure for approximating the contents of a set.
      It can tell you if a key does not appear in the database, and thus saves many
      unnecessary disk reads for nonexistent keys.

    There are also different strategies to determine the order and timing of how SSTables are
      compacted and merged.  The most common options are 'size-tiered' and 'leveled' compaction.
      LevelDB and RocksDB use leveled compaction, HBase uses size-tiered compaction, and
      Cassandra supports both.

      - In size-tiered compaction, newer and smaller SSTables are successively merged into 
          older and larger SSTables.

      - In leveled compaction, the key range is split up into smaller SSTables and older data is
          moved into separate levels, with allows the compaction to proceed more incrementally and
          use less disk space.


    Even though there are many subtleties, the basic idea of LSM-Trees - keeping a cascade of 
      SSTables that are merged in the background - is simple and effective.  Even when the 
      dataset is much bigger than the available memory, it continues to work well.  Since data is
      stored in sorted order, you can efficiently perform range queries.  Since writes are
      sequential, write throughput is remarkably high.



- B-Trees

    By far, the most widely used indexing structure is the 'B-Tree'.  It was introduced in 1970, 
      called ubiquitious less than 10 years later, and has stood the test of time.  They remain the
      standard index implementation in all relational databases and many nonrelational databases.

    Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-value
      lookups and range queries.  However, they have a very different design philosophy.  

    The log-structured indexes we saw earlier break the database down into variable-size segments, 
      typically several megabytes or more in size, and always write a segment sequentially. By 
      contrast, B-trees break the database down into fixed-size blocks or 'pages', traditionally 4 KB 
      in size (sometimes bigger), and read or write one page at a time. This design corresponds more 
      closely to the underlying hardware, as disks are also arranged in fixed-size blocks.

    Each page can be identified using an address or location, which allows one page to refer to 
      another — similar to a pointer, but on disk instead of in memory. We can use these page 
      references to construct a tree of pages.


                                  "Look up user_id = 251"

      [ ref  |  100  |  ref  |  200 |  ref |  300 |  ref |  400 |  ref |  500 |  ref ]
         |               |              |             |             |             |
         v               v              |             v             v             v
    [key < 100]  [100 <= key < 200]     |                                     [key >= 500]
                                        |
                                        |
                                        v
        [ ref | 210 | ref | 230 | ref | 250 | ref | 270 | ref | 290 | ref ]
           |           |           |           |           |           |
           v           v           v           |           v           v
                                               |
                                               v
                       [ 250 | val | 251 | val | 252 | val | 253 | val | 254 | val ]


    One page is designated as the root of the B-tree; whenever you want to look up a key in the 
      index, you start here. The page contains several keys and references to child pages. Each 
      child is responsible for a continuous range of keys, and the keys between the references 
      indicate where the boundaries between those ranges lie.

    Here, we are looking for the key 251, so we know that we need to follow the page reference 
      between the boundaries 200 and 300. That takes us to a similar-looking page that further breaks 
      down the 200–300 range into subranges. Eventually we get down to a page containing individual 
      keys (a leaf page), which either contains the value for each key inline or contains references 
      to the pages where the values can be found.

    The number of references to child pages in one page of the B-tree is called the branching factor. 
      For example, in Figure 3-6 the branching factor is six. In practice, the branching factor depends 
      on the amount of space required to store the page references and the range boundaries, but 
      typically it is several hundred.

    If you want to update the value for an existing key in a B-tree, you search for the leaf page 
      containing that key, change the value in that page, and write the page back to disk (any 
      references to that page remain valid). 

    If you want to add a new key, you need to find the page whose range encompasses the new key and 
      add it to that page. If there isn’t enough free space in the page to accommodate the new key, 
      it is split into two half-full pages, and the parent page is updated to account for the new 
      subdivision of key ranges.


    This algorithm ensures that the tree remains 'balanced'.  A b-tree with n keys always has a 
      depth of O(log n).  Most databases can fit into a B-tree that is 3 or 4 levels deep, so you
      don't need to follow many page references to find the page you are looking for.  (A 4-level
      tree of 4-KB pages with a branching factor of 500 can store up to 256 TB).



- Making B-Trees Reliable


- B-Tree Optimizations


- 
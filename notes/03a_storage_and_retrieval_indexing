---------------------------------------------------------------------------
CHAPTER 3A - STORAGE AND RETRIEVAL - INDEXING
---------------------------------------------------------------------------

- The World's Simplest Database

    #!/bin/bash

    db_set () {
        echo "$1,$2" >> database
    }

    db_get () {
        grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
    }


    These two functions implement a key-value store. You can call 'db_set key value', which will 
      store key and value in the database. The key and value can be (almost) anything you like—
      for example, the value could be a JSON document. You can then call 'db_get key', which looks 
      up the most recent value associated with that particular key and returns it.

    $ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'

    $ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'

    $ db_get 42
    {"name":"San Francisco","attractions":["Golden Gate Bridge"]}



- The Simplest Database is a Log

    Our 'db_set' function actually has pretty good performance for something that is so simple, 
      because appending to a file is generally very efficient. Similarly to what 'db_set' does, 
      many databases internally use a log, which is an append-only data file. 

    Real databases have more issues to deal with (such as concurrency control, reclaiming disk 
      space so that the log doesn’t grow forever, and handling errors and partially written 
      records), but the basic principle is the same. Logs are incredibly useful, and we will 
      encounter them several times in the rest of this book.

    Note that the word log is often used to refer to application logs, where an application outputs 
      text that describes what’s happening. In this book, log is used in the more general sense: 
      an append-only sequence of records. It doesn’t have to be human-readable; it might be binary and 
      intended only for other programs to read.



- Retrieving Values From the Simplest Database

    On the other hand, our 'db_get' function has terrible performance if you have a large number of 
      records in your database. Every time you want to look up a key, db_get has to scan the entire 
      database file from beginning to end, looking for occurrences of the key. In algorithmic terms, 
      the cost of a lookup is O(n): if you double the number of records n in your database, a lookup 
      takes twice as long. That’s not good.

    In order to efficiently find the value for a particular key in the database, we need a different 
      data structure: an 'index'. In this chapter we will look at a range of indexing structures and 
      see how they compare; the general idea behind them is to keep some additional metadata on the 
      side, which acts as a signpost and helps you to locate the data you want. If you want to search 
      the same data in several different ways, you may need several different indexes on different 
      parts of the data.

    An index is an additional structure that is derived from the primary data. Many databases allow 
      you to add and remove indexes, and this doesn’t affect the contents of the database; it only 
      affects the performance of queries. Maintaining additional structures incurs overhead, especially 
      on writes. For writes, it’s hard to beat the performance of simply appending to a file, because 
      that’s the simplest possible write operation. Any kind of index usually slows down writes, because 
      the index also needs to be updated every time data is written.

    This is an important trade-off in storage systems: well-chosen indexes speed up read queries, but 
      every index slows down writes. For this reason, databases don’t usually index everything by 
      default, but require you—the application developer or database administrator—to choose indexes 
      manually, using your knowledge of the application’s typical query patterns. You can then choose 
      the indexes that give your application the greatest benefit, without introducing more overhead 
      than necessary.



- Hash Indexes

    Let’s start with indexes for key-value data. This is not the only kind of data you can index, but 
      it’s very common, and it’s a useful building block for more complex indexes.

    Key-value stores are quite similar to the dictionary type that you can find in most programming 
      languages, and which is usually implemented as a hash map (hash table). Since we already have hash 
      maps for our in-memory data structures, why not use them to index our data on disk?

    Let’s say our data storage consists only of appending to a file, as in the preceding example. Then 
      the simplest possible indexing strategy is this: keep an in-memory hash map where every key is 
      mapped to a byte offset in the data file—the location at which the value can be found. Whenever 
      you append a new key-value pair to the file, you also update the hash map to reflect the offset 
      of the data you just wrote (this works both for inserting new keys and for updating existing keys). 
      When you want to look up a value, use the hash map to find the offset in the data file, seek to 
      that location, and read the value.



- Hash Indexes in Bitcask

  This hash indexing strategy may sound simplistic, but it is a viable approach. In fact, this is 
    essentially what Bitcask (the default storage engine in Riak) does. Bitcask offers high-performance 
    reads and writes, subject to the requirement that all the keys fit in the available RAM, since the 
    hash map is kept completely in memory. The values can use more space than there is available memory, 
    since they can be loaded from disk with just one disk seek. If that part of the data file is already 
    in the filesystem cache, a read doesn’t require any disk I/O at all.

  A storage engine like Bitcask is well suited to situations where the value for each key is updated 
    frequently. For example, the key might be the URL of a cat video, and the value might be the number 
    of times it has been played (incremented every time someone hits the play button). In this kind of 
    workload, there are a lot of writes, but there are not too many distinct keys—you have a large number 
    of writes per key, but it’s feasible to keep all keys in memory.



- Compacting

    As described so far, we only ever append to a file — so how do we avoid eventually running out of 
      disk space? A good solution is to break the log into segments of a certain size by closing a 
      segment file when it reaches a certain size, and making subsequent writes to a new segment file. 
      We can then perform 'compaction' on these segments. Compaction means throwing away duplicate keys in 
      the log, and keeping only the most recent update for each key.


    Data File Segment 1
    -----------------
    mew: 1078   
    purr: 2103  
    purr: 2014  
    mew: 1079   
    mew: 1080   
    mew: 1081
    purr: 2105  
    purr: 2106  
    purr: 2107  
    yawn: 511   
    purr: 2108  
    mew: 1082

    Data File Segment 2
    -----------------
    purr: 2109  
    purr: 2110  
    mew: 1083   
    scratch: 252
    mew: 1084
    mew: 1085
    purr: 2111
    mew: 1086
    purr: 2112
    purr: 2113
    mew: 1087
    pur: 2114

    Merged Segments 1 and 2
    ----------------
    yawn: 511
    scratch: 252
    mew: 1087
    purr: 2114


    Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to 
      find the value for a key, we first check the most recent segment’s hash map; if the key is 
      not present we check the second-most-recent segment, and so on. The merging process keeps 
      the number of segments small, so lookups don’t need to check many hash maps.



- Implementation Details of Hash Indexes

    - File Format 

        CSV is not a good format for a log, it's faster and simpler to use a binary format that 
          first encodes the length of a string in bytes, followed by the raw string

    - Deleting Records

        If you want to delete a key and its associated value, you have to append a special deletion 
          record (a 'tombstone').  When log segments are merged, the tombstone tells the merging 
          process to discard any previous values for the deleted key

    - Crash Recovery

        If the database is restarted, the in-memory hash maps are lost.  In principle, you can 
          restore each segment's hash map by reading the entire segment file from beginning to end.
          However, that might take a long time if the segment files are large, making server 
          restarts painful.  Bitcask speeds up recovery by storing a snapshot of each segment's
          hash map on disk, which can be loaded into memory more quickly.

    - Partially Written Records

        The database may crash at any time, incluing halfway through appending a record to the 
          log.  Bitcask files include checksums, allowing such corrupted parts of the logs to
          be detected and ignored.

    - Concurrency Control

        As writes are appended to the log in a strictly sequential order, a common implementation
          choice is to have only one writer thread.  Data file segments are append-only and 
          otherwise immutable, so they can be read concurrently by multiple threads.



- Advantages of Hash Indexes

    1. Appending and segment merging are sequential write operations, which are generally much
         faster than random writes, especially on magnetic hard drives.

    2. Concurrency and crash recovery are much simpler if segment files are append-only or
         immutable.  For example, you don't have to worry about the case where a crash 
         happened while a value was being overwritten, leaving you with a file containing 
         part of the old and part of the new values spliced together.

    3. Merging old segments avoids the problem of data files getting fragmented over time.



- Limitations of Hash Indexes

    1. The hash table must fit in memory, so if you have a very large number of keys, this 
         approach will not work.  It is difficult to make an on-disk hash map work well, because
         it requires a lot of random I/O, is expensive to grow, and collisions require
         fiddly logic.

    2. Range queries are not efficient.  For instance, you cannot easily scan over all keys
         between 'kitty0000' and 'kitty9999'.  You'd have to look up each key individually.



- SSTables

    With hash indexing, we include key-value pairs in the order in which they were written.  Other
      than that, the order of key-value pairs does not matter.  Now, we can make a simple change
      to the format of our segment files: we require that the sequence of key-value pairs is 
      sorted by key.  We call this format 'Sorted String Table' or 'SSTable'.  With this format,
      we cannot append new key-value pairs to the segment immediately, since writes can occur
      in any order.


    SSTables have several big advantages over log segments with hash indexes:

      1. Merging segments is simple and efficient, even if the files are bigger than the 
           available memory.  The approach is similar to a mergesort algorithm.


        Segment 1
        -------------
        handbag: 8786
        handful: 40308
        handicap: 65995
        handkerchief: 16324
        handlebars: 3869
        handprinted: 11150

        Segment 2
        -------------
        handcuffs: 2729
        handful: 42307
        handicap: 67884
        handiwork: 16912
        handkerchief: 20952
        handprinted: 15725

        Segment 3
        -------------
        handful: 446623
        handicap: 70836
        handiwork: 45521
        handlebars: 3869
        handoff: 5741
        handprinted: 33632

        Merged 1,2,3
        -------------
        handbag: 8786
        handcuffs: 2729
        handful: 44662
        handicap: 70836
        handiwork: 45521
        handkerchief: 20952
        handlebars: 2869
        handoff: 5741
        handprinted: 33632


    2. In order to find a particular key in a file, you no longer need to keep an index of all
         the keys in memory.  You can keep an in-memory index for some of the keys, then 
         look in between the keys you have for other keys.  For instance, if you have offsets 
         for 'handbag' and 'handsome' stored, you can look for the key 'handiwork' between 
         those two.

    3. Since read requests need to scan over several key-value pairs in the requested range
         anyway, it is possible to group those records in a block and compress it before
         writing to disk.  Each entry of the sparse in-memory index then points at the start
         of a compressed block.  Besides saving disk space, compression also reduces I/O
         bandwidth.



- Constructing and Maintaining SSTables

    Next, we need to figure out how we get our data sorted by key in the first place, when 
      incoming writes can occur in any order.  Maintaining a sorted structure on disk is
      possible, but maintaining it in memory is much easier.  There are plenty of well-known
      tree data structures that can be used, such as red-black trees or AVL trees.  With these
      data structures, you can insert keys in any order and read them back in sorted order.


    We can now make our storage engine work as follows:

      - When a write comes in, add it to an in-memory balanced data structure 
          (ie a red-black tree).  This in-memory tree is sometimes called a 'memtable'.

      - When the memtable gets bigger than some threshold (typically a few MBs), write it out
          to disk as an SSTable file.  This can be done efficiently, because the tree already
          maintains the key-value pairs sorted by key.  The new SSTable file becomes the most
          recent segment of the database.  While the SSTable file is being written out to disk,
          writes can continue to a new memtable instance.

      - In order to serve a read request, first try and find the key in the memtable, then in 
          the most recent on-disk segment, then in the next-older segment, etc.

      - From time to time, run a merging and compaction process in the background to combine
          segment files and to discard overwritten or deleted values.


    This scheme works very well.  It only suffers from one problem: if the database crashes, the
      most recent writes (which are in memtable and haven't been written to disk) are lost.  In
      order to avoid that problem, we can keep a separate log on disk to which every write is
      immediately appended.  That log is not in sorted order, because its only purpose is to
      restore the memtable after a crash.  Every time the memtable is written out to an SSTable,
      the corresponding log can be discarded.



- Making an LSM-Tree Out of SSTables

    The algorithm described here is essentially what is used in 'LevelDB' and 'RocksDB',
      key-value storage engine libraries that are designed to be embedded into other
      applications.  Among other things, LevelDB can be used in Riak as an alternative to
      Bitcask.  Similar storage engines are used in Cassandra and HBase, both of which were
      inspired by Google's 'BigTable' paper (which introduced the terms 'SSTable' and 'memtable').
      
    Originally this indexing structure was described by Patrick O'Neil under the name
      'Log-Structured Merge-Tree' ('LSM-Tree').  This built on earlier work on log-structured
      filesystems.  Storage engines based on this principle of merging and compacting sorted files
      are often called 'LSM Storage Engines'.

    Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a similar
      method for storing its 'term dictionary'.  A full-text index is much more complex than a
      key-value index, but is based on a similar idea: given a word in a search query, find all
      the documents (web pages, product descriptions, etc.) that mention the word.  This is 
      implemented with a key-value structure where the key is a word (a 'term') and the value
      is the list of IDs of all the documents that contain the word (the 'postings list').  In
      Lucene, this mapping from term to postings list is kept in SSTable-like sorted files, 
      which are merged in the background as needed.



- Performance Optimizations

    The LSM-Tree algorithm can be slow when looking up keys that do not exist in the database.
      You have to check the memtable, then the segments all the way back to the oldest, possibly
      having to read from disk in each one, before you can be sure the key does not exist.  In order
      to optimize this kind of access, storage engines often use additional 'Bloom filters'.  A
      Bloom filter is a memory-efficient data structure for approximating the contents of a set.
      It can tell you if a key does not appear in the database, and thus saves many
      unnecessary disk reads for nonexistent keys.

    There are also different strategies to determine the order and timing of how SSTables are
      compacted and merged.  The most common options are 'size-tiered' and 'leveled' compaction.
      LevelDB and RocksDB use leveled compaction, HBase uses size-tiered compaction, and
      Cassandra supports both.

      - In size-tiered compaction, newer and smaller SSTables are successively merged into 
          older and larger SSTables.

      - In leveled compaction, the key range is split up into smaller SSTables and older data is
          moved into separate levels, with allows the compaction to proceed more incrementally and
          use less disk space.


    Even though there are many subtleties, the basic idea of LSM-Trees - keeping a cascade of 
      SSTables that are merged in the background - is simple and effective.  Even when the 
      dataset is much bigger than the available memory, it continues to work well.  Since data is
      stored in sorted order, you can efficiently perform range queries.  Since writes are
      sequential, write throughput is remarkably high.



- B-Trees

    By far, the most widely used indexing structure is the 'B-Tree'.  It was introduced in 1970, 
      called ubiquitious less than 10 years later, and has stood the test of time.  They remain the
      standard index implementation in all relational databases and many nonrelational databases.

    Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-value
      lookups and range queries.  However, they have a very different design philosophy.  

    The log-structured indexes we saw earlier break the database down into variable-size segments, 
      typically several megabytes or more in size, and always write a segment sequentially. By 
      contrast, B-trees break the database down into fixed-size blocks or 'pages', traditionally 4 KB 
      in size (sometimes bigger), and read or write one page at a time. This design corresponds more 
      closely to the underlying hardware, as disks are also arranged in fixed-size blocks.

    Each page can be identified using an address or location, which allows one page to refer to 
      another — similar to a pointer, but on disk instead of in memory. We can use these page 
      references to construct a tree of pages.


                                  "Look up user_id = 251"

      [ ref  |  100  |  ref  |  200 |  ref |  300 |  ref |  400 |  ref |  500 |  ref ]
         |               |              |             |             |             |
         v               v              |             v             v             v
    [key < 100]  [100 <= key < 200]     |                                     [key >= 500]
                                        |
                                        |
                                        v
        [ ref | 210 | ref | 230 | ref | 250 | ref | 270 | ref | 290 | ref ]
           |           |           |           |           |           |
           v           v           v           |           v           v
                                               |
                                               v
                       [ 250 | val | 251 | val | 252 | val | 253 | val | 254 | val ]


    One page is designated as the root of the B-tree; whenever you want to look up a key in the 
      index, you start here. The page contains several keys and references to child pages. Each 
      child is responsible for a continuous range of keys, and the keys between the references 
      indicate where the boundaries between those ranges lie.

    Here, we are looking for the key 251, so we know that we need to follow the page reference 
      between the boundaries 200 and 300. That takes us to a similar-looking page that further breaks 
      down the 200–300 range into subranges. Eventually we get down to a page containing individual 
      keys (a leaf page), which either contains the value for each key inline or contains references 
      to the pages where the values can be found.

    The number of references to child pages in one page of the B-tree is called the branching factor. 
      For example, in Figure 3-6 the branching factor is six. In practice, the branching factor depends 
      on the amount of space required to store the page references and the range boundaries, but 
      typically it is several hundred.

    If you want to update the value for an existing key in a B-tree, you search for the leaf page 
      containing that key, change the value in that page, and write the page back to disk (any 
      references to that page remain valid). 

    If you want to add a new key, you need to find the page whose range encompasses the new key and 
      add it to that page. If there isn’t enough free space in the page to accommodate the new key, 
      it is split into two half-full pages, and the parent page is updated to account for the new 
      subdivision of key ranges.


    This algorithm ensures that the tree remains 'balanced'.  A b-tree with n keys always has a 
      depth of O(log n).  Most databases can fit into a B-tree that is 3 or 4 levels deep, so you
      don't need to follow many page references to find the page you are looking for.  (A 4-level
      tree of 4-KB pages with a branching factor of 500 can store up to 256 TB).



- Making B-Trees Reliable

    The basic underlying write operation of a B-tree is to overwrite a page on disk with new
      data.  It is assumed that the overwrite does not change the location of the page, so all 
      references to that page remain intact when the page is overwritten. This is in stark 
      contrast to log-structured indexes such as LSM-trees, which only append to files 
      (and eventually delete obsolete files) but never modify files in place.

    You can think of overwriting a page on disk as an actual hardware operation. On a magnetic 
      hard drive, this means moving the disk head to the right place, waiting for the right 
      position on the spinning platter to come around, and then overwriting the appropriate 
      sector with new data. On SSDs, what happens is somewhat more complicated, due to the 
      fact that an SSD must erase and rewrite fairly large blocks of a storage chip at a time.

    Moreover, some operations require several different pages to be overwritten. For example, 
      if you split a page because an insertion caused it to be overfull, you need to write the 
      two pages that were split, and also overwrite their parent page to update the references 
      to the two child pages. This is a dangerous operation, because if the database crashes 
      after only some of the pages have been written, you end up with a corrupted index (e.g., there 
      may be an orphan page that is not a child of any parent).

    In order to make the database resilient to crashes, it is common for B-tree implementations 
      to include an additional data structure on disk: a write-ahead log (WAL, also known as a 
      redo log). This is an append-only file to which every B-tree modification must be written 
      before it can be applied to the pages of the tree itself. When the database comes back up 
      after a crash, this log is used to restore the B-tree back to a consistent state.

    An additional complication of updating pages in place is that careful concurrency control is 
      required if multiple threads are going to access the B-tree at the same time—otherwise a thread 
      may see the tree in an inconsistent state. This is typically done by protecting the tree’s 
      data structures with latches (lightweight locks). Log-structured approaches are simpler in 
      this regard, because they do all the merging in the background without interfering with 
      incoming queries and atomically swap old segments for new segments from time to time.



- B-Tree Optimizations

    Since B-trees have been around for so long, many optimizations have been developed over the
      years:

      - Instead of overwriting pages and maintaining a WAL for crash recovery, some databases
          use a 'copy on write' scheme.  A modified page is written to a different location, and
          a new version of the parent pages in the tree is created, pointing at the new location.
          This approach is also useful for concurrency control.

      - We can save space in pages by not storing the entire key, but abbreviating it.  Especially
          in pages on the interior of the tree, keys only need to provide enough information to
          act as boundaries between key ranges.  Packing more keys into a page allows the tree
          to have a higher branching factor, and thus fewer levels.

      - In general, pages can be positioned anywhere on disk.  There is nothing requiring pages
          with nearby key ranges to be nearby on disk.

      - B-tree variants such as 'fractal trees' borrow some log-structured ideas to reduce disk
          seeks.



- Advantages of LSM-Trees over B-Trees

    - A B-Tree must write every piece of data twice (first to the WAL and then to the page).
        There is also overhead from having to write an entire page at a time.  This makes
        LSM-Trees generally faster for writes.

    - Log-structured indexes also rewrite data multiple times due to repeated compaction and
        merging of SSTables, however.  This effect, in which one write to the database results
        in multiple writes over time, is known as 'write amplification'.  In write-heavy 
        applications, this may cause a performance bottleneck.

    - LSM-Trees are typically able to sustain higher rates of write throughput than B-trees, 
        partly because they sometimes have lower write amplification, and partly because 
        they can sequentially write compact SSTables rather than having to overwrite entire
        pages.  This difference is particularly important on magnetic hard drives.

    - LSM-Trees can be compressed better, and thus often produce smaller files on disk than
        B-trees.  B-tree storage engines leave some disk space unused due to fragmentation.



- Advantages of B-Trees over LSM-Trees

    - A downside of log-structured storage is that the compaction process can sometimes 
        interfere withy the performance of ongoing reads and writes.  Even though storage
        engines try to perform compcation incrementally and without affecting concurrent
        access, disks have limited resouces, so a request may have to wait while the disk
        finishes an expensive compaction operation.  The response times of B-trees are more
        predictable.

    - Another issue with compaction arises at high write throughput: the disk’s finite write 
        bandwidth needs to be shared between the initial write (logging and flushing a memtable 
        to disk) and the compaction threads running in the background. When writing to an empty 
        database, the full disk bandwidth can be used for the initial write, but the bigger 
        the database gets, the more disk bandwidth is required for compaction.

    - If write throughput is high and compaction is not configured carefully, it can happen 
        that compaction cannot keep up with the rate of incoming writes. In this case, the number 
        of unmerged segments on disk keeps growing until you run out of disk space, and reads 
        also slow down because they need to check more segment files. Typically, SSTable-based 
        storage engines do not throttle the rate of incoming writes, even if compaction cannot 
        keep up, so you need explicit monitoring to detect this situation.

    - An advantage of B-trees is that each key exists in exactly one place in the index, whereas 
        a log-structured storage engine may have multiple copies of the same key in different 
        segments. This aspect makes B-trees attractive in databases that want to offer strong 
        transactional semantics: in many relational databases, transaction isolation is 
        implemented using locks on ranges of keys, and in a B-tree index, those locks can be 
        directly attached to the tree.



- Other Indexing Structures

    So far, we have only discussed key-value indexes, which are like a primary key index in the
      relational model.  A primary key uniquely identifies one row in a relational table, one
      document in a document database, or one vertex in a graph database.  Other records can refer
      to the row/document/vertex by its primary key, and the index is used to resolve such
      references.

    It is also very common to have 'secondary indexes'.  For instance, indexes on non-primary keys
      can be created in relational databases.  A secondary index can easily be constructed from
      a key-value index.  The main difference is that in a secondary index, the indexed values
      are not necessarily unique.  


    This can be solved in two ways: 

      - by making each value in the index a list of matching row identifiers (like a postings list 
          in a full-text index)

      - by making each entry unique by appending a row identifier to it 


    Either way, both B-trees and log-structured indexes can be used as secondary indexes.



- Storing Values Within the Index

    The key in an index is the thing that queries search for, but the value can be one of two 
      things: it could be the actual row (document, vertex) in question, or it could be a 
      reference to the row stored elsewhere. In the latter case, the place where rows are 
      stored is known as a 'heap file', and it stores data in no particular order (it may be 
      append-only, or it may keep track of deleted rows in order to overwrite them with new 
      data later). The heap file approach is common because it avoids duplicating data when 
      multiple secondary indexes are present: each index just references a location in the 
      heap file, and the actual data is kept in one place.

    When updating a value without changing the key, the heap file approach can be quite 
      efficient: the record can be overwritten in place, provided that the new value is not larger 
      than the old value. The situation is more complicated if the new value is larger, as it 
      probably needs to be moved to a new location in the heap where there is enough space. In 
      that case, either all indexes need to be updated to point at the new heap location of the 
      record, or a forwarding pointer is left behind in the old heap location.

    In some situations, the extra hop from the index to the heap file is too much of a 
      performance penalty for reads, so it can be desirable to store the indexed row directly 
      within an index. This is known as a 'clustered index'. For example, in MySQL’s InnoDB 
      storage engine, the primary key of a table is always a clustered index, and secondary 
      indexes refer to the primary key (rather than a heap file location). In SQL Server, you 
      can specify one clustered index per table.

    A compromise between a clustered index (storing all row data within the index) and a 
      nonclustered index (storing only references to the data within the index) is known as a 
      'covering index' or index with included columns, which stores some of a table’s columns 
      within the index. This allows some queries to be answered by using the index alone (in 
      which case, the index is said to cover the query).

    As with any kind of duplication of data, clustered and covering indexes can speed up reads, 
      but they require additional storage and can add overhead on writes. Databases also need to 
      go to additional effort to enforce transactional guarantees, because applications should 
      not see inconsistencies due to the duplication.



- Multi-Column Indexes

    The indexes discussed so far only map a single key to a value. That is not sufficient if we 
      need to query multiple columns of a table (or multiple fields in a document) simultaneously.

    The most common type of multi-column index is called a concatenated index, which simply 
      combines several fields into one key by appending one column to another (the index 
      definition specifies in which order the fields are concatenated). This is like an 
      old-fashioned paper phone book, which provides an index from (lastname, firstname) to 
      phone number. Due to the sort order, the index can be used to find all the people with a 
      particular last name, or all the people with a particular lastname-firstname combination. 
      However, the index is useless if you want to find all the people with a particular first name.

    Multi-dimensional indexes are a more general way of querying several columns at once, which is
      particularly important for geospatial data. For example, a restaurant-search website may have 
      a database containing the latitude and longitude of each restaurant. When a user is looking at 
      the restaurants on a map, the website needs to search for all the restaurants within the 
      rectangular map area that the user is currently viewing. This requires a two-dimensional range 
      query like the following:

    SELECT * FROM restaurants WHERE latitude  > 51.4946 AND latitude  < 51.5079
                              AND longitude > -0.1162 AND longitude < -0.1004;

    A standard B-tree or LSM-tree index is not able to answer that kind of query efficiently: it can 
      give you either all the restaurants in a range of latitudes (but at any longitude), or all 
      the restaurants in a range of longitudes (but anywhere between the North and South poles), 
      but not both simultaneously.

    One option is to translate a two-dimensional location into a single number using a space-filling 
      curve, and then to use a regular B-tree index [34]. More commonly, specialized spatial indexes 
      such as R-trees are used. For example, PostGIS implements geospatial indexes as R-trees using
      PostgreSQL’s Generalized Search Tree indexing facility. We don’t have space to describe R-trees 
      in detail here, but there is plenty of literature on them.

    An interesting idea is that multi-dimensional indexes are not just for geographic locations. 
      For example, on an ecommerce website you could use a three-dimensional index on the dimensions 
      (red, green, blue) to search for products in a certain range of colors, or in a database of 
      weather observations you could have a two-dimensional index on (date, temperature) in order to
      efficiently search for all the observations during the year 2013 where the temperature was 
      between 25 and 30℃. With a one-dimensional index, you would have to either scan over all the 
      records from 2013 (regardless of temperature) and then filter them by temperature, or vice 
      versa. A 2D index could narrow down by timestamp and temperature simultaneously. This 
      technique is used by HyperDex.



- Full-Text Search and Fuzzy Indexes

    All the indexes discussed so far assume that you have exact data and allow you to query for exact 
      values of a key, or a range of values of a key with a sort order. What they don’t allow you to do 
      is search for similar keys, such as misspelled words. Such fuzzy querying requires different techniques.

    For example, full-text search engines commonly allow a search for one word to be expanded to include 
      synonyms of the word, to ignore grammatical variations of words, and to search for occurrences of words 
      near each other in the same document, and support various other features that depend on linguistic 
      analysis of the text. To cope with typos in documents or queries, Lucene is able to search text for 
      words within a certain edit distance (an edit distance of 1 means that one letter has been added, 
      removed, or replaced).

    As mentioned in “Making an LSM-tree out of SSTables”, Lucene uses a SSTable-like structure for its term 
      dictionary. This structure requires a small in-memory index that tells queries at which offset in the 
      sorted file they need to look for a key. In LevelDB, this in-memory index is a sparse collection of 
      some of the keys, but in Lucene, the in-memory index is a finite state automaton over the characters 
      in the keys, similar to a trie. This automaton can be transformed into a Levenshtein automaton, 
      which supports efficient search for words within a given edit distance.



- Keeping Everything in Memory

    The data structures discussed so far in this chapter have all been answers to the limitations of disks. 
      Compared to main memory, disks are awkward to deal with. With both magnetic disks and SSDs, data on disk 
      needs to be laid out carefully if you want good performance on reads and writes. However, we tolerate 
      this awkwardness because disks have two significant advantages: they are durable (their contents are not 
      lost if the power is turned off), and they have a lower cost per gigabyte than RAM.

    As RAM becomes cheaper, the cost-per-gigabyte argument is eroded. Many datasets are simply not that big, 
      so it’s quite feasible to keep them entirely in memory, potentially distributed across several machines. 
      This has led to the development of in-memory databases.

    Some in-memory key-value stores, such as Memcached, are intended for caching use only, where it’s acceptable 
      for data to be lost if a machine is restarted. But other in-memory databases aim for durability, which can 
      be achieved with special hardware (such as battery-powered RAM), by writing a log of changes to disk, by 
      writing periodic snapshots to disk, or by replicating the in-memory state to other machines.

    When an in-memory database is restarted, it needs to reload its state, either from disk or over the network 
      from a replica (unless special hardware is used). Despite writing to disk, it’s still an in-memory database, 
      because the disk is merely used as an append-only log for durability, and reads are served entirely from 
      memory. Writing to disk also has operational advantages: files on disk can easily be backed up, inspected, 
      and analyzed by external utilities.

    Products such as VoltDB, MemSQL, and Oracle TimesTen are in-memory databases with a relational model, and 
      the vendors claim that they can offer big performance improvements by removing all the overheads associated 
      with managing on-disk data structures. RAMCloud is an open source, in-memory key-value store with durability 
      (using a log-structured approach for the data in memory as well as the data on disk). Redis and Couchbase 
      provide weak durability by writing to disk asynchronously.

    Counterintuitively, the performance advantage of in-memory databases is not due to the fact that they don’t 
      need to read from disk. Even a disk-based storage engine may never need to read from disk if you have enough 
      memory, because the operating system caches recently used disk blocks in memory anyway. Rather, they can be 
      faster because they can avoid the overheads of encoding in-memory data structures in a form that can be 
      written to disk.

    Besides performance, another interesting area for in-memory databases is providing data models that are 
      difficult to implement with disk-based indexes. For example, Redis offers a database-like interface to 
      various data structures such as priority queues and sets. Because it keeps all data in memory, its 
      implementation is comparatively simple.

    Recent research indicates that an in-memory database architecture could be extended to support datasets larger 
      than the available memory, without bringing back the overheads of a disk-centric architecture. The so-called 
      anti-caching approach works by evicting the least recently used data from memory to disk when there is not 
      enough memory, and loading it back into memory when it is accessed again in the future. This is similar to 
      what operating systems do with virtual memory and swap files, but the database can manage memory more 
      efficiently than the OS, as it can work at the granularity of individual records rather than entire memory 
      pages. This approach still requires indexes to fit entirely in memory, though (like the Bitcask example at 
      the beginning of the chapter).
---------------------------------------------------------------------------
CHAPTER 3 - STORAGE AND RETRIEVAL
---------------------------------------------------------------------------

- The World's Simplest Database

    #!/bin/bash

    db_set () {
        echo "$1,$2" >> database
    }

    db_get () {
        grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
    }


    These two functions implement a key-value store. You can call 'db_set key value', which will 
      store key and value in the database. The key and value can be (almost) anything you like—
      for example, the value could be a JSON document. You can then call 'db_get key', which looks 
      up the most recent value associated with that particular key and returns it.

    $ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'

    $ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'

    $ db_get 42
    {"name":"San Francisco","attractions":["Golden Gate Bridge"]}



- The Simplest Database is a Log

    Our 'db_set' function actually has pretty good performance for something that is so simple, 
      because appending to a file is generally very efficient. Similarly to what 'db_set' does, 
      many databases internally use a log, which is an append-only data file. 

    Real databases have more issues to deal with (such as concurrency control, reclaiming disk 
      space so that the log doesn’t grow forever, and handling errors and partially written 
      records), but the basic principle is the same. Logs are incredibly useful, and we will 
      encounter them several times in the rest of this book.

    Note that the word log is often used to refer to application logs, where an application outputs 
      text that describes what’s happening. In this book, log is used in the more general sense: 
      an append-only sequence of records. It doesn’t have to be human-readable; it might be binary and 
      intended only for other programs to read.



- Retrieving Values From the Simplest Database

    On the other hand, our 'db_get' function has terrible performance if you have a large number of 
      records in your database. Every time you want to look up a key, db_get has to scan the entire 
      database file from beginning to end, looking for occurrences of the key. In algorithmic terms, 
      the cost of a lookup is O(n): if you double the number of records n in your database, a lookup 
      takes twice as long. That’s not good.

    In order to efficiently find the value for a particular key in the database, we need a different 
      data structure: an 'index'. In this chapter we will look at a range of indexing structures and 
      see how they compare; the general idea behind them is to keep some additional metadata on the 
      side, which acts as a signpost and helps you to locate the data you want. If you want to search 
      the same data in several different ways, you may need several different indexes on different 
      parts of the data.

    An index is an additional structure that is derived from the primary data. Many databases allow 
      you to add and remove indexes, and this doesn’t affect the contents of the database; it only 
      affects the performance of queries. Maintaining additional structures incurs overhead, especially 
      on writes. For writes, it’s hard to beat the performance of simply appending to a file, because 
      that’s the simplest possible write operation. Any kind of index usually slows down writes, because 
      the index also needs to be updated every time data is written.

    This is an important trade-off in storage systems: well-chosen indexes speed up read queries, but 
      every index slows down writes. For this reason, databases don’t usually index everything by 
      default, but require you—the application developer or database administrator—to choose indexes 
      manually, using your knowledge of the application’s typical query patterns. You can then choose 
      the indexes that give your application the greatest benefit, without introducing more overhead 
      than necessary.



- Hash Indexes

    Let’s start with indexes for key-value data. This is not the only kind of data you can index, but 
      it’s very common, and it’s a useful building block for more complex indexes.

    Key-value stores are quite similar to the dictionary type that you can find in most programming 
      languages, and which is usually implemented as a hash map (hash table). Since we already have hash 
      maps for our in-memory data structures, why not use them to index our data on disk?

    Let’s say our data storage consists only of appending to a file, as in the preceding example. Then 
      the simplest possible indexing strategy is this: keep an in-memory hash map where every key is 
      mapped to a byte offset in the data file—the location at which the value can be found. Whenever 
      you append a new key-value pair to the file, you also update the hash map to reflect the offset 
      of the data you just wrote (this works both for inserting new keys and for updating existing keys). 
      When you want to look up a value, use the hash map to find the offset in the data file, seek to 
      that location, and read the value.



- Hash Indexes in Bitcask

  This hash indexing strategy may sound simplistic, but it is a viable approach. In fact, this is 
    essentially what Bitcask (the default storage engine in Riak) does. Bitcask offers high-performance 
    reads and writes, subject to the requirement that all the keys fit in the available RAM, since the 
    hash map is kept completely in memory. The values can use more space than there is available memory, 
    since they can be loaded from disk with just one disk seek. If that part of the data file is already 
    in the filesystem cache, a read doesn’t require any disk I/O at all.

  A storage engine like Bitcask is well suited to situations where the value for each key is updated 
    frequently. For example, the key might be the URL of a cat video, and the value might be the number 
    of times it has been played (incremented every time someone hits the play button). In this kind of 
    workload, there are a lot of writes, but there are not too many distinct keys—you have a large number 
    of writes per key, but it’s feasible to keep all keys in memory.



- Compacting

    As described so far, we only ever append to a file — so how do we avoid eventually running out of 
      disk space? A good solution is to break the log into segments of a certain size by closing a 
      segment file when it reaches a certain size, and making subsequent writes to a new segment file. 
      We can then perform 'compaction' on these segments. Compaction means throwing away duplicate keys in 
      the log, and keeping only the most recent update for each key.


    Data File Segment 1
    -----------------
    mew: 1078   
    purr: 2103  
    purr: 2014  
    mew: 1079   
    mew: 1080   
    mew: 1081
    purr: 2105  
    purr: 2106  
    purr: 2107  
    yawn: 511   
    purr: 2108  
    mew: 1082

    Data File Segment 2
    -----------------
    purr: 2109  
    purr: 2110  
    mew: 1083   
    scratch: 252
    mew: 1084
    mew: 1085
    purr: 2111
    mew: 1086
    purr: 2112
    purr: 2113
    mew: 1087
    pur: 2114

    Merged Segments 1 and 2
    ----------------
    yawn: 511
    scratch: 252
    mew: 1087
    purr: 2114


    Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to 
      find the value for a key, we first check the most recent segment’s hash map; if the key is 
      not present we check the second-most-recent segment, and so on. The merging process keeps 
      the number of segments small, so lookups don’t need to check many hash maps.



- Implementation Details of Hash Indexes

    - File Format 

        CSV is not a good format for a log, it's faster and simpler to use a binary format that 
          first encodes the length of a string in bytes, followed by the raw string

    - Deleting Records

        If you want to delete a key and its associated value, you have to append a special deletion 
          record (a 'tombstone').  When log segments are merged, the tombstone tells the merging 
          process to discard any previous values for the deleted key

    - Crash Recovery

        If the database is restarted, the in-memory hash maps are lost.  In principle, you can 
          restore each segment's hash map by reading the entire segment file from beginning to end.
          However, that might take a long time if the segment files are large, making server 
          restarts painful.  Bitcask speeds up recovery by storing a snapshot of each segment's
          hash map on disk, which can be loaded into memory more quickly.

    - Partially Written Records

        The database may crash at any time, incluing halfway through appending a record to the 
          log.  Bitcask files include checksums, allowing such corrupted parts of the logs to
          be detected and ignored.

    - Concurrency Control

        As writes are appended to the log in a strictly sequential order, a common implementation
          choice is to have only one writer thread.  Data file segments are append-only and 
          otherwise immutable, so they can be read concurrently by multiple threads.



- Advantages of Hash Indexes

    1. Appending and segment merging are sequential write operations, which are generally much
         faster than random writes, especially on magnetic hard drives.

    2. Concurrency and crash recovery are much simpler if segment files are append-only or
         immutable.  For example, you don't have to worry about the case where a crash 
         happened while a value was being overwritten, leaving you with a file containing 
         part of the old and part of the new values spliced together.

    3. Merging old segments avoids the problem of data files getting fragmented over time.



- Limitations of Hash Indexes

    1. The hash table must fit in memory, so if you have a very large number of keys, this 
         approach will not work.  It is difficult to make an on-disk hash map work well, because
         it requires a lot of random I/O, is expensive to grow, and collisions require
         fiddly logic.

    2. Range queries are not efficient.  For instance, you cannot easily scan over all keys
         between 'kitty0000' and 'kitty9999'.  You'd have to look up each key individually.
-----------------------------------------------------------------------------
| CHAPTER 6 - CACHING                                                       |
-----------------------------------------------------------------------------

- Latency

    - Here are the access times for different types of resources:

        Operation Type                                                       Approximate Time
        ---------------------------------------------------------------------------------------
        Time to access local memory                                          100 ns

        SSD disk seek                                                        100,000 ns

        Time of a network packet round trip within the same data center      500,000 ns

        Disk seek (non-SSD)                                                  10,000,000 ns

        Read 1MB sequentially from network                                   10,000,000 ns

        Read 1MB sequentially from disk (non-SSD)                            30,000,000 ns

        Time of a network packet round trip across Atlantic                  150,000,000 ns

        How many nanoseconds in a single second                              1,000,000,000 ns



- Caching

    - Caching is winning the battle without the fight.  It can be used to increase performance and
        scalability at low cost, without having to recompute responses.


    - Caching is a very popular technique in computer science.  Here are a few examples:

        - CPU caches
        - Hard drive caches
        - Linux OS file caches
        - database query caches
        - DNS caches
        - HTTP browser caches
        - HTTP proxies and reverse proxies
        - different types of application object caches


    - Instead of fetching data and computing a response every time a request is received, we can build
        the result once and store it in the cache.  Subsequent requests are satisfied by returning the
        cached result until it expires or is explicitly deleted.



- Cache Hit Ratio

    - Three main factors affect your cache hit ratio:

        1. Data set size
        2. Space
        3. Longevity


    - Your cache key space is all the possible unique keys your application could generate.  The more
        unique keys your application generates, the less chance you have to use any one of them.

      If you want to cache weather forecast data based on the client's IP, you would have 4 billion 
        possible cache keys.  If you cache the weather forecast based on country, you have less than
        300 keys.  The fewer possible keys, the better the cache efficiency.


    - The second factor is the number of items you can store in your cache without running out of
        space.  This depends on the size of the cache and the size of the items.

      Because caches are usually stored in memory, space is relatively limited and expensive.  If you
        run out of space, you'll have to evict entries before you can add new ones.  The more objects
        you can fit in your cache, the better your cache hit ratio.


    - The third factor is how long, on average, each object can be stored in the cache without 
        expiring or being invalidated.

      In some caches, you can cache objects for a predefined amount of time using TTL.  However, in
        other cases, you may not be willing to serve stale data.  For instance, if prices are changed
        in an ecommerce site, we cannot show the old prices, so we have to invalidate the cache
        entries.


    - Use cases with a high read-to-write ratio are usually good candidates for caching.



- Caching Based on HTTP

    - The HTTP protocol has been around for a long time, and over the years the specification has
        changed to allow different parts of the infrastructure to cache HTTP responses.

      This makes HTTP caching a bit more difficult to understand, and it's also the reason there are
         many different HTTP headers related to caching (and even some HTML metatags!).


    - All HTTP layer caches are 'read-through caches'.  A read-through cache is a caching component
        that can return cached data or fetch data from the client if necessary.

      This means that the client connects to the read-through cache, rather than the origin server
        that generates the actual response.


    - The cache is always meant to be an intermediate (aka a 'proxy'), transparently adding caching
        functionality to HTTP connections.  Clients are not able to distinguish whether they received
        a cached object or not.



- HTTP Headers

    - Simple HTTP request:

        # Request
        GET / HTTP/1.1
        Host: www.example.org
        Accept-Encoding: gzip,deflate
        Connection: keep-alive

        - 'Accept-Encoding: gzip,deflate' defines which compressed data encodings browser supports
        - 'Connection: keep-alive' means that server should keep TCP connection open for addl downloads


    - And here is the response:

        # Response
        HTTP/1.1 200 OK
        Content-Encoding: gzip
        Content-Type: text/html; charset=UTF-8
        Content-Length: 9381
        Connection: close

        ... response body with contents of the page ...

        - 'Content-Encoding: gzip' indicates the server decided to return a gzip response
        - 'Connection: close' means the client's request to keep the connection open was rejected



- The 'Cache-Control' Header

    - The 'Cache-Control' header was added to the HTTP/1.1 spec and is now supported by most browsers
        and caching packages.  Cache-Control allows you to specify multiple options:

        Cache-Control: no-cache, no-store, max-age=0, must-revalidate


    - The most important Cache-Control options that web servers may include in their responses:

        private                # Private to user that requested it, cannot be served to any other user

        public                 # Can be shared among users as long as not expired

        no-store               # Response should not be persisted to disk (for sensitive information)

        no-cache               # Response should not be cached (means that cache should always ask server)

        max-age                # How many seconds response can be served from cache before becoming stale
                               #   (defines TTL of response)

        no-transform           # Includes response should be served without modifications

        must-revalidate        # Once response becomes stale, it cannot be returned to clients without
                               #   revalidation


    - A cached object is 'fresh' until it's expiration time passes.  After that, it becomes 'stale',
        but can still be returned to clients if they explicitly allow stale responses.

    - Clients can also use the 'Cache-Control' header in requests, but it is rarely used and has
        slightly different semantics.



- The 'Expires' Header

    - The 'Expires' header allows you to specify an absolute point in time when the object becomes
        stale.

        Expires: Sat, 23 Jul 2015 13:14:28 GMT


    - Unfortunately, setting the TTL in multiple different headers leads to confusion and inconsistent
        behavior.  So, it's a best practive to set it in one place and stick with it.



- The 'Vary' Header

    - The 'Vary' header tells caches that you may need to generate multiple variations of the response
        based on some HTTP request headers.

      The most common use for this is to indicate that clients that support gzip will get a 
        compressed response, while clients that don't support gzip will not.

        Vary: Accept-Encoding



- First Caching Scenario - Cache a Response Forever

    - We want to use this technique for all static content.  Static content files should be considered
        immutable.  If we need to change them, we should create a new URL.

      For instance, you can bundle and minify your CSS files, then include a timestamp or hash of the
        contents in the URL.


    - This way, the files can always be cached safely by any server.  Users won't get stale versions,
        since clients with the new version will point to a new URL.


    - Even if we intend to cache files forever this way, we still shouldn't set the 'Expires' header 
        for more than a year in the future (since the HTTP spec doesn't allow it).

        # Headers for static asset, expires one year from now
        Cache-Control: public, no-transform
        Expires: Sat, 23 Jul 2016 13:14:28 GMT
        Vary: Accept-Encoding



- Second Caching Scenario - Content that is Never Cached

    - In this case, we want to make sure the HTTP response is never stored, cached, or reused for any
        users.  We have included the 'Pragma: no-cache' header also, so that older clients can 
        understand the intention of not caching the response.

        Cache-Control: no-cache, no-store, max-age=0, must-revalidate
        Expires: Fri, 01 Jan 1990 00:00:00 GMT
        Pragma: no-cache



- Third Caching Scenario - Cache for a User Only

    - In this case, we want to cache responses for a given user only.  For instance, we want to cache a
        user's profile page for that user.


    - We can still use the full page cache, but it will have to be a private cache to ensure users only
        see their own pages.

        Cache-Control: private, must-revalidate
        Expires: Sat, 23 Jul 2015 13:14:28 GMT
        Vary: Accept-Encoding



- Using HTML Metatags for Caching

    - There are some old HTML meta tags for caching.  It is best to avoid using them altogether.

        <meta http-equiv="cache-control" content="max-age=0" />
        <meta http-equiv="cache-control" content="no-cache" />
        <meta http-equiv="expires" content="Tue, 01 Jan 1990 1:00:00 GMT" />
        <meta http-equiv="pragma" content="no-cache" />



- Types of HTTP Cache Technologies

    - There are 4 main types of HTTP caches:

        1. Browser cache
        2. Caching proxies
        3. Reverse proxy
        4. Content delivery networks



- Browser Caches

    - All modern browsers have some kind of cache, which usually uses a combination of memory and local
        files.

    - Whenever an HTTP request is about to be sent, the browser can check the cache for a valid version
        of the resource.  If the item is present and not expired, it can be resued without having to
        send the request.



- Caching Proxies

    - A 'caching proxy' is a server usually installed in a local corporate network or by an ISP.  It
        is a read-through cache used to reduce the amount of traffic generated by the users of the 
        network by reusing responses.


            [Office Building]                       [Data Center]

             Client
                |
                v
            HTTP Proxy Server   ->   Internet   ->   Web Server


    - The larger the network, the bigger the potential savings.  This is why it was quite commong among
        ISPs to install transparent caching proxies, then route all HTTP traffic through them to cache
        as many requests as possible.

    - In recent years, installing local proxy servers has become less popular, as bandwidth becomes
        cheaper and more websites serve all resources over SSL (so cannot be intercepted by proxy).



- Reverse Proxy

    - A 'reverse proxy' works just like a regular caching proxy, but the intent is to place it in your
        own data center to reduce load put on your web servers.

            [Office Building]                   [Data Center]

             Client    ->    Internet    ->     HTTP Proxy Server
                                                       |
                                                       v
                                                  Web Server


    - For some applications, reverse proxies are an excellent way to scale.  They also give you more
        flexibility, since they can override HTTP headers for better control over which requests get
        cached and for how long.


    - Reverse proxies are also a great way to scale your web services layer, since the results of
        the REST requests can also be cached.


        Client  ->  Front-end Web Server  ->  Reverse HTTP Proxy  ->  Web Services



- Content Delivery Networks

    - A CDN is a distributed network of cache servers that work in a similar way to caching proxies.
        They depend on the same HTTP headers, but they are controlled by the CDN service provider.

    - By using a CDN, you reduce load on your own servers, save on network bandwidth, and improve 
        the user experience.  



- Scaling HTTP Caches

    - Scaling browser caches, third party proxy caches, or CDNs is out of our control.  We can scale
        out reverse proxy servers.


    - Popular Open Source Reverse Proxy Solutions:

        - Nginx
        - Varnish
        - Squid
        - Apache mod_proxy
        - Apache Traffic Server


    - The most common method for evicting old cache entries is 'Least Recently Used' (LRU), which allows 
        the cache server to eventually remove rarely accessed objects and keep hot items in memory to
        maximize cache hit ratio.


    - Here is an example of using 2 layers of reverse proxies to scale:

                                      External Traffic
                                              -
                                              v
                --------------------------------------------------------------
                [Data Center]          Load Balancer
                                         |       |
                                         v       v
                                    Reverse Proxy Servers
                                         |       |
                                         v       v
                                     Front End Servers
                                         |       |
                                         v       v
                                   Reverse Proxy Servers (caching and load balancing)
                                         |       |
                                         v       v
                                        Web Services


    - Note that the caching layer for the front-end servers is often a CDN.


    - SSD drives can sometimes be useful when scaling out a caching cluster.  If you switch your cache
        from memory to disk (typically because you have a large pool of objects and need a better hit
        rate), you have far more storage to use.

      Faster random access times of SSDs will give you a 10x+ speedup, and since the data is desinged to
        be disposable anyways, the high rate of failures dones't matter as much.



- Nginx

    - Nginx is a good general purpose reverse-proxy solution.

    - Uses asynchronous processing, which allows it to proxy tens of thousands of concurrent 
        connections with a very low per-connection overhead.

    - Nginx is also a FastCGI server, which means you can run your web application on the same web
        server stack as your reverse proxies.

    - Nginx can act as a load balancer.  It supports multiple forwarding algorithms and many advanced
        features, such as SPDY, WebSockets, and throttling.  It can also override headers to change
        caching behavior.



- Caching Application Objects

    - After HTTP-based caches, the second most important component in a web application is usually a
        custom object cache.

      Object caches are 'cache-aside' rather than read-through.  The application is aware of the 
        object cache, and actively uses it to store and retrieve.  


    - Basically, we can think of object caches as key-values stores with support of object expiration.


    - Common types of object caches:

        1. Client-side cache

        2. Caches co-located with code

        3. Distributed object caches



- Client-Side Caches

    - Years ago, it was impossible for JavaScript to store data on the client's machine, but most
        modern browsers support the WebStorage specification.  The client-side can cache ~5-25 MB
        of data, which can take a lot of pressure off of your infrastructure.

        // Store object in local storage
        var preferences = {/* data object to be stored */};
        localStorage.setItem('preferences', JSON.stringify(preferences));

        // Get object from local storage
        var cachedData = localStorage.getItem('preferences');
        var preferences = JSON.parse(cachedData);


    - SPAs make much more use of local storage.  Applications written for mobile device platforms
        use similar techniques.

    - The client side is responsible for cache refresh and invalidation.



- Caches Co-located with Code

    - This type of object cache is located directly on your web servers.  It is implemented in one of
        the following ways:

        1. Objects are cached directly in the application's memory.

        2. Objects are stored in shared memory segments so that multiple processes running on the same
             machine can access them.  This is less common in multithreaded environments like Java,
             where all execution threads run within a single process.

        3. A caching server is deployed on each web server as a separate application.  The application
             uses the cache's normal interface to communicate with it.  This is common in single-server
             deployments.  Memcached and Redis are common choices.


    - Local caches are typically not replicated or synchronized between servers.  This makes them very
        easy to deploy, but it has some drawbacks:

        1. All of the servers will duplicate the same data, so this approach doesn't scale.

        2. Caches cannot be kept consistent, and you cannot remove objects from them efficiently.



- Distributed Object Caches

    - With this type of cache, a network round trip to the cache server is required.  Redis and Memcached
        are the most common choices by far.

    - Just connect to the cache for cache invalidation.


    - Using dedicated cache servers is a good way to push responsibility out of your application.
        For instance, Redis allows for data persistence, replication, and efficient implementations of
        counters, lists, and object sets.

      They are heavily optimized when it comes to memory management, and they take care of things like
        object expirations and evictions.


    - Cache servers usually use the LRU algorithm to decide which objects should be removed from the
        cache when they reach a memory limit.


    - Distributed caches are powerful scalability tools, but adding them to your system does add
        complexity and management overhead.  It is usually simpler to start with a single cache server.



- Scaling Object Caches

    - Browser caches cannot be scaled.  Local caches can usually only scale by falling back to the
        file system.

      Distributed object caches are usually scaled using data partitioning.  In Memcached and Redis,
        this partitioning is the responsibility of the client.


    - Memcached distributes a client that uses a consistent hashing algorithm to automatically
        distribute data among the cache servers.  This allows for a shared-nothing approach.

    - Redis allows for master-follower replication, which is useful if you need a lot of read-only
        replicas.

    - AWS Elastic Cache is just a hosted version of these.  It allows you to choose whether you want to
        use Redis or Memcached.



- Caching Rules of Thumb - Caching High Up the Call Stack

    - The higher up the call stack you can cache, the more resources you can save.


            Client Caches (HTTP and Object caches)                    # Saved 100% of resources
                            |
                            |  (Web Requests)
                            v
            HTTP Reverse Proxies/CDN                                  # Saved 98% of resources
                            |
                            |  (Web Requests)
                            v
            Web Application Servers (Local and distributed caches)    # Saved 75% of resources
                            |
                            |  (Web Service Requests)
                            v
            HTTP Reverse Proxies                                      # Saved 66% of resources
                            |
                            |  (Web Service Requests)
                            v
            Web Service Servers (Local and distributed caches)        # Saved 50% of resources
                            |
                            |  (Web Service Requests)
                            v
            Main Data Store                                           # Saved 0% of resources



- Caching Rules of Thumb - Reuse Cache Among Users

    - As an example, imagine we have a service that returns restaurants close to users.  A simple
        implementation would be to check the user's coordinates, then add a cache entry with the
        user's coordinates as the key, and the list of restaurants as the value.


    - The problem with this is that the cache entry will change every time the user takes a step.

      A better approach would be to round the GPS coordinates to 3 decimal points so that users on
        the same street block could reuse the same results.



- Caching Rules of Thumb - Where to Start Caching?

    - Using some kind of profiling of your requests can be very useful to figure out where caching is
        needed most.  This is often a better approach than just guessing where you need it.



- Caching Rules of Thumb - Cache Invalidation is Difficult

    - When you start with a simple site, this is easy.  You add an object to a cache, and any time the
        data changes, you go back to the cache and remove the stale object.

      However, cached objects are usually the result of computation that takes multiple data sources as
        input.  This means that when data changes, you need to invalidate all of the cached objects that
        have used that data as input.  If you have multiple instances, this has to happen on all
        instances.


    - For instance, let's say you have an ecommerce app, and you are very aggressive about caching
        everything in an object cache.  You cache query results for paginated product lists, keyword
        searches, category and product pages.

      The problem is that every time a new product is added, all of this becomes invalid.


    - The best alternative to cache invalidation is to just set a TTL on your cached objects.

    - Due to their temporary nature, caching issues are usually difficult to reproduce and debug.
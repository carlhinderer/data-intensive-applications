-----------------------------------------------------------------------------
| CHAPTER 5 - DATA LAYER                                                    |
-----------------------------------------------------------------------------

- Scaling with MySQL - Replication

    - MySQL allows you to synchronize the state of 2 servers, a master and a replica.  Applications
        can read from the replica, but can only write to the master.

    - The leader records the data-changing statements in a log file called a 'binlog', together with
        a timestamp, and also assigns a sequence number to each statement.  Once a statement has been
        written to binlog, it can be sent to replicas.

    - At any point, the replica can connect to the master and ask for an incremental update of the
        master's binlog file.  In the request, it provides the sequence number of the last command it
        saw.

    - Note that this is asynchronous replication.  For this reason, you can have multiple replicas
        if you have a read-heavy load.

    - The master doesn't keep track of replicas.  If you want to keep track of them and whether
        they're alive, you'll need a separate application for that.



- Rebuilding From a Replica Failure

    - To rebuild a failed replica (this is a manual process):

        1. Take a database backup from the master or another replica.
        2. Connect to the master to catch up.  This might take hours in a busy DB or with an old backup.



- Rebuilding From a Master Failure

     - MySQL doesn't support automatic failover or any mechanism of automated promotion of a replica
         to a master.  You need to:

        1. Figure out which replica has the highest sequence number and make it the master.

        2. If you have multiple replicas, you need to make them identical to the new master, either by
             rebuilding them from a backup of the new master, or by manually tweaking binlog files to
             get the exact same state.

        3. You need to configure all remaining replicas to read from the new master.


    - To get high availability in the event of a master failure, we can use a master-master topology,
        where 2 masters both accept writes, and also replicate each other's binlogs.  To avoid race
        conditions, a client should only write to one master at a time.

      This approach doesn't improve scalability, since both masters still have to perform all the
        writes, and they have all the I/O overhead from replicating each other also.  Also, you need
        to take precautions to avoid data inconsistency, and this erases any performance gains from
        having multiple servers.



- Replication Challenges

    - 'Replication lag' is a measurement of how far behind a replica is from a master.  If you have
        a decent network or cloud solution, this should generally be less than 1s.

    - Replication is only useful in scaling reads.  It will not help you scale writes, no matter what
        topology you use.

    - Replication is only useful in scaling out the number of reads, not the amount of data.  This is
        because an entire copy of the data must exist on each machine.

    - Multilevel replication can be used if you really want to scale reads.

    - The concept of 'active data' says we should think about which of our data is used most often.
        For instance, in an e-commerce system, recent transactions from the last 48 hours will be
        accessed much more often than older transactions.

    - Since replication is asynchronous, replicas can return stale data.

    - Since replicated MySQL is difficult to get right, and prone to hard to debug errors, it makes
        a lot of sense to use a hosted solution.



- Data Partitioning (Sharding)

    - Data partitioning (aka sharding) is dividing data into smaller buckets and assigning each
        bucket to its own server.  A sharding key is picked that will always be present to uniquely
        determine which server the data is on.

    - For example, if you have an online store, you could just split data by UserId, and that would
        be your sharding key.

    - We can set MySQL up like this, and the servers themselves wouldn't need any special configuration.
        They all have the same schema and function as normal.  In this case, all the sharding logic
        lives in the application.

    - Since each server will auto-increment ids individually, you may get records with the same id
        in multiple servers.  In some cases this is acceptable, but if you need globally unique ids,
        you'll need to manually set the auto-increment offsets.

    - If you use a non-random sharding key (ie Country), your data will be skewed and not evenly
        distributed.  Ideally, we want to split our data into buckets of equal size.



- Advantages of Sharding

    - Sharding is how you scale data systems horizontally.  Once your data is too large to live on a
        single server, you need this approach.

    - If you split the data into completely disjoint sets, you have a shared-nothing architecture and 
        no overhead of communication between the servers.

    - Nothing needs to be changed about the database instances themselves and applications can
        handle all of the sharding logic.



- Challenges of Sharding

    - The logic for the routing adds complexity to the application tier.  If you want to run queries
        that span multiple shards, you need to run them separately, then join the data in the 
        application tier.

    - You lose ACID semantics if you make changes across multiple shards.

    - If you use modulo-based mapping, you'll have to migrate large amounts of data if you change
        the number of servers.

    - To avoid that large migration, you could keep a separate database that stores the shard for
        each key.  Then, you can add servers without migrating data or migrate slowly over time.
        You could use another MySQL instance for this.

                             Global Master
                               /       \
                          Shard 1    Shard 2
                          |     |    |     |
                       Rep 1  Rep1  Rep2  Rep2

    - If you already have another high speed data store, it may be more performant to put the shard
        mappings there, rather than another MySQL instance.


    - Another strategy is to always just split a shard in 2 if it gets too big.  That way only a
        single instance will need to be rebalanced.  

      For instance, you start out with 2 servers, but you esitmate you won't need more than 32.  Any
        time you need to, just double your capacity.


    - To deal with the auto-incrementing id problem, you just set the 'auto_increment_offset' for 
        each DB instance.  For instance, if you have 2 shards:

        Shard 1:  auto_increment_increment=2, auto_increment_offset=1
        Shard 2:  auto_increment_increment=2, auto_increment_offset=2

      Then, odd ids go to shard 1, and even ids go to shard 2.

      Alternatively, some data stores (ie Redis) provide atomic counters to solve this problem.


    - Once again, this approach involves a lot of complexity, and it is likely a cloud-hosted solution
        would be a good alternative.  Cloud offerings manage sharding, shard management, data migration,
        and cross-shard query execution.



- Putting it All Together

    - Once again, scalability can be boiled down to 3 underlying techniques:

        1. Scaling by adding copies of the same thing
        2. Functional partitioning
        3. Data partitioning


    - Example - Scaling an E-commerce Website

        1. Split the functionality into 2 services, 'ProductCatalogService' and 'CustomerService'.
             They use separate databases, the read-heavy product catalog and the write-heavy customer
             service.  (Functional partitioning)

        2. You can easily replicate your ProductCatalog, since it can fit on a single server.
             (More copies of the same thing)

        3. Your customer database is sharded on user id.  (Data partitioning)



- ACID Transactions

    - 'ACID Transactions' refers to a set of properties supported by most relational DB engines.

        Atomiticy   = A transaction is executed in its entirety.  It either completes or rolls back.

        Consistency = Every transaction transforms the data from one consistent state to another,
                        with data conforming to all constraints enforced by the schema.

        Isolation   = Transactions run in parallel without affecting one another.

        Durability  = Data is persisted, so it cannot be lost even due to server failure.



- Scaling with NoSQL

    - We have used relational databases for decades, and were taught to use data normalization and
        ACID transactions.  They were the only option.  If you needed a large database, you needed
        expensive servers and software licenses.

    - In the 2000s, a series of groundbreaking white papers from the large web companies started to
        change this mindset.  Google's 'Google File System', 'MapReduce', 'BigTable', and Amazon's
        'Dynamo' are notable examples.

    - By 2010, these principles and design decisions had made their way into open source data stores
        like Cassandra, Redis, MongoDB, Riak, and CouchDB.  These technologies were build with
        scalability in mind from the ground up.

    - To determine whether you can use any of these options, you need to think carefully about what
        data guarantees you need and don't need.  Then, you can make the right tradeoffs when 
        picking a solution.



- CAP Theorem

    - Eric Brewer said in 1999 that any distributed data store can only provide 2 of the following 3:

        Consistency = Every read receives the most recent write or an error
                        (Note this is different from the relational idea of consistency, which
                           is focused on data constraints like foreign keys and uniqueness)

        Availability = Every request receives a non-error response

        Partitioning = The system continues to operate despite an arbitrary number of messages being
                         dropped by the network between nodes


    - For instance, if all available nodes need to be process incoming requests (availability),
        and they all need to return the same data (consistency), then they cannot tolerate a
        network failure since they cannot propogate changes to other nodes.

    - In reality, there are ways to tune both consistency and availability to get the tradeoffs we
        want, and it's more complicated than 'pick 2 out of 3'.  Brewer's 2012 paper 'CAP 12 Years
        Later' addresses this detail.



- The Rise of Eventual Consistency

    - Some NoSQL data stores choose to sacrifice some consistency guarantees to make scalability
        easier.  For instance, Amazon's Dynamo data store (built to handle thier shopping carts) did
        this.

    - Rather than enforcing full consistency in distributed transactions, Amazon decided high 
        availability was more important.  Based on these priorities, Amazon sacrifieced complex
        queries, simplified the data model, and introduced eventual consistency.

    - With 'eventual consistency', different nodes may have different versions of the data, but state
        changes eventually propogate through all the servers.  Any given replica may be lagging behind.

    - If multiple servers allow writes, there may be conflicts that have to be resolved.  The easiest
        way to resolve them is to say 'most recent write wins', but this can lead to some data loss.


    - Alternatively, some data stores like Dynamo push the responsibility for conflict resolution onto
        it's clients.  Any time a client asks for data, they provide all the conflicting versions of
        the data and let the client decide how to resolve the conflict.

      For instance, if there are multiple versions of a user's shopping cart, merge them together with
        all the products and let the user decide if they want to remove some of them.  That way, you
        don't have to pick one winning version of the cart.



- Cassandra's Self-Healing Strategy

    - Many different scenarios can result in replicas having different data from the master.  (For
        instance, human errors, application bugs, hardware issues).  To help deal with this, some
        data stores use additional self-healing strategies.

    - For example, 10% of reads sent to Cassandra trigger a background 'read repair' mechanism.  After
        a response is sent to the client, the Cassandra node fetches the requested data from all of
        the replicas, compares their values, and sends updates back to any node with stale or
        inconsistent data.

    - This adds overhead, but makes the system much more resilient to failures.



- Quorum Consistency

    - Cassandra allows you to fine-tune consistency guarantees by specifying the consistency level of
        each query indepdently.  

    - 'Quorum consistency' is a good way to trade latency for consistency in eventually consistent
        data stores.  You need to wait longer for the majority of servers to respond, but you get the
        freshest data.

    - When you write using quorum consistency, the majority of servers need to respond that they have
        persisted your change.

    - When you read, the majority of servers need to respond so that the most up-to-date copy of the 
        data can be found and returned to the client.

    - If write certain data using quorum consistency, then you always read it using quorum consistency,
        you are guaranteed to always have the most up-to-date data and thus regain the
        read-after-write semantics.



- Faster Recovery to Increase Availability

    - While some databases like Dynamo and Cassandra trade some of their consistency guarantees in 
        favor of high availability, some other data stores trade some of their high availability for
        consistency.

    - Rather than guaranteeing that all clients can read and write all of the time, some data
        designers decided to focus more on quick failure recovery rather than sacrificing global
        consistency.


    - In MongoDB, data is automatically sharded and distributed among multiple servers.  Each piece
        of data belongs to a single server, and anyone who wants to update data needs to talk to the
        server responsible for the data.

      If a server becomes unavailable, MongoDB rejects all writes to the data that the failed server
        was responsible for.


    - To increase high availability, MongoDB supports replica sets, and it is recommended to set up
        each of the shards as a replica set.  When a primary node fails, an election process picks a
        new primary, enabling prompt and automatic failover.

    - Note that although MongoDB is a 'CP database', replication is asynchronous, so it is still not
        like a relational database in consistency.



- Cassandra Topology

    - Cassandra was originally built at Facebook, as a merger of design patterns from BigTable
        (developed at Google) and Dynamo (developed at Amazon).

    - All nodes are functionally equal.  There is no single point of failure, and all nodes perform
        the exact same functions.  Clients can connect to any node, and that node becomes the client's
        'session coordinator', which takes responsiblity for all cluster activites like replication
        or sharding.

    - Clients issue queries to any node without knowledge about the topology of the cluster.  Each of
        the Cassandra nodes know the status of all other nodes and what data they have, so they can
        delegate queries to the correct servers.

    - Cassandra performs data partitioning automatically so that each of the nodes gets a subset of
        the overall data set, and the nodes communicate among each other to make sure they all know
        where each part of the data lives.


    - The Cassandra data model is based on a 'wide column', similar to Google's BigTable.  In a wide
        column model, you create tables, and then each table can have an unlimited number of rows.

      Tables are not connected, each table lives independently, and Cassandra does not enforce any
        relationships between tables or rows.


    - Cassandra tables are also defined in a different way than relational DBs.  Different rows may
        have different columns, and they may live on different servers in the cluster.  This 
        schema-on-read approach gives you a lot of flexibility.

    - You get far less tools for searching than with a relational DB.  To retrive a row, you need to
        know the row's key.


    - When you send a query to your session coordinator:

        1. The session coordinator hashes the row key to a number
        2. Based on the number, it finds the partition range your row key belongs to (the correct shard)
        3. It looks up which server is responsible for that partition range and delegates the query to it


    - For replication, you can specify how many copies of each piece of data to keep across the cluster,
        and session coordinators are responsible for ensuring the correct number of replicas.

      Any time you write a query, the coordinator node forwards it to all of the servers resonsible for
        the corresponding partition range.


    - One tradeoff is that deletes are the most expensive operation in Cassandra.  This makes certain
        kinds of workflows impractical.  For instance, you don't want to use it as a queue.
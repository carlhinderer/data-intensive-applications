----------------------------------------------------
CHAPTER 1 - KEY PROJECT TYPES AND CONSIDERATIONS
----------------------------------------------------

- Major Data Project Types

    1. Data Pipelines and Data Staging

         These are ETL-type projects that provide the basis for performing subsequent analysis and
           processing of data.


    2. Data Processing and Analysis

         These are projects that end in providing some kind of actionable value, like the creation of
           reports or machine learning models.


    3. Applications

         These are data frameworks meant to support live operational needs of applications, like the
           data backends for web or mobile applications.



- Aspects of the Project Types

    1. Primary Considerations

         Each of the 3 project types have distinctions that will affect architectural decisions and 
           priorities.  The decisions will drive the rest of the project.  


    2. Risk Management

         Each project type has a set of associated risks.  There are often multiple approaches to risk
           management based on specific use cases.


    3. Team Makeup

         The types of skills, experience, and interests will vary with the different project types, so
           we provide some recommendatations around building successful teams for each project type.


    4. Security

         Security is an extensive and important topic we can't go into a ton of detail on here, but 
           we'll enumerate the security concerns we need to keep in mind throughout our projects.



- Fundamental Security Dimensions

    1. Authentication

         This involves ensuring that users accessing the system are who they claim to be.  Any mature
           system should offer support for strong authentication.  This is typically done with an
           authentication protocol like Kerberos or LDAP.


    2. Authorization

         After ensuring that a valid user is accessing a system, we need to determine which data they're
           allowed to access.  A mature system will allow access to be controlled at different levels
           of granularity, such as at both the table-level and column-level.  


    3. Encrpytion

         We encrypt data to protect it from malicious users and intrusions.  We need to consider this from
           2 different angles:

           - Data at rest = This is data stored on disk.  Many vendors have solutions for managing this.

           - Data on the wire = This is data moving through the system.  Generally, this is supported via
               standard encryption mechanisms like TLS.


    4. Auditing

         We must be able to capture activity related to data in our system, including the lineage of the
           data, who is accessing it, how it is being used, and so on.  Again here, many vendors offer
           tools to manage this.



- Project Type #1: Data Pipelines and Data Staging

    - This project type has the widest scope of the 3, because it involves the path from external data
        sources to destination data sources, and will provide the basis for building out the rest of
        our data use cases.


    - We need to design our solution with these things in mind:

        1. Types of queries, processing, and so on that we'll be performing against the destination data

        2. Any customer-facing data requirements

        3. Types of data collected in terms of its value


    - It is crucial that we pay careful attention to modeling and storing the data in a way that 
        facilitates further access.


    - Primary Concerns and Risk Management

        1. Consumption of source data

        2. Guarantees around data delivery

        3. Data governance

        4. Latency and confirmations of delivery

        5. Access patterns against the destination data



- Source Data Consumption

    - Sources can be anything from phones, sensors, applications, machine logs, operational and
        transactional databases, etc.  The source itself is mostly out of scope of your pipeline and
        staging use cases.  

      In fact, you can evaluate the success of your scoping by how much of your time you spend working 
        with the source team.  The less time your data team spends with the source team, the better the
        source integration is designed.


    - There are several standard approaches used for source data collection:

        1. Embedded Code

             This is when you provide code embedded within the source system that knows how to send 
               required data into your data pipeline.


        2. Agents

             This is an independent system that is close to the source and in many cases is on the same
               device.  This is different from the embedded code approach, because the agents run as 
               separate processes with no dependency concerns.


        3. Interfaces

             This is the lightest of the options.  Examples include REST or Websocket endpoints that
               receive data sent by the sources.


        4. Other Options

             There are other commonly used options to perform data collection.  For example, there are
               third party data integration tools, either open source or commercial.  

             Also, there are batch ingest tools such as Apache Sqoop or tools provided with specific
               projects, such as the HDFS 'put' command.



- Source Data Consumption: Embedded Code

    - The following guidelines should be considered when implementing embedded code as part of a source
        data collection:

        1. Limit Implementation Languages

             Don't try to support multiple languages.  Instead, implement with a simple language and
               use bindings for other languages as needed.  For example, consider using C, C++, or Java,
               and then create bindings for other languages you need to support.

             For example, Kafka includes a Java producer and consumer as part of the core project, 
               whereas other libraries or clients require binding to languages included as part of the
               Kafka distribution.


        2. Limit Dependencies

             Potential library conflicts are a problem with any embedded code.  Making efforts to limit
               dependencies can help mitigate this issue.


        3. Provide Visibility

             With any embedded code, there can be concerns about what is under the hood.  Providing 
               access to code, by open sourcing the code or at least providing a public repository,
               provides an easy and safe way to relieve these fears.


        4. Operationalizing Code

             Possible production issues with embedded code must also be considered.  Make sure things
               like memory leaks and performance issues have been taken into account.  Have a defined
               support model.  Logging and instrumentation of code can help to ensure we have the 
               ability to debug issues when they arise.


        5. Version Management

             When code is embedded, you won't be able to control the scheduling of updates.  Ensuring
               backwards compatibility and well-defined versions is key.



- Source Data Consumption: Agents

    - These guidelines should be considered when using agents in your architecture:

        1. Deployment

             As with other components in your architecture, make sure deployment of agents is tested
               and repeatable.  This might mean using containers or some other type of automation tool.


        2. Resource Usage

             Ensure that the source systems have sufficient resources to reliably support the agent 
               processes, including CPU, memory, etc.


        3. Isolation

             Even though agents run externally from the processing applications, you'll still want to
               protect against problems with the agent that can negatively affect data collection.


        4. Debugging

             Again, we need to take steps to ensure we can debug and recover from inevitable production
               issues.  This means logging, instrumentation, etc.



- Source Data Consumption: Interfaces

    - These guidelines should be considered when using interfaces in your architecture:

        1. Versioning

             Versioning is an issue here also, although less painful than the embedded solution.  Make
               sure your interface has versioning as a core concept from day one.


        2. Performance

             With any source collection framework, performance and throughput are critical.  Even if you
               design and implement code to ensure performance, you might still find that source or
               sink implementations are suboptimal.  Because you might not control this code, having a 
               way to detect and notify performance issues is important.


        3. Security

             While in the agent and embedded models you control the code that is talking to you, in the 
               interface model you will have only the interface as a barrier to entry.  The key is to
               keep the interface simple while still injecting security.  Models such as security
               tokens are used for this.
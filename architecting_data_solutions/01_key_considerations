----------------------------------------------------
CHAPTER 1 - KEY PROJECT TYPES AND CONSIDERATIONS
----------------------------------------------------

- Major Data Project Types

    1. Data Pipelines and Data Staging

         These are ETL-type projects that provide the basis for performing subsequent analysis and
           processing of data.


    2. Data Processing and Analysis

         These are projects that end in providing some kind of actionable value, like the creation of
           reports or machine learning models.


    3. Applications

         These are data frameworks meant to support live operational needs of applications, like the
           data backends for web or mobile applications.



- Aspects of the Project Types

    1. Primary Considerations

         Each of the 3 project types have distinctions that will affect architectural decisions and 
           priorities.  The decisions will drive the rest of the project.  


    2. Risk Management

         Each project type has a set of associated risks.  There are often multiple approaches to risk
           management based on specific use cases.


    3. Team Makeup

         The types of skills, experience, and interests will vary with the different project types, so
           we provide some recommendatations around building successful teams for each project type.


    4. Security

         Security is an extensive and important topic we can't go into a ton of detail on here, but 
           we'll enumerate the security concerns we need to keep in mind throughout our projects.



- Fundamental Security Dimensions

    1. Authentication

         This involves ensuring that users accessing the system are who they claim to be.  Any mature
           system should offer support for strong authentication.  This is typically done with an
           authentication protocol like Kerberos or LDAP.


    2. Authorization

         After ensuring that a valid user is accessing a system, we need to determine which data they're
           allowed to access.  A mature system will allow access to be controlled at different levels
           of granularity, such as at both the table-level and column-level.  


    3. Encrpytion

         We encrypt data to protect it from malicious users and intrusions.  We need to consider this from
           2 different angles:

           - Data at rest = This is data stored on disk.  Many vendors have solutions for managing this.

           - Data on the wire = This is data moving through the system.  Generally, this is supported via
               standard encryption mechanisms like TLS.


    4. Auditing

         We must be able to capture activity related to data in our system, including the lineage of the
           data, who is accessing it, how it is being used, and so on.  Again here, many vendors offer
           tools to manage this.



- Project Type #1: Data Pipelines and Data Staging

    - This project type has the widest scope of the 3, because it involves the path from external data
        sources to destination data sources, and will provide the basis for building out the rest of
        our data use cases.


    - We need to design our solution with these things in mind:

        1. Types of queries, processing, and so on that we'll be performing against the destination data

        2. Any customer-facing data requirements

        3. Types of data collected in terms of its value


    - It is crucial that we pay careful attention to modeling and storing the data in a way that 
        facilitates further access.


    - Primary Concerns and Risk Management

        1. Consumption of source data

        2. Guarantees around data delivery

        3. Data governance

        4. Latency and confirmations of delivery

        5. Access patterns against the destination data



- Source Data Consumption

    - Sources can be anything from phones, sensors, applications, machine logs, operational and
        transactional databases, etc.  The source itself is mostly out of scope of your pipeline and
        staging use cases.  

      In fact, you can evaluate the success of your scoping by how much of your time you spend working 
        with the source team.  The less time your data team spends with the source team, the better the
        source integration is designed.


    - There are several standard approaches used for source data collection:

        1. Embedded Code

             This is when you provide code embedded within the source system that knows how to send 
               required data into your data pipeline.


        2. Agents

             This is an independent system that is close to the source and in many cases is on the same
               device.  This is different from the embedded code approach, because the agents run as 
               separate processes with no dependency concerns.


        3. Interfaces

             This is the lightest of the options.  Examples include REST or Websocket endpoints that
               receive data sent by the sources.


        4. Other Options

             There are other commonly used options to perform data collection.  For example, there are
               third party data integration tools, either open source or commercial.  

             Also, there are batch ingest tools such as Apache Sqoop or tools provided with specific
               projects, such as the HDFS 'put' command.



- Source Data Consumption: Embedded Code

    - The following guidelines should be considered when implementing embedded code as part of a source
        data collection:

        1. Limit Implementation Languages

             Don't try to support multiple languages.  Instead, implement with a single language and
               use bindings for other languages as needed.  For example, consider using C, C++, or Java,
               and then create bindings for other languages you need to support.

             For example, Kafka includes a Java producer and consumer as part of the core project, 
               whereas other libraries or clients require binding to languages included as part of the
               Kafka distribution.


        2. Limit Dependencies

             Potential library conflicts are a problem with any embedded code.  Making efforts to limit
               dependencies can help mitigate this issue.


        3. Provide Visibility

             With any embedded code, there can be concerns about what is under the hood.  Providing 
               access to code, by open sourcing the code or at least providing a public repository,
               provides an easy and safe way to relieve these fears.


        4. Operationalizing Code

             Possible production issues with embedded code must also be considered.  Make sure things
               like memory leaks and performance issues have been taken into account.  Have a defined
               support model.  Logging and instrumentation of code can help to ensure we have the 
               ability to debug issues when they arise.


        5. Version Management

             When code is embedded, you won't be able to control the scheduling of updates.  Ensuring
               backwards compatibility and well-defined versions is key.



- Source Data Consumption: Agents

    - These guidelines should be considered when using agents in your architecture:

        1. Deployment

             As with other components in your architecture, make sure deployment of agents is tested
               and repeatable.  This might mean using containers or some other type of automation tool.


        2. Resource Usage

             Ensure that the source systems have sufficient resources to reliably support the agent 
               processes, including CPU, memory, etc.


        3. Isolation

             Even though agents run externally from the processing applications, you'll still want to
               protect against problems with the agent that can negatively affect data collection.


        4. Debugging

             Again, we need to take steps to ensure we can debug and recover from inevitable production
               issues.  This means logging, instrumentation, etc.



- Source Data Consumption: Interfaces

    - These guidelines should be considered when using interfaces in your architecture:

        1. Versioning

             Versioning is an issue here also, although less painful than the embedded solution.  Make
               sure your interface has versioning as a core concept from day one.


        2. Performance

             With any source collection framework, performance and throughput are critical.  Even if you
               design and implement code to ensure performance, you might still find that source or
               sink implementations are suboptimal.  Because you might not control this code, having a 
               way to detect and notify performance issues is important.


        3. Security

             While in the agent and embedded models you control the code that is talking to you, in the 
               interface model you will have only the interface as a barrier to entry.  The key is to
               keep the interface simple while still injecting security.  Models such as security
               tokens are used for this.



- Risk Management for Data Consumption

    - The risks you need to worry about when building out a data source collection system include
        everything you would normally worry about in an externally facing API, as well as concerns
        related to scale.  Major concerns include:

        1. Version Management

             Everyone loves a good API that just works, but the problem is that we can rarely design
               interfaces that won't require incompatible changes at some point.  You will want to have
               a robust versioning strategy and a plan for providing backward compatibility guarantees
               to protect against this, as well as ensuring that this plan is part of your 
               communication strategy.


        2. Impacts from Source Failures

             There are a number of possible failure scenarios at the source layer for which you need to
               plan.  For example, if you have embedded code that is part of the source execution process,
               a failure in your code can lead to a overall failure in the data collection.

             Even if you don't have embedded code, if there's a failure in your collection mechanism 
               such as an agent, how is the source affected?  Is there data loss?  Does this failure 
               affect uptime for the application?

             The answer is to have options and know your sources and communicate those different options
               with clear understanding that failure and downtime will happen.  This will allow adding
               additional safeguards to protect against the possible failure scenarios.

             Although failures in data pipelines should be rare, they will happen.  So, our pipelines
               need to have mechanisms to alert us when undesired things take place.  Examples include
               monitoring things like throughput and alerting when metrics are seen to deviate from
               specific thresholds.  

             Additionally, we may consider having replicated pipelines.  In failure cases, if one pipeline
               goes down, another can take over.  This helps protect from difficult-to-predict failures
               like badly configured deployments or a bad build getting pushed.  Ideally, our pipelines
               should be as simple to deploy as a web application.


        3. Protections from Sources that Behave Poorly

             When you build a data ingenstion system, it's possible that sources might misuse your APIs,
               send too much data, etc.  As you design and implement your system, you must put mechanisms
               to protect against these risks in place.  

             Throtlling will limit the number of records a source can send you.  As the source sends you
               more records, your system can increase the time to accept that data.  Additionally, you
               might need to send a message indicating that the source is making too many connections.

             If your system doesn't provide guarantees, you could simply drop messages if you become
               overloaded or have trouble processing input data.  If you do drop data, make sure that
               your clients have full knowledge of when and why.



- Data Delivery Guarantees

    - When planning a data pipeline, there are a number of promises that you will need to give to the
        owners of the data you're collecting.  With any data collection system, you can offer different
        levels of guarantees:

        1. Best Effort

             If a message is sent to you, you try to deliver it, but data loss is possible.  This is
               suitable when you're not concerned about capturing every event.

             An example might be if you're performing processing on incoming data to capture aggregate
               metrics for which total precision isn't required.


        2. At Least Once

             If a message is sent to you, you might duplicate it, but you won't lose it.  This is the
               most common use case, since in most cases you'll want to ensure that all events are
               captured in your pipeline.  

             Note that this might also require adding logic in your pipeline to de-duplicate data, but 
               in most cases this is easier and less expensive than the exactly-once option.


        3. Exactly Once

             If a message is received by the pipeline, you guarantee it will be processed, and will never
               be duplicated.  This is the most expensive and technically complex option.  

             Although many systems now provide this, you should still consider whether this is necessary
               or whether you can account for duplicate records in another way.



               
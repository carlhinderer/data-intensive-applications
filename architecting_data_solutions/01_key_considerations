----------------------------------------------------
CHAPTER 1 - KEY PROJECT TYPES AND CONSIDERATIONS
----------------------------------------------------

- Major Data Project Types

    1. Data Pipelines and Data Staging

         These are ETL-type projects that provide the basis for performing subsequent analysis and
           processing of data.


    2. Data Processing and Analysis

         These are projects that end in providing some kind of actionable value, like the creation of
           reports or machine learning models.


    3. Applications

         These are data frameworks meant to support live operational needs of applications, like the
           data backends for web or mobile applications.



- Aspects of the Project Types

    1. Primary Considerations

         Each of the 3 project types have distinctions that will affect architectural decisions and 
           priorities.  The decisions will drive the rest of the project.  


    2. Risk Management

         Each project type has a set of associated risks.  There are often multiple approaches to risk
           management based on specific use cases.


    3. Team Makeup

         The types of skills, experience, and interests will vary with the different project types, so
           we provide some recommendatations around building successful teams for each project type.


    4. Security

         Security is an extensive and important topic we can't go into a ton of detail on here, but 
           we'll enumerate the security concerns we need to keep in mind throughout our projects.



- Fundamental Security Dimensions

    1. Authentication

         This involves ensuring that users accessing the system are who they claim to be.  Any mature
           system should offer support for strong authentication.  This is typically done with an
           authentication protocol like Kerberos or LDAP.


    2. Authorization

         After ensuring that a valid user is accessing a system, we need to determine which data they're
           allowed to access.  A mature system will allow access to be controlled at different levels
           of granularity, such as at both the table-level and column-level.  


    3. Encrpytion

         We encrypt data to protect it from malicious users and intrusions.  We need to consider this from
           2 different angles:

           - Data at rest = This is data stored on disk.  Many vendors have solutions for managing this.

           - Data on the wire = This is data moving through the system.  Generally, this is supported via
               standard encryption mechanisms like TLS.


    4. Auditing

         We must be able to capture activity related to data in our system, including the lineage of the
           data, who is accessing it, how it is being used, and so on.  Again here, many vendors offer
           tools to manage this.



- Project Type #1: Data Pipelines and Data Staging

    - This project type has the widest scope of the 3, because it involves the path from external data
        sources to destination data sources, and will provide the basis for building out the rest of
        our data use cases.


    - We need to design our solution with these things in mind:

        1. Types of queries, processing, and so on that we'll be performing against the destination data

        2. Any customer-facing data requirements

        3. Types of data collected in terms of its value


    - It is crucial that we pay careful attention to modeling and storing the data in a way that 
        facilitates further access.


    - Primary Concerns and Risk Management

        1. Consumption of source data

        2. Guarantees around data delivery

        3. Data governance

        4. Latency and confirmations of delivery

        5. Access patterns against the destination data



- Source Data Consumption

    - Sources can be anything from phones, sensors, applications, machine logs, operational and
        transactional databases, etc.  The source itself is mostly out of scope of your pipeline and
        staging use cases.  

      In fact, you can evaluate the success of your scoping by how much of your time you spend working 
        with the source team.  The less time your data team spends with the source team, the better the
        source integration is designed.


    - There are several standard approaches used for source data collection:

        1. Embedded Code

             This is when you provide code embedded within the source system that knows how to send 
               required data into your data pipeline.


        2. Agents

             This is an independent system that is close to the source and in many cases is on the same
               device.  This is different from the embedded code approach, because the agents run as 
               separate processes with no dependency concerns.


        3. Interfaces

             This is the lightest of the options.  Examples include REST or Websocket endpoints that
               receive data sent by the sources.


        4. Other Options

             There are other commonly used options to perform data collection.  For example, there are
               third party data integration tools, either open source or commercial.  

             Also, there are batch ingest tools such as Apache Sqoop or tools provided with specific
               projects, such as the HDFS 'put' command.



- Source Data Consumption: Embedded Code

    - The following guidelines should be considered when implementing embedded code as part of a source
        data collection:

        1. Limit Implementation Languages

             Don't try to support multiple languages.  Instead, implement with a single language and
               use bindings for other languages as needed.  For example, consider using C, C++, or Java,
               and then create bindings for other languages you need to support.

             For example, Kafka includes a Java producer and consumer as part of the core project, 
               whereas other libraries or clients require binding to languages included as part of the
               Kafka distribution.


        2. Limit Dependencies

             Potential library conflicts are a problem with any embedded code.  Making efforts to limit
               dependencies can help mitigate this issue.


        3. Provide Visibility

             With any embedded code, there can be concerns about what is under the hood.  Providing 
               access to code, by open sourcing the code or at least providing a public repository,
               provides an easy and safe way to relieve these fears.


        4. Operationalizing Code

             Possible production issues with embedded code must also be considered.  Make sure things
               like memory leaks and performance issues have been taken into account.  Have a defined
               support model.  Logging and instrumentation of code can help to ensure we have the 
               ability to debug issues when they arise.


        5. Version Management

             When code is embedded, you won't be able to control the scheduling of updates.  Ensuring
               backwards compatibility and well-defined versions is key.



- Source Data Consumption: Agents

    - These guidelines should be considered when using agents in your architecture:

        1. Deployment

             As with other components in your architecture, make sure deployment of agents is tested
               and repeatable.  This might mean using containers or some other type of automation tool.


        2. Resource Usage

             Ensure that the source systems have sufficient resources to reliably support the agent 
               processes, including CPU, memory, etc.


        3. Isolation

             Even though agents run externally from the processing applications, you'll still want to
               protect against problems with the agent that can negatively affect data collection.


        4. Debugging

             Again, we need to take steps to ensure we can debug and recover from inevitable production
               issues.  This means logging, instrumentation, etc.



- Source Data Consumption: Interfaces

    - These guidelines should be considered when using interfaces in your architecture:

        1. Versioning

             Versioning is an issue here also, although less painful than the embedded solution.  Make
               sure your interface has versioning as a core concept from day one.


        2. Performance

             With any source collection framework, performance and throughput are critical.  Even if you
               design and implement code to ensure performance, you might still find that source or
               sink implementations are suboptimal.  Because you might not control this code, having a 
               way to detect and notify performance issues is important.


        3. Security

             While in the agent and embedded models you control the code that is talking to you, in the 
               interface model you will have only the interface as a barrier to entry.  The key is to
               keep the interface simple while still injecting security.  Models such as security
               tokens are used for this.



- Risk Management for Data Consumption

    - The risks you need to worry about when building out a data source collection system include
        everything you would normally worry about in an externally facing API, as well as concerns
        related to scale.  Major concerns include:

        1. Version Management

             Everyone loves a good API that just works, but the problem is that we can rarely design
               interfaces that won't require incompatible changes at some point.  You will want to have
               a robust versioning strategy and a plan for providing backward compatibility guarantees
               to protect against this, as well as ensuring that this plan is part of your 
               communication strategy.


        2. Impacts from Source Failures

             There are a number of possible failure scenarios at the source layer for which you need to
               plan.  For example, if you have embedded code that is part of the source execution process,
               a failure in your code can lead to a overall failure in the data collection.

             Even if you don't have embedded code, if there's a failure in your collection mechanism 
               such as an agent, how is the source affected?  Is there data loss?  Does this failure 
               affect uptime for the application?

             The answer is to have options and know your sources and communicate those different options
               with clear understanding that failure and downtime will happen.  This will allow adding
               additional safeguards to protect against the possible failure scenarios.

             Although failures in data pipelines should be rare, they will happen.  So, our pipelines
               need to have mechanisms to alert us when undesired things take place.  Examples include
               monitoring things like throughput and alerting when metrics are seen to deviate from
               specific thresholds.  

             Additionally, we may consider having replicated pipelines.  In failure cases, if one pipeline
               goes down, another can take over.  This helps protect from difficult-to-predict failures
               like badly configured deployments or a bad build getting pushed.  Ideally, our pipelines
               should be as simple to deploy as a web application.


        3. Protections from Sources that Behave Poorly

             When you build a data ingenstion system, it's possible that sources might misuse your APIs,
               send too much data, etc.  As you design and implement your system, you must put mechanisms
               to protect against these risks in place.  

             Throtlling will limit the number of records a source can send you.  As the source sends you
               more records, your system can increase the time to accept that data.  Additionally, you
               might need to send a message indicating that the source is making too many connections.

             If your system doesn't provide guarantees, you could simply drop messages if you become
               overloaded or have trouble processing input data.  If you do drop data, make sure that
               your clients have full knowledge of when and why.



- Data Delivery Guarantees

    - When planning a data pipeline, there are a number of promises that you will need to give to the
        owners of the data you're collecting.  With any data collection system, you can offer different
        levels of guarantees:

        1. Best Effort

             If a message is sent to you, you try to deliver it, but data loss is possible.  This is
               suitable when you're not concerned about capturing every event.

             An example might be if you're performing processing on incoming data to capture aggregate
               metrics for which total precision isn't required.


        2. At Least Once

             If a message is sent to you, you might duplicate it, but you won't lose it.  This is the
               most common use case, since in most cases you'll want to ensure that all events are
               captured in your pipeline.  

             Note that this might also require adding logic in your pipeline to de-duplicate data, but 
               in most cases this is easier and less expensive than the exactly-once option.


        3. Exactly Once

             If a message is received by the pipeline, you guarantee it will be processed, and will never
               be duplicated.  This is the most expensive and technically complex option.  

             Although many systems now provide this, you should still consider whether this is necessary
               or whether you can account for duplicate records in another way.



- Data Management and Governance

    - A robust data collection system today must have 2 critical features:

        1. Data Model Management = the ability to change or add data models

        2. Data Regulation = the ability to know everything that is being collected and the risks that
                               might exist if the data is misused or exposed


    - You need to have mechanisms in place to capture your system's data models.  Ideally, this should
        mean that groups using your data pipeline don't need to engage your team to make a new data
        feed or change an existing data feed.


    - An example of a system that provides an approach for capturing the schema is 'Confluent Schema
        Registry' for Kafka, which allows for the storate of multiple versions of schemas.  This 
        provides support for backward compatibility for different versions of applications that access
        data in the system.


    - Declaring a schema is only part of the problem.  You might also need the following mechanisms:

        1. Registration

             The definition of a new data feed along with its schema


        2. Routing

             Which data feeds should go to which topics, processing systems, and possible storage
               systems.


        3. Sampling

             An extension to routing, with the added feature of removing part of the data.  This is 
               ideal for staging environment and for testing.


        4. Access Controls

             Who will be able to see the data, both in final persisted state and off the stream.


        5. Metadata Captured

             The ability to attach metadata to the fields.


      Additional features that you will find in more advanced systems include:

        6. Transformational Logic

             The ability to transform the data in custom ways before it lands in the staging areas.


        7. Aggregation and Sessionization

             Transformational logic that understands how to perform operations on data windows.


    - As more data is collected, stored, and analyzed, concern for things like data protection and privacy
        have grown.  This means you need to have plans in place for responding to regulation, as well as
        protecting against external hacks, internal misuse, and so on.



- Latency

    - Unlike the requirements of a real-time system, a data pipeline normally gets a lot of leeway when
        it comes to latency and confirmations of delivery.  However, this is a very important area for
        you to scope and define expectations.  


    - Latency is the time it takes from when a source publishes information until that information is
        accessible by a given processing layer or staging system.  

      To illustrate this, we'll look at the example of a stream-processing application.  For this example,
        assume we have data coming in through Kafka, and is being consumed by a Flink or Spark Streaming
        application.  This application might then be sending alerts based on the outcome of processing
        to downstream systems.  

      In this case, we quantify latency in multiple buckets:

        Source  -->  Kafka  -->  Streaming Application  -->  Kafka  -->  Downstream

        |     A     |     B     |         C           |         D         |


        A: This is the time to get from the source to Kafka.  It can be a direct shot, where the time
             is just to buffer and send over the network.  However, there might be a fan-in architecture
             that includes load balancers, microservices, or other hops that can result in increased
             latency.


        B: This is the time to get through Kafka.  It depends on a number of things such as the Kafka
             configuration and consumer configuration.


        C: This is the time between when the processing engine receives the event to when it triggers on
             the event.  With some engines like Spark Streaming, triggering happens on a time interval.
             With other like Flink, the latency is lower.  This will also be affected by configuration and
             use cases.


        D: Again, this is the time to get into Kafka and be read by Kafka.  Latency here will be highly
             dependent on buffering and polling configurations in the producer and consumer.



- Delivery Confirmation

    - With respect to the data pipeline, delivery confirmations let the source know when the data has
        arrived at different stages in your pipeline and could even let the source know if the data has
        reached the staging area.


    - Here are some guidelines on designing this into your system:

        1. Do you really need confirmation?

             The advantage of confirmation is that it allows the source to resend data in the event of
               a failure.  Because failure is inevitable, providing confirmation will likely be suitable
               for most use cases.  However, if this is not required, you can reduce time and complexity
               by omitting it.


        2. How to deliver the confirmation?

             If you need to provide confirmations, you need to add this to your design.  This will likely
               include selecting software solutions that provide confirmation functionality and
               bulding logic into any custom code you implement as part of your pipeline.

             Because this can be complex, its best to use existing solutions that provide this.



- Risk Management for Data Delivery

    - Data delivery promises can be risky.  We want to offer a system that gives users everything they
        want, but in the real world guarantees will fail when something goes wrong.


    - The first way to deal with this risk is to have a clean architecture that's shared with stakeholders
        in order to solicit input that might help ensure stability.  Also, having adequate metrics and
        logging is important to validate that the implementation has met the requirements.


    - The second way to deal with this risk is to have a mechanism that notifies users and the source
        systems when guarantees are being missed.  This will provide time to adjust or switch to a 
        backup system.



- Access Patterns

    - It's important to call out the types of access and requirements we should plan for when defining
        our data pipeline.  This can be broken into 2 groupings, access to data, and retention of data.


    - These are the types of jobs that will most likely come up as priorities:

        1. Batch jobs that do large scans and aggregations

        2. Streaming jobs that do large scans

        3. Point data requests (ie random access)

        4. Search queries



- Access Pattern: Batch Jobs with Large Scans

    - Batch jobs that scan large blocks of data are core workloads of data research and analytics.
        Here are some typical workloads that fit into this category with real-world examples for a
        supermarket chain.

        A. Analytical SQL

             This might entail using SQL to do rollups of which items are selling by zip code and date.
             It would be very common that reports like this would run daily and be visible to 
             leadership within the company.


        B. Sessionization

             Using SQL or tools like Apache Spark, we might want to sessionize the buying habits of our
               customers.  For example, we could better predict their individual shopping needs and
               patterns and be alerted to churn risks.


        C. Model Training

             A use case for machine learning in our supermarket example is a recommendation solution
               that would look at the shopping habits of our customers and provide suggestions for
               items based on customers' known preferences.


        D. Scenario Predictions Evaluation

             In the supermarket use case, we need to define a strategy for deciding how to order the
               items to fill shelves in our stores.  We can use historical data to know whether our
               purchasing model is effective.



- Access Patterns: Streaming Jobs with Large Scans

    - There are 2 main differences between batch and streaming workloads: the time between execution
        of the job, and the idea of progressive workloads.  

        1. With respect to time, streaming jobs are normally thought to be in the range of milliseconds
             to minutes.  Batch workloads are typically minutes to hours or even days.

        2. The progressive workload is maybe the bigger difference because it is a difference in the output
             of the job.


    - Where the batch processing needs massive storage and massive compute, the streaming component
        really only needs enough compute, storage, and memory to hold windows in context.


    - Here are our 4 types of jobs and how progressiveness affacts them:

        A. Analytical SQL

             With streaming jobs, our rollups and reports will update at smaller intervals like seconds or
               minutes, allowing for visibility into near-real-time changes for faster reaction time.
               Additionally, ideally the processing expense will not be that much bigger than batch
               because we are working on only the newly added data and not reprocessing past data.


        B. Sessionization

             As with the rollups, sessionization in streaming jobs also happens in smaller intervals,
               allowing for more real-time actionable results.  In our supermarket example, maybe we are
               using sessionization to analyze the items customers put into our carts and predict what
               they are making for dinner.


        C. Model Training

             Not all models lend themselves to real-time training.  However, the trained models might be
               ideal for real-time execution to make real-time actionable calls in our business.


        D. Scenario Prediction Evaluation

             Additionally, now that we are making real-time calls with technology, we need a real-time
               way to evaluate those decisions to power humans to know whether corrections need to be
               made.  As we move to more automation in terms of decision making, we need to match that
               with real-time evaluation.



- Access Patterns: Point Requests

    - The idea with point requests is that we want to fetch a specific record or data point very fast
        and with high concurrency.


    - For instance, we may want to:

        1. Look up the current items in stock for a given store

        2. Look up the current location of items being shipped to your store

        3. Look up the complete supply chain for items in your store

        4. Look up events over time and be able to scan forward and backward in time to investigate
             how those events affect things like sales



- Access Patterns: Searchable Access

    - The last pattern that is very common is searchable access to data.  In this use case, access 
        patterns need to be fast while also allowing for flexibility in queries.  


    - In our supermarket example, you might want to learn what types of vegetables are in stock or
        look at all the shipments from a given farm since there was a contamination.



- Pipeline and Staging Team Makeup

    - The following are some job roles to consider:

        1. Service and Support Engineers

             These are engineers that are tasked with working with users, including stakeholders of
               source data systems, users accessing data, etc.  They are trained to optimally use the
               system, look for good use cases, work with the team to address user issues, and help
               users be successful.


        2. System Engineers and Administrators

             These are engineers obsessed with uptime, latency, efficiency, failure, and data integrity.
               They don't need to care much about the real use cases for the system.  They focus on the
               reliability of the system and its performance.


        3. Data Engineers

             These are engineers who know the data and the types of storage systems within the system.
               Their main job is to ensure that the data offerings are being used correctly and
               that the appropriate big data solutions are being used for the right thing.  They are
               experts in data storage and data processing.


        4. Data Architects

             These are the experts in data modeling who can define the structures that define how data
               in the system should be structured.  In some cases, this role may be assumed by the 
               data engineers.



- Project Type #2: Data Processing and Analysis

    - These are the data project for which we use the data populated by the data pipeline and staging
        projects and then transform and analyze the data to derive value.  


    - We focus on these items when we're evaluating this use case:

        1. Defining the problems we're trying to solve

        2. Implementing and operationalizing the processing and analysis to solve the problems we've
             identified

      In other words, what do we want to do, how do we get there, how do we make it repeatable, and how
        do we quantify its value when we get there?



- Defining the Problems to be Solved

    - In many cases, the business will already know the questions that need to be answered to provide 
        value.  Often, though, determining the appropriate questions to ask and which are the correct
        problems to solve can be a challenge.

      We want to ask what will make an impact to our business and what can be made actionable.  
        Determining the answers usually involves talking to stakeholders.  These are some good basic
        questions to ask:

        1. What are the most important things to capture in terms of trackable metrics?

        2. What are things that affect customer engagement?

        3. Is there a gap in the company's offerings?

        4. Are there any active pain points?


    - A good next step is to try to further define our objectives in solving the problem.  This will
        generally mean defining specific metrics, numbers, and visualizations so that we can help
        evaluate approaches to solving the problem.  

      Let's apply this idea to our supermarket example.  Suppose that our problem is a concern that
        shelves are not fully stocked with the proper products.  We could create visualizations to help
        illustrate this, such as:

        1. Products stocked over time

             We could use a time-based chart that shows the average, max, min, top 10%, and bottom 10%
               of inventory for each product.


        2. Products out of stock

             We could use a time-based chart illustrating times when each item is unstocked.


        3. Substitute purchase patterns

             During times when a product is out of stock, do customers select a different product?


        4. Delivery supply chain

             For products that were understocked, was the problem caused by delivery delays?


        5. Region or store variations

             Are there differences in stock levels across regions or stores?


        6. Customer impact

             For customers who are normal purchasers of the understocked items, do we see a change in 
               spending between time periods when inventory levels are acceptable and when the item is
               out of stock?



- Risk Management for Problem Definition

    - Here are 2 things that can help manage risk in problem definition:

        1. Get many viewpoints

             Make sure you talk to as many people as possible to try and understand the problem from 
               all angles.


        2. Build trust

             You want to be seen as a collaborator, not someone who is trying to find shortcomings in an 
               organization.  Work closely with the stakeholders to gain trust.



- Implementing and Operationalizing Solutions

    - Now that we've identified problems and explored possible solutions, we're ready to move on to 
        implementing these solutions.  While keeping in mind the original problem we're trying to solve,
        we also need ot remain flexible and adapt to new information.


    - We want to build a robust solution.  A common trap is to build out systems that only solve a 
        single problem.  A better approach is to provide a platform for solving multiple problems.

      On the other hand, if you try to build a system that solves every problem, you might end up with
        a system that's not well suited to solve any problem.  It's always good to start small and make
        sure you have a clear understanding of the business requirements.


    - Another common trap is trusing the same people who find or solve problems to operationalize the
        solutions, even though these activities require completely different skill sets.  You will want
        a good pipeline and good communication between these groups.



- Data Processing and Analytics Team Makeup

    - Unlike the pipeline and staging teams, the teams that can successfully use data to provide value
        need to be more focused on problem discovery, working across teams, and finding paths to solutions.
        This means we need a different mix of people:

        1. Problem Seekers

             These are people who earn trust with different groups in your company and are good at
               identifying and quantifying problems.  They are treasure hunters who work with 
               stakeholders to find problems whose solutions will have a high impact.  They can be
               project managers, product managers, technical leads, or even developers and analysts.


        2. Architects

             A requirement for successful problem-solving is to pick not only the correct problems to 
               solve, but also the correct order to solve them in.  We want to pick the problems with
               the least effort to solve that include reusable parts.


        3. The Brains

             These are the data scientists and analysts who will help come up with solutions.


        4. The Engineers

             You'll need a group of engineers who know how to work with all of the aformentioned 
               parties and how to take their work and productionize it.


        5. Solution Communication Experts

             Communication experts can evangelize the solutions so that they get to reach their full
               potential.  These are usually project managers, product managers, or technical leads.



- Project Type #3: Application Development

    - Whereas the first 2 project types are focused on exploring data and gaining value from that data,
        the final case is about deploying applications that use our data to provide some service to
        users, either internal or external.  An example is a website that relies on data to drive
        functionality.


    - The assumption here is that the data is being used to drive applications that support large numbers
        of users, requiring scalability, reliability, and availability.


    - Primary considerations for building applications are:

        1. Latency and throughput

             How long does it take to execute an operation, and how many operations can the system 
               handle per second?


        2. State locality and consistency

             If your system is available in more than one region, how is replication handled?  Is it
               siloed, eventually replicated, or strongly locked?


        3. Availability of the systems

             What are the failure and recovery characteristics of the system?



- Latency and Throughput

    - When building out requirements for an application, a good place to begin is learning what data you
        will need to have saved and how you're going to interact with that data.


    - There are a number of potential concerns we need to be considering for every data interaction in
        our design:

        1. Race Conditions

             For example, if 2 clients update the same row at the same time, who wins?  There are a few
               ways to address this, including:

               - Last One Wins = It doesn't matter who wins, they randomly overwrite each other

               - Transactional Locking = There is a lock at the data-store level or at the server level
                                           before mutation is allowed.


        2. Asynchronous vs Synchronous Operations

             In systems that reqire very low latency, it might be unrealistic to save in real time.  There
               might be a desire to have state in server or client memory and persist to the final
               store asynchronously.  

             There is a speed vs truth tradeoff.  With an asynchronous model, there is a short window 
               during which there is a chance for data loss.  You need to evaluate how important that is
               compared to latency.


        3. Performance Consistency

            Does the tested performance hold?  What if the insertion order changes?  Will the performance
              change as data scales?  Does performance change as the storage solution grows?  How will
              performance be affected by maintenance work?



- Risk Management for Latency

    - The best way to mitigate risk in relation to performance is to test early and often and always
        communicate.  Everything around performance should be monitored.  Every connection, whether 
        internal or external, should be fully documented.  You should question results often.


    - Make sure you use interfaces in your design, so that you can swap out implementations.  The odds
        are high that you'll think of a better strategy after you build the first solution.  



- State Locality

    - There are a number of places where state can exist.  In a distributed system, we can highlight
        4 major locations to hold state:

        1. Client

             This is the client-side interface the user is using.  Caching data in the client and 
               allowing client-side mutation provides high performance and scalability.  

             However, the client is ephemeral, since it can die at any time, causing data loss before the
               data reaches the server.  Also, the client runs on potentially untrusted hosts with 
               untrusted users.


        2. Server

             This is the server or servers that the client accesses.  On the server we don't need to be
               concerned with trust issues, but ephemeral data can still be lost.  Also, in a partitioned
               system, the data is not shared across all partitions.


        3. Datacenter

             This is persistence with the local datacenter housing the application servers.  This layer
               is where you see state persistence for common technologies like relational databases,
               NoSQL systems, and distributed caches.  The goal with this layer is to store all the
               state needed for this local region and the server and clients within this region.

             The data in this datacenter will most likely be replicated and protected from failure of
               one or more nodes, although an entire region can still go down.


        4. Multidatacenter

             This is replicated persistence across datacenters.  


    - Multidatacenter Models

        There are several models for the multidatacenter scenario:

          A. Replication for disaster recovery

               This configuration can provide protection against data loss if an entire datacenter
                 becomes unavailable.  This is normally a batch process in which data becomes eventually
                 consistent across datacenters.  It won't protect against all data loss, but it protects
                 against most of it.

               One concern is what happens if items are being mutated in multiple regions at the same
                 time.  We don't have globally consistent state, so there is no single source of truth.


          B. Locking

               Global locking is one way to ensure consistency across regions in case of mutated data.
                 The idea is that a resource is globally locked to a region.  No one can mutate that
                 data source until they get the lock.

               The locking can be implemented in the client (every client must obtain a lock to change
                 the record), at the datacenter (all mutations have to go through a given datacenter
                 first), or at the record level (using a quorum).